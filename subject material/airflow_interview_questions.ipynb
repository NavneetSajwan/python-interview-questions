{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q:1) : what is airflow\n",
    "Apache Airflow is a workflow engine that will easily schedule and run your complex data pipelines. It will make sure that each task of your data pipeline will get executed in the correct order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [which operator used to run databricks job](https://hevodata.com/learn/airflow-databricks/)\n",
    "<!-- https://hevodata.com/learn/airflow-databricks/ -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## [which operator used to run databricks job](https://hevodata.com/learn/airflow-databricks/)\n",
    "# <!-- https://hevodata.com/learn/airflow-databricks/ -->\n",
    "# Runs an existing Spark job run to Databricks using the\n",
    "\n",
    "# <!-- https://hevodata.com/learn/airflow-databricks/ -->\n",
    "# Runs an existing Spark job run to Databricks using the\n",
    "\n",
    "notebook_params = {\n",
    "    \"Variable\":5\n",
    "    }\n",
    "\n",
    "notebook_run = DatabricksRunNowOperator(\n",
    "    job_id=job_id,\n",
    "    notebook_params=notebook_params,\n",
    "    python_params=python_params,\n",
    "    jar_params=jar_params,\n",
    "    spark_submit_params=spark_submit_params\n",
    ")\n",
    "\n",
    "\n",
    "opr_submit_run = DatabricksSubmitRunOperator(\n",
    "task_id='submit_run',\n",
    "databricks_conn_id='databricks',\n",
    "new_cluster=new_cluster,\n",
    "notebook_task=notebook_task\n",
    ")\n",
    "\n",
    "\n",
    "opr_run_now = DatabricksRunNowOperator(\n",
    "task_id='run_now',\n",
    "databricks_conn_id='databricks',\n",
    "job_id=5,\n",
    "notebook_params=notebook_params\n",
    ")\n",
    "notebook_params = {\n",
    "    \"Variable\":5\n",
    "    }\n",
    "\n",
    "notebook_run = DatabricksRunNowOperator(\n",
    "    job_id=job_id,\n",
    "    notebook_params=notebook_params,\n",
    "    python_params=python_params,\n",
    "    jar_params=jar_params,\n",
    "    spark_submit_params=spark_submit_params\n",
    ")\n",
    "\n",
    "\n",
    "opr_submit_run = DatabricksSubmitRunOperator(\n",
    "task_id='submit_run',\n",
    "databricks_conn_id='databricks',\n",
    "new_cluster=new_cluster,\n",
    "notebook_task=notebook_task\n",
    ")\n",
    "\n",
    "\n",
    "opr_run_now = DatabricksRunNowOperator(\n",
    "task_id='run_now',\n",
    "databricks_conn_id='databricks',\n",
    "job_id=5,\n",
    "notebook_params=notebook_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to get job parameter  in notebook\n",
    "\n",
    "- using dbutils.fs.get('variable_name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbt operator in airflow\n",
    "\n",
    "There are five operators currently implemented:\n",
    "DbtDocsGenerateOperator. Calls dbt docs generate.\n",
    "DbtDepsOperator. Calls dbt deps.\n",
    "DbtSeedOperator. Calls dbt seed.\n",
    "DbtSnapshotOperator. Calls dbt snapshot.\n",
    "DbtRunOperator. Calls dbt run.\n",
    "DbtTestOperator. Calls dbt test.\n",
    "DbtCleanOperator. Calls dbt clean.\n",
    "\n",
    "dbt_run = DbtRunOperator(\n",
    "  task_id='dbt_run',\n",
    "  dbt_bin='/usr/local/airflow/.local/bin/dbt',\n",
    "  profiles_dir='/usr/local/airflow/dags/{DBT_FOLDER}/',\n",
    "  dir='/usr/local/airflow/dags/{DBT_FOLDER}/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## airflow logs\n",
    "we have configure logs dir with s3 so we are storing logs on s3 location\n",
    "\n",
    "##### Q:2) Hooks\n",
    "Hooks are Airflow's way of interfacing with third-party systems. They allow you to connect to external APIs and databases like Hive, S3, GCS, MySQL, Postgres, etc. \n",
    "\n",
    "\n",
    "##### Q:3)What are the components used by Airflow?\n",
    "webserver => used for tracking the status of our jobs .\n",
    "\n",
    "scheduler => used for scheduling our job and is a multithreaded python processs which use dags objects\n",
    "\n",
    "Executor => are used for task done\n",
    "\n",
    "Metadata Database => used for storing the Airflow States.\n",
    "\n",
    "##### Q:4) What are the Types of Executor in Apache Airflow?\n",
    "\n",
    "Local Executor=>Helps in running multiple tasks at one time.\n",
    "\n",
    "Sequential Executor=>Helps by running only one task at a time.\n",
    "\n",
    "Celery Executor=>Helps by running distributed asynchronous Python Tasks.\n",
    "\n",
    "Kubernetes Executor=>Helps in running tasks in an individual kubernet pod. \n",
    "\n",
    "##### Q:5)What is XComs? \n",
    "XComs(Cross Communication) means a mechanism that lets tasks talk with each other, for sharing information between two task by using xcom\n",
    "\n",
    "xcom.push()\n",
    "xcom.pull()\n",
    "\n",
    "##### Q:6) What are Jinja Templates?\n",
    " A jinja template is simply a text file that contains the following: variables and/or expressions - these get replaced with values when a template is rendered.\n",
    "\n",
    "##### Q:7) What is DAG in Airflow?\n",
    "Directed Acyclic Graph(DAG) helps in maintaining tasks dependencies at a particular time.\n",
    "It is also used for maintaining tasks relations and for ensuring those tasks are executed in an expected Order. \n",
    "\n",
    "##### Q:9) How do we Instantiate a DAG?\n",
    "using below command to instantiate a dag\n",
    "```\n",
    "    dag = DAG(\n",
    "    'tutorial', default_args=default_args, schedule_interval=timedelta(days=10))\n",
    "```\n",
    "\n",
    "##### Q:10) How can we delete a DAG?\n",
    "We can Delete a DAG by using the following command:\n",
    "For CLI\n",
    "```\n",
    "airflow delete_dag my_dag_id\n",
    "```\n",
    "##### Q:12) airflow catchup vs backfill\n",
    "Backfill and Catchup are the same thing 1. If the catchup parameter is set to True in your DAG arguments, then the Airflow scheduler will perform Backfill,\n",
    " i.e. it will perform all the missing DAG Runs between your start_date and your potential end_date \n",
    "\n",
    "\n",
    "##### Q:13) [how to create custom operator in airflow]()\n",
    "You can create any operator you want by extending the \n",
    "```\n",
    "airflow.models.baseoperator.BaseOperator\n",
    "```\n",
    "\n",
    "##### Q:14) Sensors\n",
    "Sensors are a special type of Operator that are designed to do exactly one thing - wait for something to occur. It can be time-based, or waiting for a file, or an external event, but all they do is wait until something happens, and then succeed so their downstream tasks can run.\n",
    "\n",
    "\n",
    "##### Q:15) how to send alert message to slack\n",
    "```\n",
    "from airflow.contrib.operators.slack_webhook_operator import SlackWebhookOperator\n",
    "\n",
    "slack_msg = \"Hi Wssup?\"\n",
    "\n",
    "slack_test =  SlackWebhookOperator(\n",
    "        task_id='slack_test',\n",
    "        http_conn_id='slack_connection',\n",
    "        webhook_token='/1234/abcd',\n",
    "        message=slack_msg,\n",
    "        channel='#airflow_updates',\n",
    "        username='airflow_'+os.environ['ENVIRONMENT'],\n",
    "        icon_emoji=None,\n",
    "        link_names=False,\n",
    "        dag=dag)\n",
    "```\n",
    "\n",
    "##### Q:16) what is airflow pool\n",
    "Airflow pools can be used to limit the execution parallelism on arbitrary sets of tasks. The list of pools is managed in the UI ( Menu -> Admin -> Pools ) by giving the pools a name and assigning it a number of worker slots. ... The number of slots occupied by a task can be configured by pool_slots .\n",
    "\n",
    "\n",
    "##### Q:17) [airflow cli commands](https://airflow.apache.org/docs/apache-airflow/1.10.5/cli.html)\n",
    " https://airflow.readthedocs.io/en/1.10.14/cli-ref.html\n",
    "```\n",
    "$ airflow webserver -p 5000\n",
    "\n",
    "$ airflow users create  --username admin --firstname rahul --lastname pandit --role Admin --email rahul@metaorigins.com\n",
    "\n",
    "$ airflow scheduler\n",
    "\n",
    "$ airflow list_dags\n",
    "\n",
    "$  airflow list_tasks [-h] [-t] [-sd SUBDIR] dag_id\n",
    "\n",
    "\n",
    "$ airflow tasks test sql_dag_client task1 2015-01-01\n",
    "\n",
    "$ airflow trigger_dag [-h] [-sd SUBDIR] [-r RUN_ID] [-c CONF] [-e EXEC_DATE]\n",
    "                    dag_id\n",
    "\n",
    "$ airflow render [-h] [-sd SUBDIR] dag_id task_id execution_date\n",
    "\n",
    "$ db reset will delete all entries from the metadata database. This includes all dag runs, Variables and Connections.\n",
    "\n",
    "$ db init is only run once, when airflow is installed.\n",
    "\n",
    "$ airflow show_dag example_bash_operator --imgcat\n",
    "```\n",
    "\n",
    "##### Q:) how to call task using xcom\n",
    "    params={'s3_path_filename': \"{{ ti.xcom_pull(task_ids=submit_file_to_spark) }}\" },\n",
    "\n",
    "\n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "##### Q:) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
