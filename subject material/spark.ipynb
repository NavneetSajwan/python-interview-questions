{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you solve Data skew issue?\n",
    "\n",
    "- pareto principle:  80 percent of data comes from 20 percent of users\n",
    "- run a spark job to get a summary of the data\n",
    "- changed \n",
    "-  two man ways to solve it:\n",
    "    - use different key\n",
    "    - partition data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Apache Spark, and how does it differ from Hadoop MapReduce?\n",
    "\n",
    "Apache Spark is an open-source, distributed data processing framework designed for big data and analytics. \n",
    "\n",
    "In-Memory Processing: Spark stores data in-memory, which allows for faster data processing compared to Hadoop MapReduce, which primarily relies on disk storage. This in-memory processing capability is particularly advantageous for iterative algorithms and interactive data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark architecture:\n",
    "\n",
    "Apache Spark has a distributed architecture designed to process large-scale data across a cluster of machines efficiently. Its architecture consists of several key components that work together to perform distributed data processing and computation. Here are the main components of the Apache Spark architecture:\n",
    "\n",
    "![architecture](https://static.javatpoint.com/tutorial/spark/images/spark-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Driver Program:\n",
    "   - The driver program is the entry point for a Spark application. It runs the user's main function and creates a SparkContext to coordinate the execution of tasks across the cluster.\n",
    "   - The driver program is responsible for defining the application and its execution plan, and it manages the overall control flow.\n",
    "\n",
    "2. SparkContext:\n",
    "   - SparkContext is the heart of a Spark application. It coordinates the execution of tasks across a cluster and manages the cluster resources.\n",
    "   - SparkContext is responsible for setting up the application, connecting to the cluster manager, and distributing tasks to worker nodes.\n",
    "   - It also manages the configuration and controls the parallelism of data processing tasks.\n",
    "\n",
    "3. Cluster Manager:\n",
    "   - The cluster manager is responsible for allocating and managing resources in the cluster. Apache Spark supports various cluster managers like Apache Mesos, Hadoop YARN, and its built-in cluster manager.\n",
    "   - The cluster manager ensures that resources are allocated to the Spark application's tasks and that the application runs efficiently.\n",
    "\n",
    "4. Executors:\n",
    "   - Executors are worker nodes in the Spark cluster that run tasks on behalf of the driver program.\n",
    "   - Each executor runs in its own JVM (Java Virtual Machine) and is responsible for executing tasks and caching data in memory for fast access.\n",
    "   - Executors communicate with the driver program and the cluster manager to receive tasks and report status.\n",
    "\n",
    "5. RDD (Resilient Distributed Dataset):\n",
    "   - RDD is the fundamental data structure in Spark, representing a distributed collection of data that can be processed in parallel.\n",
    "   - RDDs are fault-tolerant, distributed, and immutable, and they can be cached in memory for faster access.\n",
    "   - Spark applications perform transformations and actions on RDDs to process data.\n",
    "\n",
    "6. Spark Core:\n",
    "   - Spark Core is the foundation of the Spark framework, providing essential functionalities like task scheduling, memory management, and fault recovery.\n",
    "   - It includes the core APIs for working with RDDs and offers the basic building blocks for distributed data processing.\n",
    "\n",
    "7. Libraries and APIs:\n",
    "   - Spark provides various libraries and APIs for different data processing tasks, including:\n",
    "     - Spark SQL: For structured data processing using SQL queries.\n",
    "     - Spark Streaming: For processing real-time data streams.\n",
    "     - MLlib: For machine learning tasks.\n",
    "     - GraphX: For graph processing.\n",
    "     - SparkR: For R language integration.\n",
    "\n",
    "8. Cluster Mode:\n",
    "   - Spark applications can run in different cluster modes, including standalone, Mesos, and YARN, allowing users to choose the cluster manager that best fits their requirements.\n",
    "\n",
    "9. Storage Systems:\n",
    "   - Spark can read and write data from/to various storage systems, including HDFS, Apache Cassandra, HBase, Amazon S3, and more.\n",
    "\n",
    "These components work together to enable distributed data processing, fault tolerance, and in-memory computing, making Apache Spark a powerful framework for big data analytics and processing. The flexibility and scalability of Spark's architecture make it suitable for a wide range of data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
