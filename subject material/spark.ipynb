{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you solve Data skew issue?\n",
    "\n",
    "- pareto principle:  80 percent of data comes from 20 percent of users\n",
    "- run a spark job to get a summary of the data\n",
    "- changed \n",
    "-  two man ways to solve it:\n",
    "    - use different key\n",
    "    - partition data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Apache Spark, and how does it differ from Hadoop MapReduce?\n",
    "\n",
    "Apache Spark is an open-source, distributed data processing framework designed for big data and analytics. \n",
    "\n",
    "In-Memory Processing: Spark stores data in-memory, which allows for faster data processing compared to Hadoop MapReduce, which primarily relies on disk storage. This in-memory processing capability is particularly advantageous for iterative algorithms and interactive data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark architecture:\n",
    "\n",
    "Apache Spark has a distributed architecture designed to process large-scale data across a cluster of machines efficiently. Its architecture consists of several key components that work together to perform distributed data processing and computation. Here are the main components of the Apache Spark architecture:\n",
    "\n",
    "![architecture](https://static.javatpoint.com/tutorial/spark/images/spark-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Driver Program:\n",
    "   - The driver program is the entry point for a Spark application. It runs the user's main function and creates a SparkContext to coordinate the execution of tasks across the cluster.\n",
    "   - The driver program is responsible for defining the application and its execution plan, and it manages the overall control flow.\n",
    "\n",
    "2. SparkContext:\n",
    "   - SparkContext is the heart of a Spark application. It coordinates the execution of tasks across a cluster and manages the cluster resources.\n",
    "   - SparkContext is responsible for setting up the application, connecting to the cluster manager, and distributing tasks to worker nodes.\n",
    "   - It also manages the configuration and controls the parallelism of data processing tasks.\n",
    "\n",
    "3. Cluster Manager:\n",
    "   - The cluster manager is responsible for allocating and managing resources in the cluster. Apache Spark supports various cluster managers like Apache Mesos, Hadoop YARN, and its built-in cluster manager.\n",
    "   - The cluster manager ensures that resources are allocated to the Spark application's tasks and that the application runs efficiently.\n",
    "\n",
    "4. Executors:\n",
    "   - Executors are worker nodes in the Spark cluster that run tasks on behalf of the driver program.\n",
    "   - Each executor runs in its own JVM (Java Virtual Machine) and is responsible for executing tasks and caching data in memory for fast access.\n",
    "   - Executors communicate with the driver program and the cluster manager to receive tasks and report status.\n",
    "\n",
    "5. RDD (Resilient Distributed Dataset):\n",
    "   - RDD is the fundamental data structure in Spark, representing a distributed collection of data that can be processed in parallel.\n",
    "   - RDDs are fault-tolerant, distributed, and immutable, and they can be cached in memory for faster access.\n",
    "   - Spark applications perform transformations and actions on RDDs to process data.\n",
    "\n",
    "6. Spark Core:\n",
    "   - Spark Core is the foundation of the Spark framework, providing essential functionalities like task scheduling, memory management, and fault recovery.\n",
    "   - It includes the core APIs for working with RDDs and offers the basic building blocks for distributed data processing.\n",
    "\n",
    "7. Libraries and APIs:\n",
    "   - Spark provides various libraries and APIs for different data processing tasks, including:\n",
    "     - Spark SQL: For structured data processing using SQL queries.\n",
    "     - Spark Streaming: For processing real-time data streams.\n",
    "     - MLlib: For machine learning tasks.\n",
    "     - GraphX: For graph processing.\n",
    "     - SparkR: For R language integration.\n",
    "\n",
    "8. Cluster Mode:\n",
    "   - Spark applications can run in different cluster modes, including standalone, Mesos, and YARN, allowing users to choose the cluster manager that best fits their requirements.\n",
    "\n",
    "9. Storage Systems:\n",
    "   - Spark can read and write data from/to various storage systems, including HDFS, Apache Cassandra, HBase, Amazon S3, and more.\n",
    "\n",
    "These components work together to enable distributed data processing, fault tolerance, and in-memory computing, making Apache Spark a powerful framework for big data analytics and processing. The flexibility and scalability of Spark's architecture make it suitable for a wide range of data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a broadcast join in spark?\n",
    "\n",
    "- for optimization we can broadacast a smaller dataframe to al the nodes in the cluster and then perform the join. This is called a broadcast join\n",
    "- `large_df.join(broadcast(small_df), \"id\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a braodcast variable in spark?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what are wide and narrow transformations in spark?\n",
    "\n",
    "\n",
    "##### Narrow transformations:\n",
    "- Narrow transformations are the transformations where each input partition contributes to only one output partition\n",
    "- example:  `map`, `filter`, `union`\n",
    "\n",
    "###### Wide transformations:\n",
    "- Wide transformations are transformations that require data to be shuffled between partitions. Each output partition depends on multiple input \n",
    "- `join`, `grouping` , `reduceby key`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is broadcast variable:\n",
    "\n",
    "- A broadcast variable is a read-only variable cached on each machine in the Spark cluster, rather than shipping a copy of it with tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/08 17:44:20 WARN Utils: Your hostname, navneetsajwan-ThinkPad-L480 resolves to a loopback address: 127.0.1.1; using 172.20.10.3 instead (on interface wlp5s0)\n",
      "23/12/08 17:44:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/navneetsajwan/miniconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/12/08 17:44:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a Spark context\n",
    "sc = SparkContext(\"local\", \"Broadcast Example\")\n",
    "\n",
    "# Create a large read-only variable\n",
    "large_variable = range(1, 1000)\n",
    "\n",
    "# Broadcast the variable to all worker nodes\n",
    "broadcast_variable = sc.broadcast(large_variable)\n",
    "\n",
    "# Define a function that uses the broadcast variable\n",
    "def process_data(x):\n",
    "    # Access the broadcast variable locally\n",
    "    local_data = broadcast_variable.value\n",
    "    return x * local_data[0]\n",
    "\n",
    "# Create an RDD\n",
    "data = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Use the broadcast variable in a Spark transformation\n",
    "result = data.map(process_data)\n",
    "\n",
    "# Collect the results\n",
    "print(result.collect())\n",
    "\n",
    "# Stop the Spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is sc.parallelize in spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creates a RDD\n",
    "- automatically distributes data on the cluster\n",
    "- transformations can be perfoormed parallely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD vs dataframe:\n",
    "\n",
    "- RDD:\n",
    "    - low level adn more flexible\n",
    "    - less performant due to manual optimizations\n",
    "\n",
    "- Dataframe:\n",
    "    - High level and easy to understand\n",
    "    - high performant due to automatic optimiation\n",
    "    - uses spark's catalyst optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is accumulator in spark?\n",
    "\n",
    "- An accumulator is a variable that can be used to accumulate values across multiple tasks in a parallel and fault-tolerant manner.\n",
    "- Required where the result of a computation needs to be efficiently shared across multiple tasks running on different nodes of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a  simple example in PySpark using an accumulator to count the number of elements in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Count: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Accumulator Example\")\n",
    "\n",
    "# Create an accumulator with an initial value of 0\n",
    "accumulator = sc.accumulator(0)\n",
    "\n",
    "# Create an RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Define a function to update the accumulator\n",
    "def process_data(x):\n",
    "    global accumulator\n",
    "    accumulator += 1\n",
    "    return x\n",
    "\n",
    "# Use the accumulator in a Spark transformation\n",
    "result = rdd.map(process_data)\n",
    "\n",
    "# Perform an action to trigger the execution\n",
    "result.collect()\n",
    "\n",
    "# Access the final value of the accumulator in the driver program\n",
    "final_count = accumulator.value\n",
    "print(\"Final Count:\", final_count)\n",
    "\n",
    "# Stop the Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "#create sparkcontext object\n",
    "sc= SparkContext(\"local\", \"Acculmulator example\")\n",
    "\n",
    "# Create an accumulator with an initial value of 0\n",
    "accumulator = sc.accumulator(0)\n",
    "\n",
    "data = [1,2,3,4,5]\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(x):\n",
    "    global accumulator\n",
    "    accumulator+=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rdd.map(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Count: 5\n"
     ]
    }
   ],
   "source": [
    "# Access the final value of the accumulator in the driver program\n",
    "final_count = accumulator.value\n",
    "print(\"Final Count:\", final_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What s udf in spark?\n",
    "\n",
    " User-Defined Function (UDF) refers to a feature that allows you to define your own functions for use in Spark SQL or DataFrame API operations. UDFs enable you to apply custom, user-defined logic to the data in a distributed and parallelized manner across a Spark cluster.\n",
    "\n",
    " - define\n",
    " - register\n",
    " - use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a UDF in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------+\n",
      "|   Name|Value|SquaredValue|\n",
      "+-------+-----+------------+\n",
      "|  Alice|    1|           1|\n",
      "|    Bob|    2|           4|\n",
      "|Charlie|    3|           9|\n",
      "+-------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"UDF Example\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "columns = [\"Name\", \"Value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a UDF to square a number\n",
    "@udf(IntegerType()) # retrun type should be integer\n",
    "def square_udf(value):\n",
    "    return value ** 2\n",
    "\n",
    "# Register the UDF\n",
    "spark.udf.register(\"square\", square_udf)\n",
    "\n",
    "# Use the UDF in a DataFrame transformation\n",
    "result_df = df.withColumn(\"SquaredValue\", square_udf(df[\"Value\"]))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
