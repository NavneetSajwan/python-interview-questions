{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"data_skew\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Set Spark parameters - We have to turn off AQL to demonstrate Salting\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the example dataset of fact and dimension we would use for demonstration\n",
    "# Python program to generate random Fact table data\n",
    "# [1, ,\"ORD1001\", \"D102\", 56]\n",
    "import random\n",
    "def generate_fact_data(counter=100):\n",
    "    fact_records = []\n",
    "    dim_keys = [\"D100\", \"D101\", \"D102\", \"D103\", \"D104\"]\n",
    "    order_ids = [\"ORD\" + str(i) for i in range(1001, 1010)]\n",
    "    qty_range = [i for i in range(10, 120)]\n",
    "    for i in range(counter):\n",
    "        _record = [i, random.choice(order_ids), random.choice(dim_keys), random.choice(qty_range)]\n",
    "        fact_records.append(_record)\n",
    "    return fact_records\n",
    "\n",
    "# We will generate 200 records with random data in Fact to create skewness\n",
    "fact_records = generate_fact_data(200)\n",
    "dim_records = [\n",
    "    [\"D100\", \"Product A\"],\n",
    "    [\"D101\", \"Product B\"],\n",
    "    [\"D102\", \"Product C\"],\n",
    "    [\"D103\", \"Product D\"],\n",
    "    [\"D104\", \"Product E\"]\n",
    "]\n",
    "_fact_cols = [\"id\", \"order_id\", \"prod_id\", \"qty\"]\n",
    "_dim_cols = [\"prod_id\", \"prod_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      "\n",
      "+---+--------+-------+---+\n",
      "|id |order_id|prod_id|qty|\n",
      "+---+--------+-------+---+\n",
      "|0  |ORD1002 |D104   |23 |\n",
      "|1  |ORD1006 |D104   |95 |\n",
      "|2  |ORD1005 |D103   |14 |\n",
      "|3  |ORD1008 |D103   |31 |\n",
      "|4  |ORD1007 |D103   |16 |\n",
      "|5  |ORD1008 |D102   |106|\n",
      "|6  |ORD1007 |D101   |14 |\n",
      "|7  |ORD1009 |D101   |24 |\n",
      "|8  |ORD1003 |D104   |80 |\n",
      "|9  |ORD1004 |D102   |17 |\n",
      "+---+--------+-------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Fact Data Frame\n",
    "fact_df = spark.createDataFrame(data = fact_records, schema=_fact_cols)\n",
    "fact_df.printSchema()\n",
    "fact_df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|prod_id|count(id)|\n",
      "+-------+---------+\n",
      "|   D103|       40|\n",
      "|   D104|       44|\n",
      "|   D100|       41|\n",
      "|   D101|       34|\n",
      "|   D102|       41|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.groupBy(\"prod_id\").agg(count(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- prod_name: string (nullable = true)\n",
      "\n",
      "+-------+---------+\n",
      "|prod_id|prod_name|\n",
      "+-------+---------+\n",
      "|D100   |Product A|\n",
      "|D101   |Product B|\n",
      "|D102   |Product C|\n",
      "|D103   |Product D|\n",
      "|D104   |Product E|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Prod Dim Data Frame\n",
    "dim_df = spark.createDataFrame(data = dim_records, schema=_dim_cols)\n",
    "dim_df.printSchema()\n",
    "dim_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Spark parameters - We have to turn off AQL to demonstrate Salting\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+---+---------+\n",
      "|prod_id|id |order_id|qty|prod_name|\n",
      "+-------+---+--------+---+---------+\n",
      "|D103   |2  |ORD1005 |14 |Product D|\n",
      "|D103   |3  |ORD1008 |31 |Product D|\n",
      "|D103   |4  |ORD1007 |16 |Product D|\n",
      "|D103   |29 |ORD1006 |21 |Product D|\n",
      "|D103   |33 |ORD1001 |27 |Product D|\n",
      "|D103   |52 |ORD1008 |78 |Product D|\n",
      "|D103   |53 |ORD1009 |94 |Product D|\n",
      "|D103   |77 |ORD1006 |63 |Product D|\n",
      "|D103   |80 |ORD1003 |88 |Product D|\n",
      "|D103   |103|ORD1008 |50 |Product D|\n",
      "+-------+---+--------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets join the fact and dim without salting\n",
    "joined_df = fact_df.join(dim_df, on=\"prod_id\", how=\"leftouter\")\n",
    "joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|partition_num|count(id)|\n",
      "+-------------+---------+\n",
      "|            4|      116|\n",
      "|            2|       84|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "partition_df = joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(\"id\"))\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "# UDF to return a random number every time\n",
    "def rand(): return random.randint(0, 4) #Since we are distributing the data in 5 partitions\n",
    "rand_udf = udf(rand)\n",
    "# Salt Data Frame to add to dimension\n",
    "salt_df = spark.range(0, 5)\n",
    "salt_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---+--------------+\n",
      "|id |order_id|prod_id|qty|salted_prod_id|\n",
      "+---+--------+-------+---+--------------+\n",
      "|0  |ORD1002 |D104   |23 |D104_2        |\n",
      "|1  |ORD1006 |D104   |95 |D104_1        |\n",
      "|2  |ORD1005 |D103   |14 |D103_0        |\n",
      "|3  |ORD1008 |D103   |31 |D103_0        |\n",
      "|4  |ORD1007 |D103   |16 |D103_3        |\n",
      "|5  |ORD1008 |D102   |106|D102_3        |\n",
      "|6  |ORD1007 |D101   |14 |D101_3        |\n",
      "|7  |ORD1009 |D101   |24 |D101_2        |\n",
      "|8  |ORD1003 |D104   |80 |D104_4        |\n",
      "|9  |ORD1004 |D102   |17 |D102_1        |\n",
      "+---+--------+-------+---+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted Fact\n",
    "from pyspark.sql.functions import lit, expr, concat\n",
    "salted_fact_df = fact_df.withColumn(\"salted_prod_id\", concat(\"prod_id\",lit(\"_\"), lit(rand_udf())))\n",
    "salted_fact_df.show(10, False)\n",
    "\n",
    "# adding salt id sampled from a uniform distribution to a bunch of product_ids creates a uniformly distributed product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+\n",
      "|prod_id|prod_name|salted_prod_id|\n",
      "+-------+---------+--------------+\n",
      "|   D100|Product A|        D100_0|\n",
      "|   D100|Product A|        D100_1|\n",
      "|   D100|Product A|        D100_2|\n",
      "|   D100|Product A|        D100_3|\n",
      "|   D100|Product A|        D100_4|\n",
      "|   D101|Product B|        D101_0|\n",
      "|   D101|Product B|        D101_1|\n",
      "|   D101|Product B|        D101_2|\n",
      "|   D101|Product B|        D101_3|\n",
      "|   D101|Product B|        D101_4|\n",
      "|   D102|Product C|        D102_0|\n",
      "|   D102|Product C|        D102_1|\n",
      "|   D102|Product C|        D102_2|\n",
      "|   D102|Product C|        D102_3|\n",
      "|   D102|Product C|        D102_4|\n",
      "|   D103|Product D|        D103_0|\n",
      "|   D103|Product D|        D103_1|\n",
      "|   D103|Product D|        D103_2|\n",
      "|   D103|Product D|        D103_3|\n",
      "|   D103|Product D|        D103_4|\n",
      "+-------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted DIM\n",
    "salted_dim_df = dim_df.join(salt_df, how=\"cross\").withColumn(\"salted_prod_id\", concat(\"prod_id\", lit(\"_\"), \"id\")).drop(\"id\")\n",
    "salted_dim_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "|salted_prod_id|id |order_id|prod_id|qty|prod_id|prod_name|\n",
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "|D100_0        |75 |ORD1002 |D100   |64 |D100   |Product A|\n",
      "|D100_0        |79 |ORD1008 |D100   |82 |D100   |Product A|\n",
      "|D100_0        |85 |ORD1006 |D100   |44 |D100   |Product A|\n",
      "|D100_0        |100|ORD1002 |D100   |39 |D100   |Product A|\n",
      "|D100_0        |125|ORD1006 |D100   |99 |D100   |Product A|\n",
      "|D100_0        |129|ORD1005 |D100   |88 |D100   |Product A|\n",
      "|D100_1        |126|ORD1007 |D100   |68 |D100   |Product A|\n",
      "|D100_1        |182|ORD1004 |D100   |90 |D100   |Product A|\n",
      "|D104_1        |8  |ORD1003 |D104   |80 |D104   |Product E|\n",
      "|D104_1        |82 |ORD1003 |D104   |84 |D104   |Product E|\n",
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets make the salted join now\n",
    "salted_joined_df = salted_fact_df.join(salted_dim_df, on=\"salted_prod_id\", how=\"leftouter\")\n",
    "salted_joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|salted_prod_id|count(id)|\n",
      "+--------------+---------+\n",
      "|        D100_0|        7|\n",
      "|        D100_1|       10|\n",
      "|        D104_1|       13|\n",
      "|        D101_1|        7|\n",
      "|        D103_1|       11|\n",
      "|        D103_2|        7|\n",
      "|        D103_3|       11|\n",
      "|        D104_0|       10|\n",
      "|        D104_2|        6|\n",
      "|        D104_3|        7|\n",
      "|        D100_2|        8|\n",
      "|        D102_1|        7|\n",
      "|        D104_4|        8|\n",
      "|        D101_0|        4|\n",
      "|        D101_2|        6|\n",
      "|        D101_4|        7|\n",
      "|        D102_0|        6|\n",
      "|        D102_2|        9|\n",
      "|        D102_3|       11|\n",
      "|        D102_4|        8|\n",
      "+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salted_joined_df.groupBy(\"salted_prod_id\").agg(count(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|partition_num|count|\n",
      "+-------------+-----+\n",
      "|            0|   24|\n",
      "|            1|   55|\n",
      "|            2|   26|\n",
      "|            3|   66|\n",
      "|            4|   29|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "partition_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\") \\\n",
    "    .agg(count(lit(1)).alias(\"count\")).orderBy(\"partition_num\")\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
