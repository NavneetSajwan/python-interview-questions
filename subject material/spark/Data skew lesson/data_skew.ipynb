{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/09 12:59:48 WARN Utils: Your hostname, Navneets-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.20.10.7 instead (on interface en0)\n",
      "24/02/09 12:59:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/09 12:59:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/09 12:59:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"abc\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Set Spark parameters - We have to turn off AQL to demonstrate Salting\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the example dataset of fact and dimension we would use for demonstration\n",
    "# Python program to generate random Fact table data\n",
    "# [1, ,\"ORD1001\", \"D102\", 56]\n",
    "import random\n",
    "def generate_fact_data(counter=100):\n",
    "    fact_records = []\n",
    "    dim_keys = [\"D100\", \"D101\", \"D102\", \"D103\", \"D104\"]\n",
    "    order_ids = [\"ORD\" + str(i) for i in range(1001, 1010)]\n",
    "    qty_range = [i for i in range(10, 120)]\n",
    "    for i in range(counter):\n",
    "        _record = [i, random.choice(order_ids), random.choice(dim_keys), random.choice(qty_range)]\n",
    "        fact_records.append(_record)\n",
    "    return fact_records\n",
    "\n",
    "# We will generate 200 records with random data in Fact to create skewness\n",
    "fact_records = generate_fact_data(200)\n",
    "dim_records = [\n",
    "    [\"D100\", \"Product A\"],\n",
    "    [\"D101\", \"Product B\"],\n",
    "    [\"D102\", \"Product C\"],\n",
    "    [\"D103\", \"Product D\"],\n",
    "    [\"D104\", \"Product E\"]\n",
    "]\n",
    "_fact_cols = [\"id\", \"order_id\", \"prod_id\", \"qty\"]\n",
    "_dim_cols = [\"prod_id\", \"prod_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---+\n",
      "|id |order_id|prod_id|qty|\n",
      "+---+--------+-------+---+\n",
      "|0  |ORD1006 |D100   |44 |\n",
      "|1  |ORD1001 |D103   |40 |\n",
      "|2  |ORD1007 |D100   |37 |\n",
      "|3  |ORD1005 |D104   |96 |\n",
      "|4  |ORD1009 |D103   |104|\n",
      "|5  |ORD1003 |D101   |115|\n",
      "|6  |ORD1003 |D100   |12 |\n",
      "|7  |ORD1006 |D101   |55 |\n",
      "|8  |ORD1001 |D101   |88 |\n",
      "|9  |ORD1007 |D104   |60 |\n",
      "+---+--------+-------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Fact Data Frame\n",
    "fact_df = spark.createDataFrame(data = fact_records, schema=_fact_cols)\n",
    "fact_df.printSchema()\n",
    "fact_df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|prod_id|id_count|\n",
      "+-------+--------+\n",
      "|   D103|      32|\n",
      "|   D104|      43|\n",
      "|   D100|      40|\n",
      "|   D101|      39|\n",
      "|   D102|      46|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_df.groupBy('prod_id').agg(count('id').alias('id_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- prod_name: string (nullable = true)\n",
      "\n",
      "+-------+---------+\n",
      "|prod_id|prod_name|\n",
      "+-------+---------+\n",
      "|D100   |Product A|\n",
      "|D101   |Product B|\n",
      "|D102   |Product C|\n",
      "|D103   |Product D|\n",
      "|D104   |Product E|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Prod Dim Data Frame\n",
    "dim_df = spark.createDataFrame(data = dim_records, schema=_dim_cols)\n",
    "dim_df.printSchema()\n",
    "dim_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Set Spark parameters - We have to turn off AQL to demonstrate Salting\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+---+---------+\n",
      "|prod_id|id |order_id|qty|prod_name|\n",
      "+-------+---+--------+---+---------+\n",
      "|D103   |1  |ORD1001 |40 |Product D|\n",
      "|D103   |4  |ORD1009 |104|Product D|\n",
      "|D103   |54 |ORD1008 |10 |Product D|\n",
      "|D103   |76 |ORD1001 |86 |Product D|\n",
      "|D103   |79 |ORD1008 |110|Product D|\n",
      "|D103   |81 |ORD1006 |11 |Product D|\n",
      "|D103   |84 |ORD1004 |106|Product D|\n",
      "|D103   |104|ORD1007 |63 |Product D|\n",
      "|D103   |110|ORD1008 |84 |Product D|\n",
      "|D103   |135|ORD1001 |51 |Product D|\n",
      "+-------+---+--------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets join the fact and dim without salting\n",
    "joined_df = fact_df.join(dim_df, on=\"prod_id\", how=\"leftouter\")\n",
    "joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|partition_num|count(id)|\n",
      "+-------------+---------+\n",
      "|            4|      125|\n",
      "|            2|       75|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "partition_df = joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(\"id\"))\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "# UDF to return a random number every time\n",
    "def rand(): return random.randint(0, 4) #Since we are distributing the data in 5 partitions\n",
    "rand_udf = udf(rand)\n",
    "# Salt Data Frame to add to dimension\n",
    "salt_df = spark.range(0, 5)\n",
    "salt_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---+--------------+\n",
      "|id |order_id|prod_id|qty|salted_prod_id|\n",
      "+---+--------+-------+---+--------------+\n",
      "|0  |ORD1006 |D100   |44 |D100_2        |\n",
      "|1  |ORD1001 |D103   |40 |D103_2        |\n",
      "|2  |ORD1007 |D100   |37 |D100_3        |\n",
      "|3  |ORD1005 |D104   |96 |D104_3        |\n",
      "|4  |ORD1009 |D103   |104|D103_3        |\n",
      "|5  |ORD1003 |D101   |115|D101_3        |\n",
      "|6  |ORD1003 |D100   |12 |D100_1        |\n",
      "|7  |ORD1006 |D101   |55 |D101_0        |\n",
      "|8  |ORD1001 |D101   |88 |D101_0        |\n",
      "|9  |ORD1007 |D104   |60 |D104_4        |\n",
      "+---+--------+-------+---+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted Fact\n",
    "from pyspark.sql.functions import lit, expr, concat\n",
    "salted_fact_df = fact_df.withColumn(\"salted_prod_id\", concat(\"prod_id\",lit(\"_\"), lit(rand_udf())))\n",
    "salted_fact_df.show(10, False)\n",
    "\n",
    "# adding salt id sampled from a uniform distribution to a bunch of product_ids creates a uniformly distributed product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+\n",
      "|prod_id|prod_name|salted_prod_id|\n",
      "+-------+---------+--------------+\n",
      "|   D100|Product A|        D100_0|\n",
      "|   D100|Product A|        D100_1|\n",
      "|   D100|Product A|        D100_2|\n",
      "|   D100|Product A|        D100_3|\n",
      "|   D100|Product A|        D100_4|\n",
      "|   D101|Product B|        D101_0|\n",
      "|   D101|Product B|        D101_1|\n",
      "|   D101|Product B|        D101_2|\n",
      "|   D101|Product B|        D101_3|\n",
      "|   D101|Product B|        D101_4|\n",
      "|   D102|Product C|        D102_0|\n",
      "|   D102|Product C|        D102_1|\n",
      "|   D102|Product C|        D102_2|\n",
      "|   D102|Product C|        D102_3|\n",
      "|   D102|Product C|        D102_4|\n",
      "|   D103|Product D|        D103_0|\n",
      "|   D103|Product D|        D103_1|\n",
      "|   D103|Product D|        D103_2|\n",
      "|   D103|Product D|        D103_3|\n",
      "|   D103|Product D|        D103_4|\n",
      "+-------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted DIM\n",
    "salted_dim_df = dim_df.join(salt_df, how=\"cross\").withColumn(\"salted_prod_id\", concat(\"prod_id\", lit(\"_\"), \"id\")).drop(\"id\")\n",
    "salted_dim_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "|salted_prod_id|id |order_id|prod_id|qty|prod_id|prod_name|\n",
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "|D100_0        |2  |ORD1007 |D100   |37 |D100   |Product A|\n",
      "|D100_0        |27 |ORD1006 |D100   |103|D100   |Product A|\n",
      "|D100_0        |80 |ORD1001 |D100   |29 |D100   |Product A|\n",
      "|D100_1        |0  |ORD1006 |D100   |44 |D100   |Product A|\n",
      "|D100_1        |52 |ORD1004 |D100   |92 |D100   |Product A|\n",
      "|D100_1        |85 |ORD1009 |D100   |115|D100   |Product A|\n",
      "|D100_1        |102|ORD1001 |D100   |10 |D100   |Product A|\n",
      "|D100_1        |179|ORD1006 |D100   |73 |D100   |Product A|\n",
      "|D100_1        |185|ORD1009 |D100   |105|D100   |Product A|\n",
      "|D104_1        |3  |ORD1005 |D104   |96 |D104   |Product E|\n",
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets make the salted join now\n",
    "salted_joined_df = salted_fact_df.join(salted_dim_df, on=\"salted_prod_id\", how=\"leftouter\")\n",
    "salted_joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|salted_prod_id|count(id)|\n",
      "+--------------+---------+\n",
      "|        D100_0|       13|\n",
      "|        D100_1|        5|\n",
      "|        D104_1|       11|\n",
      "|        D101_1|       11|\n",
      "|        D103_1|        4|\n",
      "|        D103_2|        3|\n",
      "|        D103_3|       13|\n",
      "|        D104_0|        9|\n",
      "|        D104_2|        5|\n",
      "|        D104_3|       10|\n",
      "|        D100_2|        9|\n",
      "|        D102_1|        8|\n",
      "|        D104_4|        8|\n",
      "|        D101_0|        6|\n",
      "|        D101_2|        5|\n",
      "|        D101_4|       14|\n",
      "|        D102_0|       13|\n",
      "|        D102_2|        9|\n",
      "|        D102_3|        7|\n",
      "|        D102_4|        9|\n",
      "+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salted_joined_df.groupBy(\"salted_prod_id\").agg(count(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|partition_num|count|\n",
      "+-------------+-----+\n",
      "|            0|   24|\n",
      "|            1|   55|\n",
      "|            2|   26|\n",
      "|            3|   66|\n",
      "|            4|   29|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "partition_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\") \\\n",
    "    .agg(count(lit(1)).alias(\"count\")).orderBy(\"partition_num\")\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(('a','b','c','d','e'),(1,2,3,4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1088809615.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    a=\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
