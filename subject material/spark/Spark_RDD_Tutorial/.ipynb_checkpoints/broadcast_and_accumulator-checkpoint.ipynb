{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a860e75f-3f00-4a37-9429-a5fbb2ea8138",
   "metadata": {},
   "source": [
    "## PySpark Broadcast Variables\n",
    "https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/ <br>\n",
    "\n",
    "In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fddd3bd-3944-4c72-80ac-6c9b6fce59f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/\n",
    "\"\"\"\n",
    "Let me explain with an example when to use broadcast variables,\n",
    "assume you are getting a two-letter country state code in a file \n",
    "and you wanted to transform it to full state name, (for example CA to California, NY to New York e.t.c)\n",
    "by doing a lookup to reference mapping. In some instances, this data could be large\n",
    "and you may have many such lookups (like zip code e.t.c).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "print(broadcastStates.value)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d998d-eb9a-4f8c-a39f-a82b024414e5",
   "metadata": {},
   "source": [
    "## diff between  Accumulator and Broadcast Variables\n",
    "\n",
    "An accumulator is also a variable that is broadcasted to the worker nodes. <br>\n",
    "The key difference between a broadcast variable and an accumulator is that while <br>\n",
    "the broadcast variable is read-only, the accumulator can be added to <br>\n",
    "\n",
    "Accumulator can be used to implement counters (as in MapReduce) or sums. <br>\n",
    "Spark natively supports accumulators of numeric types, and programmers can add support for new types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3e1d4-2eeb-4bcb-ab81-62090cf6ab5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575ef63d-e4bf-4a86-a494-785da90e3b22",
   "metadata": {},
   "source": [
    "## PySpark Accumulator with Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0089630-ad84-41e7-ad39-34b8a79fd0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accumulator= 10\n",
      "Accumulated value is -> 150\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num = sc.accumulator(10) \n",
    "print(\"accumulator=\",num)\n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "rdd.foreach(f) \n",
    "final = num.value \n",
    "print (\"Accumulated value is -> %i\" % (final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3396d8-f711-4fb4-8e2c-fdb22d41b2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
