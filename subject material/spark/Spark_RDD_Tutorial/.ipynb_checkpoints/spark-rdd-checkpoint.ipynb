{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3a35b7-5d67-47c1-b261-89681c975d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: inferSchema\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/08 14:24:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession. \\\n",
    "        builder. \\\n",
    "        master(\"local[5]\"). \\\n",
    "        appName(\"rdd tutorial\"). \\\n",
    "        config(\"inferSchema\" , \"True\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "# spark.conf.set(\"spark.default.parallelism\", \"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19cd537-993b-422d-9289-dfd8a2e18c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://worker01:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rdd tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f29597e2490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d498028-2302-4b18-bb73-9ccf415c3d39",
   "metadata": {},
   "source": [
    "## notes rdd how its work\n",
    "https://sparkbyexamples.com/pyspark-rdd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aed1568-0734-473b-afc6-b823c48fa01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark._sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f324ee6-88bc-45a2-8bd5-3ee9269ef323",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd=sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc97197-6a27-407d-81b5-afc94b9e361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "5 \n",
      "6 \n",
      "7 \n",
      "8 \n",
      "9 \n",
      "10 \n",
      "11 \n",
      "12 \n"
     ]
    }
   ],
   "source": [
    "for item in rdd.collect():\n",
    "    print(item,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac9a16-5ba5-4b8f-8047-132759c5599f",
   "metadata": {},
   "source": [
    "## read data from Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265dc4f5-a91b-47c8-b222-021acafd7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=sc.textFile('../README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86520261-a52d-4692-985f-94d95e8a1a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## spark coding links',\n",
       " '    https://sparkbyexamples.com/spark/spark-web-ui-understanding/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13036a9-4996-4719-9b96-a61b069dd570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4853d43f-0a0e-4ad7-9f33-3d707e19405f",
   "metadata": {},
   "source": [
    "### Create empty RDD with partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c434394a-b729-4cee-b9ca-9b35dbd65460",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13871ae-1795-4ef3-8bde-76b514b20cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5721f7-2b96-40d5-8398-21cac141eef1",
   "metadata": {},
   "source": [
    "## read data from dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5c06a0-ece2-4815-a1b4-b55171eb233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=sc.textFile('../data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41469e9f-ae16-4c49-be84-28d8ee909bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cloutcubeDeployment\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "tinagolo\n",
      "--------------------\n",
      "https://github.com/tiangolo/uwsgi-nginx-flask-docker\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "```\n",
      "--------------------\n",
      "version: '3'\n",
      "--------------------\n",
      "services:\n",
      "--------------------\n",
      "  web:\n",
      "--------------------\n",
      "    build: ./\n",
      "--------------------\n",
      "    network_mode: host\n",
      "--------------------\n",
      "    ports:\n",
      "--------------------\n",
      "      - \"80:80\"\n",
      "--------------------\n",
      " \n",
      "--------------------\n",
      "```\n",
      "--------------------\n",
      "Presently a Data Engineer with more than 2 years of hands-on experience in building ETL packages and engineering OLAP cubes, I’m an expert in implementing advanced algorithms and integrating them within project architecture, as well as developing applications against various NoSQL databases. I also re-designed a critical ingestion pipeline which increased the volume of processed data by 50%. This is why I am certain I make a perfect candidate for the Data Engineer position at [Company] and I am happy to officially submit my job application.\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "i am a Data Engineer,and also Working in Northwestern Mutual Projects ,my Role are to build and create Data Pipeline and Ingest the data from raw Area to Structure Area, i have build a databricks job via terraform in python the name of job is pdb-udp-data-pipeline and i have create a ci/cd pipeline for Automate Data Contract v2,and also build and deploy python sdk on NM Artifactory.\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "i wanna to move from india to us that's why i will require H-1B visa for work in office in NM\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Adventures in Wonderland\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "[{\n",
      "--------------------\n",
      "  \"RecordNumber\": 2,\n",
      "--------------------\n",
      "  \"Zipcode\": 704,\n",
      "--------------------\n",
      "  \"ZipCodeType\": \"STANDARD\",\n",
      "--------------------\n",
      "  \"City\": \"PASEO COSTA DEL SUR\",\n",
      "--------------------\n",
      "  \"State\": \"PR\"\n",
      "--------------------\n",
      "},\n",
      "--------------------\n",
      "{\n",
      "--------------------\n",
      "  \"RecordNumber\": 10,\n",
      "--------------------\n",
      "  \"Zipcode\": 709,\n",
      "--------------------\n",
      "  \"ZipCodeType\": \"STANDARD\",\n",
      "--------------------\n",
      "  \"City\": \"BDA SAN LUIS\",\n",
      "--------------------\n",
      "  \"State\": \"PR\"\n",
      "--------------------\n",
      "}]\n",
      "--------------------\n",
      "RecordNumber,Country,City,Zipcode,State\n",
      "--------------------\n",
      "1,US,PARC PARQUE,704,PR\n",
      "--------------------\n",
      "2,US,PASEO COSTA DEL SUR,704,PR\n",
      "--------------------\n",
      "10,US,BDA SAN LUIS,709,PR\n",
      "--------------------\n",
      "49347,US,HOLT,32564,FL\n",
      "--------------------\n",
      "49348,US,HOMOSASSA,34487,FL\n",
      "--------------------\n",
      "61391,US,CINGULAR WIRELESS,76166,TX\n",
      "--------------------\n",
      "61392,US,FORT WORTH,76177,TX\n",
      "--------------------\n",
      "61393,US,FT WORTH,76177,TX\n",
      "--------------------\n",
      "54356,US,SPRUCE PINE,35585,AL\n",
      "--------------------\n",
      "76511,US,ASH HILL,27007,NC\n",
      "--------------------\n",
      "4,US,URB EUGENE RICE,704,PR\n",
      "--------------------\n",
      "39827,US,MESA,85209,AZ\n",
      "--------------------\n",
      "39828,US,MESA,85210,AZ\n",
      "--------------------\n",
      "49345,US,HILLIARD,32046,FL\n",
      "--------------------\n",
      "49346,US,HOLDER,34445,FL\n",
      "--------------------\n",
      "3,US,SECT LANAUSSE,704,PR\n",
      "--------------------\n",
      "54354,US,SPRING GARDEN,36275,AL\n",
      "--------------------\n",
      "54355,US,SPRINGVILLE,35146,AL\n",
      "--------------------\n",
      "76512,US,ASHEBORO,27203,NC\n",
      "--------------------\n",
      "76513,US,ASHEBORO,27204,NC\n",
      "--------------------\n",
      "id,zipcode,type,city,state,population\n",
      "--------------------\n",
      "1,704,STANDARD,,PR,30100\n",
      "--------------------\n",
      "2,704,,PASEO COSTA DEL SUR,PR,\n",
      "--------------------\n",
      "3,709,,BDA SAN LUIS,PR,3700\n",
      "--------------------\n",
      "4,76166,UNIQUE,CINGULAR WIRELESS,TX,84000\n",
      "--------------------\n",
      "5,76177,STANDARD,,TX,\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Project Gutenberg’s\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "Alice’s Adventures in Wonderland\n",
      "--------------------\n",
      "by Lewis Carroll\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "This eBook is for the use\n",
      "--------------------\n",
      "of anyone anywhere\n",
      "--------------------\n",
      "at no cost and with\n",
      "--------------------\n",
      "{\"RecordNumber\":1,\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":17.96,\"Long\":-66.22,\"Xaxis\":0.38,\"Yaxis\":-0.87,\"Zaxis\":0.3,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Parc Parque, PR\",\"Location\":\"NA-US-PR-PARC PARQUE\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":2,\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PASEO COSTA DEL SUR\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":17.96,\"Long\":-66.22,\"Xaxis\":0.38,\"Yaxis\":-0.87,\"Zaxis\":0.3,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Paseo Costa Del Sur, PR\",\"Location\":\"NA-US-PR-PASEO COSTA DEL SUR\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":10,\"Zipcode\":709,\"ZipCodeType\":\"STANDARD\",\"City\":\"BDA SAN LUIS\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":18.14,\"Long\":-66.26,\"Xaxis\":0.38,\"Yaxis\":-0.86,\"Zaxis\":0.31,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Bda San Luis, PR\",\"Location\":\"NA-US-PR-BDA SAN LUIS\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "RecordNumber,Zipcode,ZipCodeType,City,State,LocationType,Lat,Long,Xaxis,Yaxis,Zaxis,WorldRegion,Country,LocationText,Location,Decommisioned,TaxReturnsFiled,EstimatedPopulation,TotalWages,Notes\n",
      "--------------------\n",
      "1,704,STANDARD,PARC PARQUE,PR,NOT ACCEPTABLE,17.96,-66.22,0.38,-0.87,0.3,NA,US,\"Parc Parque, PR\",NA-US-PR-PARC PARQUE,FALSE,,,,\n",
      "--------------------\n",
      "2,704,STANDARD,PASEO COSTA DEL SUR,PR,NOT ACCEPTABLE,17.96,-66.22,0.38,-0.87,0.3,NA,US,\"Paseo Costa Del Sur, PR\",NA-US-PR-PASEO COSTA DEL SUR,FALSE,,,,\n",
      "--------------------\n",
      "10,709,STANDARD,BDA SAN LUIS,PR,NOT ACCEPTABLE,18.14,-66.26,0.38,-0.86,0.31,NA,US,\"Bda San Luis, PR\",NA-US-PR-BDA SAN LUIS,FALSE,,,,\n",
      "--------------------\n",
      "61391,76166,UNIQUE,CINGULAR WIRELESS,TX,NOT ACCEPTABLE,32.72,-97.31,-0.1,-0.83,0.54,NA,US,\"Cingular Wireless, TX\",NA-US-TX-CINGULAR WIRELESS,FALSE,,,,\n",
      "--------------------\n",
      "61392,76177,STANDARD,FORT WORTH,TX,PRIMARY,32.75,-97.33,-0.1,-0.83,0.54,NA,US,\"Fort Worth, TX\",NA-US-TX-FORT WORTH,FALSE,2126,4053,122396986,\n",
      "--------------------\n",
      "61393,76177,STANDARD,FT WORTH,TX,ACCEPTABLE,32.75,-97.33,-0.1,-0.83,0.54,NA,US,\"Ft Worth, TX\",NA-US-TX-FT WORTH,FALSE,2126,4053,122396986,\n",
      "--------------------\n",
      "4,704,STANDARD,URB EUGENE RICE,PR,NOT ACCEPTABLE,17.96,-66.22,0.38,-0.87,0.3,NA,US,\"Urb Eugene Rice, PR\",NA-US-PR-URB EUGENE RICE,FALSE,,,,\n",
      "--------------------\n",
      "39827,85209,STANDARD,MESA,AZ,PRIMARY,33.37,-111.64,-0.3,-0.77,0.55,NA,US,\"Mesa, AZ\",NA-US-AZ-MESA,FALSE,14962,26883,563792730,\"no NWS data, \"\n",
      "--------------------\n",
      "39828,85210,STANDARD,MESA,AZ,PRIMARY,33.38,-111.84,-0.31,-0.77,0.55,NA,US,\"Mesa, AZ\",NA-US-AZ-MESA,FALSE,14374,25446,471000465,\n",
      "--------------------\n",
      "49345,32046,STANDARD,HILLIARD,FL,PRIMARY,30.69,-81.92,0.12,-0.85,0.51,NA,US,\"Hilliard, FL\",NA-US-FL-HILLIARD,FALSE,3922,7443,133112149,\n",
      "--------------------\n",
      "49346,34445,PO BOX,HOLDER,FL,PRIMARY,28.96,-82.41,0.11,-0.86,0.48,NA,US,\"Holder, FL\",NA-US-FL-HOLDER,FALSE,,,,\n",
      "--------------------\n",
      "49347,32564,STANDARD,HOLT,FL,PRIMARY,30.72,-86.67,0.04,-0.85,0.51,NA,US,\"Holt, FL\",NA-US-FL-HOLT,FALSE,1207,2190,36395913,\n",
      "--------------------\n",
      "49348,34487,PO BOX,HOMOSASSA,FL,PRIMARY,28.78,-82.61,0.11,-0.86,0.48,NA,US,\"Homosassa, FL\",NA-US-FL-HOMOSASSA,FALSE,,,,\n",
      "--------------------\n",
      "10,708,STANDARD,BDA SAN LUIS,PR,NOT ACCEPTABLE,18.14,-66.26,0.38,-0.86,0.31,NA,US,\"Bda San Luis, PR\",NA-US-PR-BDA SAN LUIS,FALSE,,,,\n",
      "--------------------\n",
      "3,704,STANDARD,SECT LANAUSSE,PR,NOT ACCEPTABLE,17.96,-66.22,0.38,-0.87,0.3,NA,US,\"Sect Lanausse, PR\",NA-US-PR-SECT LANAUSSE,FALSE,,,,\n",
      "--------------------\n",
      "54354,36275,PO BOX,SPRING GARDEN,AL,PRIMARY,33.97,-85.55,0.06,-0.82,0.55,NA,US,\"Spring Garden, AL\",NA-US-AL-SPRING GARDEN,FALSE,,,,\n",
      "--------------------\n",
      "54355,35146,STANDARD,SPRINGVILLE,AL,PRIMARY,33.77,-86.47,0.05,-0.82,0.55,NA,US,\"Springville, AL\",NA-US-AL-SPRINGVILLE,FALSE,4046,7845,172127599,\n",
      "--------------------\n",
      "54356,35585,STANDARD,SPRUCE PINE,AL,PRIMARY,34.37,-87.69,0.03,-0.82,0.56,NA,US,\"Spruce Pine, AL\",NA-US-AL-SPRUCE PINE,FALSE,610,1209,18525517,\n",
      "--------------------\n",
      "76511,27007,STANDARD,ASH HILL,NC,NOT ACCEPTABLE,36.4,-80.56,0.13,-0.79,0.59,NA,US,\"Ash Hill, NC\",NA-US-NC-ASH HILL,FALSE,842,1666,28876493,\n",
      "--------------------\n",
      "76512,27203,STANDARD,ASHEBORO,NC,PRIMARY,35.71,-79.81,0.14,-0.79,0.58,NA,US,\"Asheboro, NC\",NA-US-NC-ASHEBORO,FALSE,8355,15228,215474318,\n",
      "--------------------\n",
      "76513,27204,PO BOX,ASHEBORO,NC,PRIMARY,35.71,-79.81,0.14,-0.79,0.58,NA,US,\"Asheboro, NC\",NA-US-NC-ASHEBORO,FALSE,1035,1816,30322473,\n",
      "--------------------\n",
      "{\"RecordNumber\":1,\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":17.96,\"Long\":-66.22,\"Xaxis\":0.38,\"Yaxis\":-0.87,\"Zaxis\":0.3,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Parc Parque, PR\",\"Location\":\"NA-US-PR-PARC PARQUE\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":2,\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PASEO COSTA DEL SUR\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":17.96,\"Long\":-66.22,\"Xaxis\":0.38,\"Yaxis\":-0.87,\"Zaxis\":0.3,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Paseo Costa Del Sur, PR\",\"Location\":\"NA-US-PR-PASEO COSTA DEL SUR\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":10,\"Zipcode\":709,\"ZipCodeType\":\"STANDARD\",\"City\":\"BDA SAN LUIS\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":18.14,\"Long\":-66.26,\"Xaxis\":0.38,\"Yaxis\":-0.86,\"Zaxis\":0.31,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Bda San Luis, PR\",\"Location\":\"NA-US-PR-BDA SAN LUIS\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":61391,\"Zipcode\":76166,\"ZipCodeType\":\"UNIQUE\",\"City\":\"CINGULAR WIRELESS\",\"State\":\"TX\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":32.72,\"Long\":-97.31,\"Xaxis\":-0.1,\"Yaxis\":-0.83,\"Zaxis\":0.54,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Cingular Wireless, TX\",\"Location\":\"NA-US-TX-CINGULAR WIRELESS\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":61392,\"Zipcode\":76177,\"ZipCodeType\":\"STANDARD\",\"City\":\"FORT WORTH\",\"State\":\"TX\",\"LocationType\":\"PRIMARY\",\"Lat\":32.75,\"Long\":-97.33,\"Xaxis\":-0.1,\"Yaxis\":-0.83,\"Zaxis\":0.54,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Fort Worth, TX\",\"Location\":\"NA-US-TX-FORT WORTH\",\"Decommisioned\":false,\"TaxReturnsFiled\":2126,\"EstimatedPopulation\":4053,\"TotalWages\":122396986}\n",
      "--------------------\n",
      "{\"RecordNumber\":61393,\"Zipcode\":76177,\"ZipCodeType\":\"STANDARD\",\"City\":\"FT WORTH\",\"State\":\"TX\",\"LocationType\":\"ACCEPTABLE\",\"Lat\":32.75,\"Long\":-97.33,\"Xaxis\":-0.1,\"Yaxis\":-0.83,\"Zaxis\":0.54,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Ft Worth, TX\",\"Location\":\"NA-US-TX-FT WORTH\",\"Decommisioned\":false,\"TaxReturnsFiled\":2126,\"EstimatedPopulation\":4053,\"TotalWages\":122396986}\n",
      "--------------------\n",
      "{\"RecordNumber\":4,\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"URB EUGENE RICE\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":17.96,\"Long\":-66.22,\"Xaxis\":0.38,\"Yaxis\":-0.87,\"Zaxis\":0.3,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Urb Eugene Rice, PR\",\"Location\":\"NA-US-PR-URB EUGENE RICE\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":39827,\"Zipcode\":85209,\"ZipCodeType\":\"STANDARD\",\"City\":\"MESA\",\"State\":\"AZ\",\"LocationType\":\"PRIMARY\",\"Lat\":33.37,\"Long\":-111.64,\"Xaxis\":-0.3,\"Yaxis\":-0.77,\"Zaxis\":0.55,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Mesa, AZ\",\"Location\":\"NA-US-AZ-MESA\",\"Decommisioned\":false,\"TaxReturnsFiled\":14962,\"EstimatedPopulation\":26883,\"TotalWages\":563792730,\"Notes\":\"no NWS data, \"}\n",
      "--------------------\n",
      "{\"RecordNumber\":39828,\"Zipcode\":85210,\"ZipCodeType\":\"STANDARD\",\"City\":\"MESA\",\"State\":\"AZ\",\"LocationType\":\"PRIMARY\",\"Lat\":33.38,\"Long\":-111.84,\"Xaxis\":-0.31,\"Yaxis\":-0.77,\"Zaxis\":0.55,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Mesa, AZ\",\"Location\":\"NA-US-AZ-MESA\",\"Decommisioned\":false,\"TaxReturnsFiled\":14374,\"EstimatedPopulation\":25446,\"TotalWages\":471000465}\n",
      "--------------------\n",
      "{\"RecordNumber\":49345,\"Zipcode\":32046,\"ZipCodeType\":\"STANDARD\",\"City\":\"HILLIARD\",\"State\":\"FL\",\"LocationType\":\"PRIMARY\",\"Lat\":30.69,\"Long\":-81.92,\"Xaxis\":0.12,\"Yaxis\":-0.85,\"Zaxis\":0.51,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Hilliard, FL\",\"Location\":\"NA-US-FL-HILLIARD\",\"Decommisioned\":false,\"TaxReturnsFiled\":3922,\"EstimatedPopulation\":7443,\"TotalWages\":133112149}\n",
      "--------------------\n",
      "{\"RecordNumber\":49346,\"Zipcode\":34445,\"ZipCodeType\":\"PO BOX\",\"City\":\"HOLDER\",\"State\":\"FL\",\"LocationType\":\"PRIMARY\",\"Lat\":28.96,\"Long\":-82.41,\"Xaxis\":0.11,\"Yaxis\":-0.86,\"Zaxis\":0.48,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Holder, FL\",\"Location\":\"NA-US-FL-HOLDER\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":49347,\"Zipcode\":32564,\"ZipCodeType\":\"STANDARD\",\"City\":\"HOLT\",\"State\":\"FL\",\"LocationType\":\"PRIMARY\",\"Lat\":30.72,\"Long\":-86.67,\"Xaxis\":0.04,\"Yaxis\":-0.85,\"Zaxis\":0.51,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Holt, FL\",\"Location\":\"NA-US-FL-HOLT\",\"Decommisioned\":false,\"TaxReturnsFiled\":1207,\"EstimatedPopulation\":2190,\"TotalWages\":36395913}\n",
      "--------------------\n",
      "{\"RecordNumber\":49348,\"Zipcode\":34487,\"ZipCodeType\":\"PO BOX\",\"City\":\"HOMOSASSA\",\"State\":\"FL\",\"LocationType\":\"PRIMARY\",\"Lat\":28.78,\"Long\":-82.61,\"Xaxis\":0.11,\"Yaxis\":-0.86,\"Zaxis\":0.48,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Homosassa, FL\",\"Location\":\"NA-US-FL-HOMOSASSA\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":10,\"Zipcode\":708,\"ZipCodeType\":\"STANDARD\",\"City\":\"BDA SAN LUIS\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":18.14,\"Long\":-66.26,\"Xaxis\":0.38,\"Yaxis\":-0.86,\"Zaxis\":0.31,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Bda San Luis, PR\",\"Location\":\"NA-US-PR-BDA SAN LUIS\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":3,\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"SECT LANAUSSE\",\"State\":\"PR\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":17.96,\"Long\":-66.22,\"Xaxis\":0.38,\"Yaxis\":-0.87,\"Zaxis\":0.3,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Sect Lanausse, PR\",\"Location\":\"NA-US-PR-SECT LANAUSSE\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":54354,\"Zipcode\":36275,\"ZipCodeType\":\"PO BOX\",\"City\":\"SPRING GARDEN\",\"State\":\"AL\",\"LocationType\":\"PRIMARY\",\"Lat\":33.97,\"Long\":-85.55,\"Xaxis\":0.06,\"Yaxis\":-0.82,\"Zaxis\":0.55,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Spring Garden, AL\",\"Location\":\"NA-US-AL-SPRING GARDEN\",\"Decommisioned\":false}\n",
      "--------------------\n",
      "{\"RecordNumber\":54355,\"Zipcode\":35146,\"ZipCodeType\":\"STANDARD\",\"City\":\"SPRINGVILLE\",\"State\":\"AL\",\"LocationType\":\"PRIMARY\",\"Lat\":33.77,\"Long\":-86.47,\"Xaxis\":0.05,\"Yaxis\":-0.82,\"Zaxis\":0.55,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Springville, AL\",\"Location\":\"NA-US-AL-SPRINGVILLE\",\"Decommisioned\":false,\"TaxReturnsFiled\":4046,\"EstimatedPopulation\":7845,\"TotalWages\":172127599}\n",
      "--------------------\n",
      "{\"RecordNumber\":54356,\"Zipcode\":35585,\"ZipCodeType\":\"STANDARD\",\"City\":\"SPRUCE PINE\",\"State\":\"AL\",\"LocationType\":\"PRIMARY\",\"Lat\":34.37,\"Long\":-87.69,\"Xaxis\":0.03,\"Yaxis\":-0.82,\"Zaxis\":0.56,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Spruce Pine, AL\",\"Location\":\"NA-US-AL-SPRUCE PINE\",\"Decommisioned\":false,\"TaxReturnsFiled\":610,\"EstimatedPopulation\":1209,\"TotalWages\":18525517}\n",
      "--------------------\n",
      "{\"RecordNumber\":76511,\"Zipcode\":27007,\"ZipCodeType\":\"STANDARD\",\"City\":\"ASH HILL\",\"State\":\"NC\",\"LocationType\":\"NOT ACCEPTABLE\",\"Lat\":36.4,\"Long\":-80.56,\"Xaxis\":0.13,\"Yaxis\":-0.79,\"Zaxis\":0.59,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Ash Hill, NC\",\"Location\":\"NA-US-NC-ASH HILL\",\"Decommisioned\":false,\"TaxReturnsFiled\":842,\"EstimatedPopulation\":1666,\"TotalWages\":28876493}\n",
      "--------------------\n",
      "{\"RecordNumber\":76512,\"Zipcode\":27203,\"ZipCodeType\":\"STANDARD\",\"City\":\"ASHEBORO\",\"State\":\"NC\",\"LocationType\":\"PRIMARY\",\"Lat\":35.71,\"Long\":-79.81,\"Xaxis\":0.14,\"Yaxis\":-0.79,\"Zaxis\":0.58,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Asheboro, NC\",\"Location\":\"NA-US-NC-ASHEBORO\",\"Decommisioned\":false,\"TaxReturnsFiled\":8355,\"EstimatedPopulation\":15228,\"TotalWages\":215474318}\n",
      "--------------------\n",
      "{\"RecordNumber\":76513,\"Zipcode\":27204,\"ZipCodeType\":\"PO BOX\",\"City\":\"ASHEBORO\",\"State\":\"NC\",\"LocationType\":\"PRIMARY\",\"Lat\":35.71,\"Long\":-79.81,\"Xaxis\":0.14,\"Yaxis\":-0.79,\"Zaxis\":0.58,\"WorldRegion\":\"NA\",\"Country\":\"US\",\"LocationText\":\"Asheboro, NC\",\"Location\":\"NA-US-NC-ASHEBORO\",\"Decommisioned\":false,\"TaxReturnsFiled\":1035,\"EstimatedPopulation\":1816,\"TotalWages\":30322473}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for item in input_file.collect():\n",
    "    print(item)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa72cd0-779d-47a7-99d5-8969fc7bb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read text file into df\n",
    "df=spark.read.text(\"../data/cover_letter.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "652e7491-be17-4f38-81de-169312ed9c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Presently a Data Engineer with more than 2 yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               value\n",
       "0  Presently a Data Engineer with more than 2 yea...\n",
       "1                                                   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd9ce9-f84e-4570-aee3-84e3398f1333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbc9321a-1645-4ba9-a0bd-6709e70b8793",
   "metadata": {},
   "source": [
    "## Spark RDD Transformations with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b99ebec3-17a1-4f8a-ac26-dc5f79e6c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.textFile(\"../data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "951f6bf3-d231-402c-959f-c63222fb4df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project Gutenberg’s'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faaac71-080c-4ca5-a277-cf41d0bd67f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98fdf49c-022d-4037-beb9-b6b03e9a2bb6",
   "metadata": {},
   "source": [
    "## flatMap() ,map(),filter(),reducebykey Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ead6a2-7fbb-4c0e-922b-09e0c852c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=rdd.flatMap(lambda x:x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d43914-981c-402b-be86-49ad487d1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94fad24a-c21e-4dc5-a65e-7569cfae78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd1.filter(lambda x:x.startswith(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a7ea29d-b16a-4154-9fe8-e4bdd52ab6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=rdd2.map(lambda x:(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fbb47cf-d434-4c6e-a206-5f2b23e41fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now perform reduceBykey\n",
    "rdd4=rdd3.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c9cf4be-e07d-473a-938d-0ba49e5f800e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 27), ('anyone', 27), ('anywhere', 27), ('at', 27)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d1d39-fa13-4ef6-b133-5b48eb8f2d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06e1d3d4-c83d-45fa-aaf8-ae7c9ebb91c1",
   "metadata": {},
   "source": [
    "## Spark RDD Actions with examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0d0a8de-a1d9-45f6-bfe8-820ce3d0fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputRDD = sc.parallelize([(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)],3)\n",
    "listRdd = sc.parallelize([1,2,3,4,5,3,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8606b8ef-6f0f-477c-a3b7-69e0fe69076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 120), ('C', 40), ('A', 20), ('Z', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reduce\n",
    "inputRDD.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12ac94d0-5ab5-45ea-850d-f06ea87291ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRdd.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "056f8f2c-28c1-4626-929e-1c680f61460a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add = lambda x, y: x + y\n",
    "rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
    "rdd.treeAggregate(0, add, add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad15ef-8a6d-4df0-b951-c161d95db23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2761a6f7-fc29-41d8-beec-52f0b1492fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da54b82f-7439-43f9-9805-9abd0cc8c694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6db79-a31a-410d-a348-3055c2c642f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72d3247e-8b51-47c3-b254-3db9dfe41a12",
   "metadata": {},
   "source": [
    "## Spark Repartition() vs Coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f865f4b-98b3-4d4f-a37a-2fc8f88474b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition :  6\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(0,25), 6)\n",
    "print(\"Repartition : \",rdd1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9033fcf5-b92a-43f0-9b76-b5cb23adfcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition :  4\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd1.repartition(4)\n",
    "print(\"Repartition : \",rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89c037df-73e4-4c86-b7e9-24929bd2c4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coalesce :  3\n"
     ]
    }
   ],
   "source": [
    "rdd3=rdd2.coalesce(3)\n",
    "print(\"coalesce : \",rdd3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db3f07-c24f-4a78-adcd-7c60f20787ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeaef99b-f1e5-4353-9ca0-c1e0fb325900",
   "metadata": {},
   "source": [
    "## spark suffling and partitions\n",
    "https://sparkbyexamples.com/spark/spark-shuffle-partitions/ <br>\n",
    "\n",
    "Based on your dataset size, number of cores, and memory, Spark shuffling can benefit or harm your jobs. When you dealing with less amount of data, you should typically reduce the shuffle partitions otherwise you will end up with many partitioned files with a fewer number of records in each partition. which results in running many tasks with lesser data to process.\n",
    "\n",
    "On other hand, when you have too much of data and having less number of partitions results in fewer longer running tasks and some times you may also get out of memory error.\n",
    "\n",
    "Getting a right size of the shuffle partition is always tricky and takes many runs with different value to achieve the optimized number. This is one of the key property to look for when you have performance issues on Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97676322-86c7-4196-bf10-886f7809b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count,desc\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "    ]\n",
    "\n",
    "column=[\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "\n",
    "df=spark.createDataFrame(data=simpleData,schema=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f208e21d-f531-4597-bffe-24e0d47dc905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a9fb7a53-c941-475c-bf1c-1ba7a0dd0a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before applying action partitions= 5\n"
     ]
    }
   ],
   "source": [
    "print(\"before applying action partitions=\",df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cbad09d7-b8b7-41b2-9f9a-a6c0b3ee76e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|department|total_salary|\n",
      "+----------+------------+\n",
      "|   Finance|      351000|\n",
      "|     Sales|      257000|\n",
      "| Marketing|      171000|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#applying some group by action\n",
    "# df2=df.groupBy(df.department).agg(sum('salary').alias(\"total_salary\")).orderBy(desc('total_salary')).show()\n",
    "df2=df.groupBy(df.department).sum('salary').orderBy(desc(sum('salary'))).withColumnRenamed('sum(salary)','total_salary')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a05cb233-0949-4989-ad33-5f1cebbeb07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 partition= 5\n"
     ]
    }
   ],
   "source": [
    "print(\"df1 partition=\",df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c6242b8d-3f68-4c3d-9a11-4672cd9375df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 partition= 1\n"
     ]
    }
   ],
   "source": [
    "print(\"df2 partition=\",df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571f92b-7d63-4daa-8c84-c49cf3db6c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d88e3fa7-d575-402b-8b0f-4862ef814ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8c82d3-7ef9-47d9-a334-ebaeff2629b7",
   "metadata": {},
   "source": [
    "## spark.default.parallelism vs spark.sql.shuffle.partitions\n",
    "https://www.learntospark.com/2020/05/persist-and-cache-in-apache-spark.html <br>\n",
    "<spark.default.parallelism > was introduced with RDD hence this property is only applicable to RDD. <br>\n",
    "The default value for this configuration set to the number of all cores on all nodes in a cluster, <br>\n",
    "on local, it is set to the number of cores on your system.\n",
    "\n",
    "\n",
    "DataFrame: Whereas < spark.sql.shuffle.partitions > was introduced with DataFrame and <br>\n",
    "it only works with DataFrame, the default value for this configuration set to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ae7fccfd-a22f-4a30-9539-948c11e5d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc9600-177c-4208-b6ef-fd0139f06d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5556c386-f495-4675-bd63-6867b66d4adf",
   "metadata": {},
   "source": [
    "## Spark – Difference between Cache and Persist?\n",
    "\n",
    "Using cache() and persist() methods, Spark provides an optimization mechanism to store the intermediate computation of an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, Dataframe, and Dataset computation result’s).\n",
    "\n",
    "Both caching and persisting are used to save the Spark RDD, Dataframe, and Dataset’s. But, the difference is, RDD cache() method default saves it to memory (MEMORY_ONLY) whereas persist() method is used to store it to the user-defined storage level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "987df5f2-933f-4ee7-a5ec-7c2bcafb76a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLet us consider the input data as csv file, and we need to read the data as dataframe and \\nuse cache on top of df.\\n\\n\\n\\nSyntax:\\n#cache RDD to store data in MEMORY_ONLY\\nrdd.cache()\\n\\n#cache DF to store data in MEMORY_ONLY\\ndf.cache()\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "\"\"\"\n",
    "Let us consider the input data as csv file, and we need to read the data as dataframe and \n",
    "use cache on top of df.\n",
    "\n",
    "\n",
    "Syntax:\n",
    "#cache RDD to store data in MEMORY_ONLY\n",
    "rdd.cache()\n",
    "\n",
    "#cache DF to store data in MEMORY_ONLY\n",
    "df.cache()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803a9745-2d2d-40c3-bb4a-9927ae031eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"../data/simple-zipcodes.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cd6ec4-38a4-4580-b114-227aeefeb221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5abd4c66-7b76-4c83-b366-c573c9661d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.storageLevel.useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dc6e896b-0c1f-4f5b-a6df-0625a5e7c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_cache= True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/08 14:13:29 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "#cache dataframe\n",
    "df_cached=df.cache()\n",
    "print(\"is_cache=\",df_cached.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b8a97d97-76b5-4dbe-b55c-3bbe92e00b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|City               |Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|1           |US     |PARC PARQUE        |704    |PR   |\n",
      "|2           |US     |PASEO COSTA DEL SUR|704    |PR   |\n",
      "|10          |US     |BDA SAN LUIS       |709    |PR   |\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5937ba91-585c-462d-9d44-ad66c5c33e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41974c-caa2-465b-a005-0a45b875e530",
   "metadata": {},
   "source": [
    "<img src=\"https://1.bp.blogspot.com/-d36kJGTjXlA/XskIMtb4NgI/AAAAAAAABWA/qjU2gEmStcYMVR8kkpl-WuirN65ogjpxQCLcBGAsYHQ/s1600/persist_type.JPG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5cb11a99-1ed3-4c4e-9e85-eef50cc0028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(df.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78320f3f-5da6-44bb-aab2-596447ec0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve persist and cache data\n",
    "import pyspark\n",
    "df_persist=df.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616ff94f-9347-436e-87ce-1ac7ae3bca00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, False, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_persist.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c10eb4-fe90-4da3-bc32-2aa002d4790b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[RecordNumber: int, Country: string, City: string, Zipcode: int, State: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd20831-f29d-4a43-8a10-bde6921a6223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[RecordNumber: int, Country: string, City: string, Zipcode: int, State: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove the cached or persisted data\n",
    "df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded80873-fc50-49f7-8975-ea15ec310982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd72a23e-9e57-44dc-89a9-475afe40cb55",
   "metadata": {},
   "source": [
    "## PySpark Broadcast Variables\n",
    "https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/ <br>\n",
    "\n",
    "In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9458e3de-4195-4175-97c6-88bc36eff233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/\n",
    "\"\"\"\n",
    "Let me explain with an example when to use broadcast variables,\n",
    "assume you are getting a two-letter country state code in a file \n",
    "and you wanted to transform it to full state name, (for example CA to California, NY to New York e.t.c)\n",
    "by doing a lookup to reference mapping. In some instances, this data could be large\n",
    "and you may have many such lookups (like zip code e.t.c).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "print(broadcastStates.value)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc1c58-442c-4212-a3cd-6ca47e3ebe5e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
