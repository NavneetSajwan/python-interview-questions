{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: inferSchema\n",
      "24/02/09 12:58:20 WARN Utils: Your hostname, Navneets-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.20.10.7 instead (on interface en0)\n",
      "24/02/09 12:58:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/09 12:58:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession. \\\n",
    "        builder. \\\n",
    "        master(\"local[5]\"). \\\n",
    "        appName(\"rdd tutorial\"). \\\n",
    "        config(\"inferSchema\" , \"True\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "# spark.conf.set(\"spark.default.parallelism\", \"1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860e75f-3f00-4a37-9429-a5fbb2ea8138",
   "metadata": {},
   "source": [
    "## PySpark Broadcast Variables\n",
    "https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/ <br>\n",
    "\n",
    "In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fddd3bd-3944-4c72-80ac-6c9b6fce59f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_110206/2855929750.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"NY\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"New York\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CA\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"California\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FL\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Florida\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mbroadcastStates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcastStates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/\n",
    "\"\"\"\n",
    "Let me explain with an example when to use broadcast variables,\n",
    "assume you are getting a two-letter country state code in a file \n",
    "and you wanted to transform it to full state name, (for example CA to California, NY to New York e.t.c)\n",
    "by doing a lookup to reference mapping. In some instances, this data could be large\n",
    "and you may have many such lookups (like zip code e.t.c).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "print(broadcastStates.value)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d998d-eb9a-4f8c-a39f-a82b024414e5",
   "metadata": {},
   "source": [
    "## diff between  Accumulator and Broadcast Variables\n",
    "\n",
    "An accumulator is also a variable that is broadcasted to the worker nodes. <br>\n",
    "The key difference between a broadcast variable and an accumulator is that while <br>\n",
    "the broadcast variable is read-only, the accumulator can be added to <br>\n",
    "\n",
    "Accumulator can be used to implement counters (as in MapReduce) or sums. <br>\n",
    "Spark natively supports accumulators of numeric types, and programmers can add support for new types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3e1d4-2eeb-4bcb-ab81-62090cf6ab5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575ef63d-e4bf-4a86-a494-785da90e3b22",
   "metadata": {},
   "source": [
    "## PySpark Accumulator with Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0089630-ad84-41e7-ad39-34b8a79fd0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accumulator= 10\n",
      "Accumulated value is -> 150\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num = sc.accumulator(10) \n",
    "print(\"accumulator=\",num)\n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "rdd.foreach(f) \n",
    "final = num.value \n",
    "print (\"Accumulated value is -> %i\" % (final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3396d8-f711-4fb4-8e2c-fdb22d41b2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
