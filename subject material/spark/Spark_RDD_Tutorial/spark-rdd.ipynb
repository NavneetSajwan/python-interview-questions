{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3a35b7-5d67-47c1-b261-89681c975d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: inferSchema\n",
      "24/01/12 12:14:13 WARN Utils: Your hostname, navneetsajwan-ThinkPad-L480 resolves to a loopback address: 127.0.1.1; using 192.168.0.149 instead (on interface enp0s31f6)\n",
      "24/01/12 12:14:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/navneetsajwan/miniconda3/envs/spark/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/12 12:14:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession. \\\n",
    "        builder. \\\n",
    "        master(\"local[5]\"). \\\n",
    "        appName(\"rdd tutorial\"). \\\n",
    "        config(\"inferSchema\" , \"True\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "# spark.conf.set(\"spark.default.parallelism\", \"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19cd537-993b-422d-9289-dfd8a2e18c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.149:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rdd tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f56616681c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d498028-2302-4b18-bb73-9ccf415c3d39",
   "metadata": {},
   "source": [
    "## notes rdd how its work\n",
    "https://sparkbyexamples.com/pyspark-rdd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.149:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rdd tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[5] appName=rdd tutorial>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f324ee6-88bc-45a2-8bd5-3ee9269ef323",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd=sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc97197-6a27-407d-81b5-afc94b9e361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "5 \n",
      "6 \n",
      "7 \n",
      "8 \n",
      "9 \n",
      "10 \n",
      "11 \n",
      "12 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for item in rdd.collect():\n",
    "    print(item,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac9a16-5ba5-4b8f-8047-132759c5599f",
   "metadata": {},
   "source": [
    "## read data from Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "265dc4f5-a91b-47c8-b222-021acafd7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=sc.textFile('/media/navneetsajwan/New Volume2/Data Engineering/Interview Prep stuff/spark_tutorial/imp_question.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86520261-a52d-4692-985f-94d95e8a1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853d43f-0a0e-4ad7-9f33-3d707e19405f",
   "metadata": {},
   "source": [
    "### Create empty RDD with partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c434394a-b729-4cee-b9ca-9b35dbd65460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[9] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([],4) #This creates 10 partitions\n",
    "rdd2\n",
    "\n",
    "# ParallelCollectionRDD[8] at readRDDFromFile at PythonRDD.scala:274\n",
    " # 8 here reperesents the count/index of the RDDs in our spark instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e13871ae-1795-4ef3-8bde-76b514b20cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5721f7-2b96-40d5-8398-21cac141eef1",
   "metadata": {},
   "source": [
    "## read data from dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a5c06a0-ece2-4815-a1b4-b55171eb233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=sc.textFile('../resources/*') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41469e9f-ae16-4c49-be84-28d8ee909bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation of all PySpark RDD, DataFrame and SQL examples present on this project are available at [Apache PySpark Tutorial](https://sparkbyexamples.com/pyspark-tutorial/), All these examples are coded in Python language and tested in our development environment.\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Table of Contents (Spark Examples in Python)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# PySpark Basic Examples\n",
      "--------------------\n",
      "- [How to create SparkSession](https://sparkbyexamples.com/pyspark/pyspark-what-is-sparksession/)\n",
      "--------------------\n",
      "- [PySpark – Accumulator](https://sparkbyexamples.com/pyspark/pyspark-accumulator-with-example/)\n",
      "--------------------\n",
      "- [PySpark Repartition vs Coalesce](https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-coalesce/)\n",
      "--------------------\n",
      "- [PySpark Broadcast variables](https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/)\n",
      "--------------------\n",
      "- [PySpark – repartition() vs coalesce()](https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-coalesce/)\n",
      "--------------------\n",
      "- [PySpark – Parallelize](https://sparkbyexamples.com/pyspark/pyspark-parallelize-create-rdd/)\n",
      "--------------------\n",
      "- [PySpark – RDD](https://sparkbyexamples.com/pyspark-rdd)\n",
      "--------------------\n",
      "- [PySpark – Web/Application UI](https://sparkbyexamples.com/spark/spark-web-ui-understanding/)\n",
      "--------------------\n",
      "- [PySpark – SparkSession](https://sparkbyexamples.com/pyspark/pyspark-what-is-sparksession/)\n",
      "--------------------\n",
      "- [PySpark – Cluster Managers](https://sparkbyexamples.com/pyspark-tutorial/#cluster-manager)\n",
      "--------------------\n",
      "- [PySpark – Install on Windows](https://sparkbyexamples.com/pyspark-tutorial/#pyspark-installation)\n",
      "--------------------\n",
      "- [PySpark – Modules & Packages](https://sparkbyexamples.com/pyspark-tutorial/#modules-packages)\n",
      "--------------------\n",
      "- [PySpark – Advantages](https://sparkbyexamples.com/pyspark-tutorial/#advantages)\n",
      "--------------------\n",
      "- [PySpark – Features](https://sparkbyexamples.com/pyspark-tutorial/#features)\n",
      "--------------------\n",
      "- [PySpark – What is it? & Who uses it?](https://sparkbyexamples.com/pyspark/what-is-pyspark-and-who-uses-it/)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "## PySpark DataFrame Examples \n",
      "--------------------\n",
      "- [PySpark – Create a DataFrame](https://sparkbyexamples.com/pyspark/different-ways-to-create-dataframe-in-pyspark/)\n",
      "--------------------\n",
      "- [PySpark – Create an empty DataFrame](https://sparkbyexamples.com/pyspark/pyspark-create-an-empty-dataframe/)\n",
      "--------------------\n",
      "- [PySpark – Convert RDD to DataFrame](https://sparkbyexamples.com/pyspark/convert-pyspark-rdd-to-dataframe/)\n",
      "--------------------\n",
      "- [PySpark – Convert DataFrame to Pandas](https://sparkbyexamples.com/pyspark/convert-pyspark-dataframe-to-pandas/)\n",
      "--------------------\n",
      "- [PySpark – StructType & StructField](https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/)\n",
      "--------------------\n",
      "- [PySpark Row using on DataFrame and RDD](https://sparkbyexamples.com/pyspark/pyspark-row-using-rdd-dataframe/)\n",
      "--------------------\n",
      "- [Select columns from PySpark DataFrame ](https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/)\n",
      "--------------------\n",
      "- [PySpark Collect() – Retrieve data from DataFrame](https://sparkbyexamples.com/pyspark/pyspark-collect/)\n",
      "--------------------\n",
      "- [PySpark withColumn to update or add a column](https://sparkbyexamples.com/pyspark/pyspark-withcolumn/)\n",
      "--------------------\n",
      "- [PySpark using where filter function ](https://sparkbyexamples.com/pyspark/pyspark-where-filter/)\n",
      "--------------------\n",
      "- [PySpark – Distinct to drop duplicate rows ](https://sparkbyexamples.com/pyspark/pyspark-distinct-to-drop-duplicates/)\n",
      "--------------------\n",
      "- [ PySpark orderBy() and sort() explained](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)\n",
      "--------------------\n",
      "- [PySpark Groupby Explained with Example](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/)\n",
      "--------------------\n",
      "- [PySpark Join Types Explained with Examples](https://sparkbyexamples.com/pyspark/pyspark-join/)\n",
      "--------------------\n",
      "- [PySpark Union and UnionAll Explained](https://sparkbyexamples.com/pyspark/pyspark-union-and-unionall/)\n",
      "--------------------\n",
      "- [PySpark UDF (User Defined Function)](https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/)\n",
      "--------------------\n",
      "- [PySpark flatMap() Transformation](https://sparkbyexamples.com/pyspark/pyspark-flatmap-transformation/)\n",
      "--------------------\n",
      "- [PySpark map Transformation](https://sparkbyexamples.com/pyspark/pyspark-map-transformation/)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "## PySpark SQL Functions\n",
      "--------------------\n",
      "- [PySpark Aggregate Functions with Examples](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/)\n",
      "--------------------\n",
      "- [PySpark Window Functions](https://sparkbyexamples.com/pyspark/pyspark-window-functions/)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "## PySpark Datasources\n",
      "--------------------\n",
      "- [PySpark Read CSV file into DataFrame](https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/)\n",
      "--------------------\n",
      "- [PySpark read and write Parquet File ](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.master(\"local[1]\") \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
      "--------------------\n",
      "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data=data,schema=columns)\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "print(df.collect())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "states1=df.rdd.map(lambda x: x[3]).collect()\n",
      "--------------------\n",
      "print(states1)\n",
      "--------------------\n",
      "#['CA', 'NY', 'CA', 'FL']\n",
      "--------------------\n",
      "from collections import OrderedDict \n",
      "--------------------\n",
      "res = list(OrderedDict.fromkeys(states1)) \n",
      "--------------------\n",
      "print(res)\n",
      "--------------------\n",
      "#['CA', 'NY', 'FL']\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Example 2\n",
      "--------------------\n",
      "states2=df.rdd.map(lambda x: x.state).collect()\n",
      "--------------------\n",
      "print(states2)\n",
      "--------------------\n",
      "#['CA', 'NY', 'CA', 'FL']\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "states3=df.select(df.state).collect()\n",
      "--------------------\n",
      "print(states3)\n",
      "--------------------\n",
      "#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "states4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\n",
      "--------------------\n",
      "print(states4)\n",
      "--------------------\n",
      "#['CA', 'NY', 'CA', 'FL']\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "states5=df.select(df.state).toPandas()['state']\n",
      "--------------------\n",
      "states6=list(states5)\n",
      "--------------------\n",
      "print(states6)\n",
      "--------------------\n",
      "#['CA', 'NY', 'CA', 'FL']\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "pandDF=df.select(df.state,df.firstname).toPandas()\n",
      "--------------------\n",
      "print(list(pandDF['state']))\n",
      "--------------------\n",
      "print(list(pandDF['firstname']))\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Thu Oct 24 22:42:50 2019\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: prabha\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "from pyspark.sql.functions import to_timestamp, current_timestamp\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "            StructField(\"seq\", StringType(), True)])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dates = ['1']\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(list('1'), schema=schema)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pandas as pd    \n",
      "--------------------\n",
      "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "# Create the pandas DataFrame \n",
      "--------------------\n",
      "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "# print dataframe. \n",
      "--------------------\n",
      "print(pandasDF)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "    .master(\"local[1]\") \\\n",
      "--------------------\n",
      "    .appName(\"SparkByExamples.com\") \\\n",
      "--------------------\n",
      "    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "sparkDF=spark.createDataFrame(pandasDF) \n",
      "--------------------\n",
      "sparkDF.printSchema()\n",
      "--------------------\n",
      "sparkDF.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
      "--------------------\n",
      "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
      "--------------------\n",
      "                       ,StructField(\"Age\", IntegerType(), True)])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
      "--------------------\n",
      "sparkDF2.printSchema()\n",
      "--------------------\n",
      "sparkDF2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
      "--------------------\n",
      "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
      "--------------------\n",
      "print(pandasDF2)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "test=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\n",
      "--------------------\n",
      "print(test)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "test123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\n",
      "--------------------\n",
      "print(test123)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,expr\n",
      "--------------------\n",
      "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
      "--------------------\n",
      "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
      "--------------------\n",
      "    .select(col(\"date\"),col(\"increment\"), \\\n",
      "--------------------\n",
      "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [('James','Smith','M',3000),\n",
      "--------------------\n",
      "  ('Anna','Rose','F',4100),\n",
      "--------------------\n",
      "  ('Robert','Williams','M',6200), \n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data, schema = columns)\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "if 'salary1' not in df.columns:\n",
      "--------------------\n",
      "    print(\"aa\")\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "# Add new constanct column\n",
      "--------------------\n",
      "from pyspark.sql.functions import lit\n",
      "--------------------\n",
      "df.withColumn(\"bonus_percent\", lit(0.3)) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "#Add column from existing column\n",
      "--------------------\n",
      "df.withColumn(\"bonus_amount\", df.salary*0.3) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Add column by concatinating existing columns\n",
      "--------------------\n",
      "from pyspark.sql.functions import concat_ws\n",
      "--------------------\n",
      "df.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Add current date\n",
      "--------------------\n",
      "from pyspark.sql.functions import current_date\n",
      "--------------------\n",
      "df.withColumn(\"current_date\", current_date()) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import when\n",
      "--------------------\n",
      "df.withColumn(\"grade\", \\\n",
      "--------------------\n",
      "   when((df.salary < 4000), lit(\"A\")) \\\n",
      "--------------------\n",
      "     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n",
      "--------------------\n",
      "     .otherwise(lit(\"C\")) \\\n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "# Add column using select\n",
      "--------------------\n",
      "df.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\")).show()\n",
      "--------------------\n",
      "df.select(\"firstname\",\"salary\", lit(df.salary * 0.3).alias(\"bonus_amount\")).show()\n",
      "--------------------\n",
      "df.select(\"firstname\",\"salary\", current_date().alias(\"today_date\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Add columns using SQL\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"PER\")\n",
      "--------------------\n",
      "spark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\n",
      "--------------------\n",
      "spark.sql(\"select firstname,salary, salary * 0.3 as bonus_amount from PER\").show()\n",
      "--------------------\n",
      "spark.sql(\"select firstname,salary, current_date() as today_date from PER\").show()\n",
      "--------------------\n",
      "spark.sql(\"select firstname,salary, \" +\n",
      "--------------------\n",
      "          \"case salary when salary < 4000 then 'A' \"+\n",
      "--------------------\n",
      "          \"else 'B' END as grade from PER\").show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: prabha\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
      "--------------------\n",
      "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
      "--------------------\n",
      "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
      "--------------------\n",
      "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
      "--------------------\n",
      "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\", \"Sales\", 3000),\n",
      "--------------------\n",
      "    (\"Michael\", \"Sales\", 4600),\n",
      "--------------------\n",
      "    (\"Robert\", \"Sales\", 4100),\n",
      "--------------------\n",
      "    (\"Maria\", \"Finance\", 3000),\n",
      "--------------------\n",
      "    (\"James\", \"Sales\", 3000),\n",
      "--------------------\n",
      "    (\"Scott\", \"Finance\", 3300),\n",
      "--------------------\n",
      "    (\"Jen\", \"Finance\", 3900),\n",
      "--------------------\n",
      "    (\"Jeff\", \"Marketing\", 3000),\n",
      "--------------------\n",
      "    (\"Kumar\", \"Marketing\", 2000),\n",
      "--------------------\n",
      "    (\"Saif\", \"Sales\", 4100)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "schema = [\"employee_name\", \"department\", \"salary\"]\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"approx_count_distinct: \" + \\\n",
      "--------------------\n",
      "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(collect_list(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(collect_set(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "print(\"Distinct Count of Department &amp; Salary: \"+str(df2.collect()[0][0]))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
      "--------------------\n",
      "df.select(first(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(last(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(max(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(min(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(mean(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(skewness(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
      "--------------------\n",
      "    stddev_pop(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(sum(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
      "--------------------\n",
      "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.master(\"local[1]\") \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
      "--------------------\n",
      "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
      "--------------------\n",
      "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
      "--------------------\n",
      "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import col, concat_ws\n",
      "--------------------\n",
      "df2 = df.withColumn(\"languagesAtSchool\",\n",
      "--------------------\n",
      "   concat_ws(\",\",col(\"languagesAtSchool\")))\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
      "--------------------\n",
      "spark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,\" + \\\n",
      "--------------------\n",
      "    \" currentState from ARRAY_STRING\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "arrayCol = ArrayType(StringType(),False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [\n",
      "--------------------\n",
      " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
      "--------------------\n",
      " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
      "--------------------\n",
      " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([ \n",
      "--------------------\n",
      "    StructField(\"name\",StringType(),True), \n",
      "--------------------\n",
      "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
      "--------------------\n",
      "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
      "--------------------\n",
      "    StructField(\"currentState\", StringType(), True), \n",
      "--------------------\n",
      "    StructField(\"previousState\", StringType(), True) \n",
      "--------------------\n",
      "  ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import explode\n",
      "--------------------\n",
      "df.select(df.name,explode(df.languagesAtSchool)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import split\n",
      "--------------------\n",
      "df.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import array\n",
      "--------------------\n",
      "df.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import array_contains\n",
      "--------------------\n",
      "df.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n",
      "--------------------\n",
      "    .alias(\"array_contains\")).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "Created on Sat Jan 11 19:38:27 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: sparkbyexamples.com\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
      "--------------------\n",
      "broadcastStates = spark.sparkContext.broadcast(states)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
      "--------------------\n",
      "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
      "--------------------\n",
      "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
      "--------------------\n",
      "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "def state_convert(code):\n",
      "--------------------\n",
      "    return broadcastStates.value[code]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
      "--------------------\n",
      "result.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Broadcast variable on filter\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "filteDf= df.where((df['state'].isin(broadcastStates.value)))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sat Jun 13 21:08:30 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: NNK\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n",
      "--------------------\n",
      "    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n",
      "--------------------\n",
      "    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "from pyspark.sql.types import StringType,BooleanType,DateType\n",
      "--------------------\n",
      "df2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n",
      "--------------------\n",
      "    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n",
      "--------------------\n",
      "    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3 = df2.selectExpr(\"cast(age as int) age\",\n",
      "--------------------\n",
      "    \"cast(isGraduated as string) isGraduated\",\n",
      "--------------------\n",
      "    \"cast(jobStartDate as string) jobStartDate\")\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "df3.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3.createOrReplaceTempView(\"CastExample\")\n",
      "--------------------\n",
      "df4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\n",
      "--------------------\n",
      "df4.printSchema()\n",
      "--------------------\n",
      "df4.show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import DoubleType, IntegerType\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\",\"34\",\"true\",\"M\",\"3000.6089\"),\n",
      "--------------------\n",
      "    (\"Michael\",\"33\",\"true\",\"F\",\"3300.8067\"),\n",
      "--------------------\n",
      "    (\"Robert\",\"37\",\"false\",\"M\",\"5000.5034\")\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"age\",\"isGraduated\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,round,expr\n",
      "--------------------\n",
      "df.withColumn(\"salary\",df.salary.cast('double')).printSchema()    \n",
      "--------------------\n",
      "df.withColumn(\"salary\",df.salary.cast(DoublerType())).printSchema()    \n",
      "--------------------\n",
      "df.withColumn(\"salary\",col(\"salary\").cast('double')).printSchema()    \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#df.withColumn(\"salary\",round(df.salary.cast(DoubleType()),2)).show(truncate=False).printSchema()    \n",
      "--------------------\n",
      "df.selectExpr(\"firstname\",\"isGraduated\",\"cast(salary as double) salary\").printSchema()    \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"CastExample\")\n",
      "--------------------\n",
      "spark.sql(\"SELECT firstname,isGraduated,DOUBLE(salary) as salary from CastExample\").printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#df.select(\"firstname\",expr(df.age),\"isGraduated\",col(\"salary\").cast('float').alias(\"salary\")).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dept = [(\"Finance\",10), \\\n",
      "--------------------\n",
      "    (\"Marketing\",20), \\\n",
      "--------------------\n",
      "    (\"Sales\",30), \\\n",
      "--------------------\n",
      "    (\"IT\",40) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "deptColumns = [\"dept_name\",\"dept_id\"]\n",
      "--------------------\n",
      "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
      "--------------------\n",
      "deptDF.printSchema()\n",
      "--------------------\n",
      "deptDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataCollect = deptDF.collect()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(dataCollect)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataCollect2 = deptDF.select(\"dept_name\").collect()\n",
      "--------------------\n",
      "print(dataCollect2)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "for row in dataCollect:\n",
      "--------------------\n",
      "    print(row['dept_name'] + \",\" +str(row['dept_id']))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data=[(\"James\",\"Bond\",\"100\",None),\n",
      "--------------------\n",
      "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
      "--------------------\n",
      "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
      "--------------------\n",
      "      (\"Tom Brand\",None,\"400\",'M')] \n",
      "--------------------\n",
      "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data,columns)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#alias\n",
      "--------------------\n",
      "from pyspark.sql.functions import expr\n",
      "--------------------\n",
      "df.select(df.fname.alias(\"first_name\"), \\\n",
      "--------------------\n",
      "          df.lname.alias(\"last_name\"), \\\n",
      "--------------------\n",
      "          expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n",
      "--------------------\n",
      "   ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#asc, desc\n",
      "--------------------\n",
      "df.sort(df.fname.asc()).show()\n",
      "--------------------\n",
      "df.sort(df.fname.desc()).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#cast\n",
      "--------------------\n",
      "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#between\n",
      "--------------------\n",
      "df.filter(df.id.between(100,300)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#contains\n",
      "--------------------\n",
      "df.filter(df.fname.contains(\"Cruise\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#startswith, endswith()\n",
      "--------------------\n",
      "df.filter(df.fname.startswith(\"T\")).show()\n",
      "--------------------\n",
      "df.filter(df.fname.endswith(\"Cruise\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#eqNullSafe\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#isNull & isNotNull\n",
      "--------------------\n",
      "df.filter(df.lname.isNull()).show()\n",
      "--------------------\n",
      "df.filter(df.lname.isNotNull()).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#like , rlike\n",
      "--------------------\n",
      "df.select(df.fname,df.lname,df.id) \\\n",
      "--------------------\n",
      "  .filter(df.fname.like(\"%om\")) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#over\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#substr\n",
      "--------------------\n",
      "df.select(df.fname.substr(1,2).alias(\"substr\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#when & otherwise\n",
      "--------------------\n",
      "from pyspark.sql.functions import when\n",
      "--------------------\n",
      "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
      "--------------------\n",
      "              .when(df.gender==\"F\",\"Female\") \\\n",
      "--------------------\n",
      "              .when(df.gender==None ,\"\") \\\n",
      "--------------------\n",
      "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
      "--------------------\n",
      "    ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#isin\n",
      "--------------------\n",
      "li=[\"100\",\"200\"]\n",
      "--------------------\n",
      "df.select(df.fname,df.lname,df.id) \\\n",
      "--------------------\n",
      "  .filter(df.id.isin(li)) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
      "--------------------\n",
      "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
      "--------------------\n",
      "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
      "--------------------\n",
      "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
      "--------------------\n",
      "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "        StructField('name', StructType([\n",
      "--------------------\n",
      "            StructField('fname', StringType(), True),\n",
      "--------------------\n",
      "            StructField('lname', StringType(), True)])),\n",
      "--------------------\n",
      "        StructField('languages', ArrayType(StringType()),True),\n",
      "--------------------\n",
      "        StructField('properties', MapType(StringType(),StringType()),True)\n",
      "--------------------\n",
      "     ])\n",
      "--------------------\n",
      "df=spark.createDataFrame(data,schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "#getItem()\n",
      "--------------------\n",
      "df.select(df.languages.getItem(1)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(df.properties.getItem(\"hair\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#getField from Struct or Map\n",
      "--------------------\n",
      "df.select(df.properties.getField(\"hair\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(df.name.getField(\"fname\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#dropFields\n",
      "--------------------\n",
      "#from pyspark.sql.functions import col\n",
      "--------------------\n",
      "#df.withColumn(\"name1\",col(\"name\").dropFields([\"fname\"])).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#withField\n",
      "--------------------\n",
      "#from pyspark.sql.functions import lit\n",
      "--------------------\n",
      "#df.withColumn(\"name\",df.name.withField(\"fname\",lit(\"AA\"))).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#from pyspark.sql import Row\n",
      "--------------------\n",
      "#from pyspark.sql.functions import lit\n",
      "--------------------\n",
      "#df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      "--------------------\n",
      "#df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      "--------------------\n",
      "        \n",
      "--------------------\n",
      "#from pyspark.sql import Row\n",
      "--------------------\n",
      "#from pyspark.sql.functions import col, lit\n",
      "--------------------\n",
      "#df = spark.createDataFrame([\n",
      "--------------------\n",
      "#Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      "--------------------\n",
      "#df.withColumn('a', df['a'].dropFields('b')).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession,Row\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data=[(\"James\",23),(\"Ann\",40)]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "df.select(col(\"`name.fname`\")).show()\n",
      "--------------------\n",
      "df.select(df[\"`name.fname`\"]).show()\n",
      "--------------------\n",
      "df.withColumn(\"new_col\",col(\"`name.fname`\").substr(1,2)).show()\n",
      "--------------------\n",
      "df.filter(col(\"`name.fname`\").startswith(\"J\")).show()\n",
      "--------------------\n",
      "new_cols=(column.replace('.', '_') for column in df.columns)\n",
      "--------------------\n",
      "df2 = df.toDF(*new_cols)\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using DataFrame object\n",
      "--------------------\n",
      "df.select(df.gender).show()\n",
      "--------------------\n",
      "df.select(df[\"gender\"]).show()\n",
      "--------------------\n",
      "#Accessing column name with dot (with backticks)\n",
      "--------------------\n",
      "df.select(df[\"`name.fname`\"]).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Using SQL col() function\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "df.select(col(\"gender\")).show()\n",
      "--------------------\n",
      "#Accessing column name with dot (with backticks)\n",
      "--------------------\n",
      "df.select(col(\"`name.fname`\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Access struct column\n",
      "--------------------\n",
      "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
      "--------------------\n",
      "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(df.prop.hair).show()\n",
      "--------------------\n",
      "df.select(df[\"prop.hair\"]).show()\n",
      "--------------------\n",
      "df.select(col(\"prop.hair\")).show()\n",
      "--------------------\n",
      "df.select(col(\"prop.*\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Column operators\n",
      "--------------------\n",
      "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
      "--------------------\n",
      "df.select(df.col1 + df.col2).show()\n",
      "--------------------\n",
      "df.select(df.col1 - df.col2).show() \n",
      "--------------------\n",
      "df.select(df.col1 * df.col2).show()\n",
      "--------------------\n",
      "df.select(df.col1 / df.col2).show()\n",
      "--------------------\n",
      "df.select(df.col1 % df.col2).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(df.col2 > df.col3).show()\n",
      "--------------------\n",
      "df.select(df.col2 < df.col3).show()\n",
      "--------------------\n",
      "df.select(df.col2 == df.col3).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataDictionary = [\n",
      "--------------------\n",
      "        ('James',{'hair':'black','eye':'brown'}),\n",
      "--------------------\n",
      "        ('Michael',{'hair':'brown','eye':None}),\n",
      "--------------------\n",
      "        ('Robert',{'hair':'red','eye':'black'}),\n",
      "--------------------\n",
      "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
      "--------------------\n",
      "        ('Jefferson',{'hair':'brown','eye':''})\n",
      "--------------------\n",
      "        ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3=df.rdd.map(lambda x: \\\n",
      "--------------------\n",
      "    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n",
      "--------------------\n",
      "    .toDF([\"name\",\"hair\",\"eye\"])\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "df3.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n",
      "--------------------\n",
      "  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
      "--------------------\n",
      "  .drop(\"properties\") \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n",
      "--------------------\n",
      "  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n",
      "--------------------\n",
      "  .drop(\"properties\") \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Functions\n",
      "--------------------\n",
      "from pyspark.sql.functions import explode,map_keys,col\n",
      "--------------------\n",
      "keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
      "--------------------\n",
      "keysList = keysDF.rdd.map(lambda x:x[0]).collect()\n",
      "--------------------\n",
      "keyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\n",
      "--------------------\n",
      "df.select(df.name, *keyCols).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "data = [ (\"36636\",\"Finance\",3000,\"USA\"), \n",
      "--------------------\n",
      "    (\"40288\",\"Finance\",5000,\"IND\"), \n",
      "--------------------\n",
      "    (\"42114\",\"Sales\",3900,\"USA\"), \n",
      "--------------------\n",
      "    (\"39192\",\"Marketing\",2500,\"CAN\"), \n",
      "--------------------\n",
      "    (\"34534\",\"Sales\",6500,\"USA\") ]\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "     StructField('id', StringType(), True),\n",
      "--------------------\n",
      "     StructField('dept', StringType(), True),\n",
      "--------------------\n",
      "     StructField('salary', IntegerType(), True),\n",
      "--------------------\n",
      "     StructField('location', StringType(), True)\n",
      "--------------------\n",
      "     ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Convert scolumns to Map\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,lit,create_map\n",
      "--------------------\n",
      "df = df.withColumn(\"propertiesMap\",create_map(\n",
      "--------------------\n",
      "        lit(\"salary\"),col(\"salary\"),\n",
      "--------------------\n",
      "        lit(\"location\"),col(\"location\")\n",
      "--------------------\n",
      "        )).drop(\"salary\",\"location\")\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "         .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "         .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\", \"Sales\", 3000),\n",
      "--------------------\n",
      "    (\"Michael\", \"Sales\", 4600),\n",
      "--------------------\n",
      "    (\"Robert\", \"Sales\", 4100),\n",
      "--------------------\n",
      "    (\"Maria\", \"Finance\", 3000),\n",
      "--------------------\n",
      "    (\"James\", \"Sales\", 3000),\n",
      "--------------------\n",
      "    (\"Scott\", \"Finance\", 3300),\n",
      "--------------------\n",
      "    (\"Jen\", \"Finance\", 3900),\n",
      "--------------------\n",
      "    (\"Jeff\", \"Marketing\", 3000),\n",
      "--------------------\n",
      "    (\"Kumar\", \"Marketing\", 2000),\n",
      "--------------------\n",
      "    (\"Saif\", \"Sales\", 4100)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "columns = [\"Name\",\"Dept\",\"Salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=columns)\n",
      "--------------------\n",
      "df.distinct().show()\n",
      "--------------------\n",
      "print(\"Distinct Count: \" + str(df.distinct().count()))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using countDistrinct()\n",
      "--------------------\n",
      "from pyspark.sql.functions import countDistinct\n",
      "--------------------\n",
      "df2=df.select(countDistinct(\"Dept\",\"Salary\"))\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"Distinct Count of Department &amp; Salary: \"+ str(df2.collect()[0][0]))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"PERSON\")\n",
      "--------------------\n",
      "spark.sql(\"select distinct(count(*)) from PERSON\").show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataDictionary = [\n",
      "--------------------\n",
      "        ('James',{'hair':'black','eye':'brown'}),\n",
      "--------------------\n",
      "        ('Michael',{'hair':'brown','eye':None}),\n",
      "--------------------\n",
      "        ('Robert',{'hair':'red','eye':'black'}),\n",
      "--------------------\n",
      "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
      "--------------------\n",
      "        ('Jefferson',{'hair':'brown','eye':''})\n",
      "--------------------\n",
      "        ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using StructType schema\n",
      "--------------------\n",
      "from pyspark.sql.types import StructField, StructType, StringType, MapType,IntegerType\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "    StructField('name', StringType(), True),\n",
      "--------------------\n",
      "    StructField('properties', MapType(StringType(),StringType()),True)\n",
      "--------------------\n",
      "])\n",
      "--------------------\n",
      "df2 = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3=df.rdd.map(lambda x: \\\n",
      "--------------------\n",
      "    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n",
      "--------------------\n",
      "    .toDF([\"name\",\"hair\",\"eye\"])\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "df3.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n",
      "--------------------\n",
      "  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
      "--------------------\n",
      "  .drop(\"properties\") \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n",
      "--------------------\n",
      "  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n",
      "--------------------\n",
      "  .drop(\"properties\") \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Functions\n",
      "--------------------\n",
      "from pyspark.sql.functions import explode,map_keys,col\n",
      "--------------------\n",
      "keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
      "--------------------\n",
      "keysList = keysDF.rdd.map(lambda x:x[0]).collect()\n",
      "--------------------\n",
      "keyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\n",
      "--------------------\n",
      "df.select(df.name, *keyCols).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "Created on Sat Jan 11 19:38:27 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: sparkbyexamples.com\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession, Row\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"language\",\"users_count\"]\n",
      "--------------------\n",
      "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "rdd = spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfFromRDD1 = rdd.toDF()\n",
      "--------------------\n",
      "dfFromRDD1.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfFromRDD1 = rdd.toDF(columns)\n",
      "--------------------\n",
      "dfFromRDD1.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
      "--------------------\n",
      "dfFromRDD2.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfFromData2 = spark.createDataFrame(data).toDF(*columns)\n",
      "--------------------\n",
      "dfFromData2.printSchema()     \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rowData = map(lambda x: Row(*x), data) \n",
      "--------------------\n",
      "dfFromData3 = spark.createDataFrame(rowData,columns)\n",
      "--------------------\n",
      "dfFromData3.printSchema()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "Created on Sat Jan 11 19:38:27 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: sparkbyexamples.com\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession, Row\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Using List\n",
      "--------------------\n",
      "dept = [(\"Finance\",10), \n",
      "--------------------\n",
      "        (\"Marketing\",20), \n",
      "--------------------\n",
      "        (\"Sales\",30), \n",
      "--------------------\n",
      "        (\"IT\",40) \n",
      "--------------------\n",
      "      ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "deptColumns = [\"dept_name\",\"dept_id\"]\n",
      "--------------------\n",
      "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
      "--------------------\n",
      "deptDF.printSchema()\n",
      "--------------------\n",
      "deptDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "deptSchema = StructType([       \n",
      "--------------------\n",
      "    StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "    StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "    StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "deptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\n",
      "--------------------\n",
      "deptDF1.printSchema()\n",
      "--------------------\n",
      "deptDF1.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using list of Row type\n",
      "--------------------\n",
      "dept2 = [Row(\"Finance\",10), \n",
      "--------------------\n",
      "        Row(\"Marketing\",20), \n",
      "--------------------\n",
      "        Row(\"Sales\",30), \n",
      "--------------------\n",
      "        Row(\"IT\",40) \n",
      "--------------------\n",
      "      ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "deptDF2 = spark.createDataFrame(data=dept, schema = deptColumns)\n",
      "--------------------\n",
      "deptDF2.printSchema()\n",
      "--------------------\n",
      "deptDF2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Convert list to RDD\n",
      "--------------------\n",
      "rdd = spark.sparkContext.parallelize(dept)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "               .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "               .getOrCreate()\n",
      "--------------------\n",
      "data=[[\"1\"]]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data,[\"id\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#current_date() & current_timestamp()\n",
      "--------------------\n",
      "df.withColumn(\"current_date\",current_date()) \\\n",
      "--------------------\n",
      "  .withColumn(\"current_timestamp\",current_timestamp()) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#SQL\n",
      "--------------------\n",
      "spark.sql(\"select current_date(), current_timestamp()\") \\\n",
      "--------------------\n",
      "     .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Date & Timestamp into custom format\n",
      "--------------------\n",
      "df.withColumn(\"date_format\",date_format(current_date(),\"MM-dd-yyyy\")) \\\n",
      "--------------------\n",
      "  .withColumn(\"to_timestamp\",to_timestamp(current_timestamp(),\"MM-dd-yyyy HH mm ss SSS\")) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#SQL\n",
      "--------------------\n",
      "spark.sql(\"select date_format(current_date(),'MM-dd-yyyy') as date_format ,\" + \\\n",
      "--------------------\n",
      "          \"to_timestamp(current_timestamp(),'MM-dd-yyyy HH mm ss SSS') as to_timestamp\") \\\n",
      "--------------------\n",
      "     .show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
      "--------------------\n",
      "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
      "--------------------\n",
      "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
      "--------------------\n",
      "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Flatmap    \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "        .master(\"local[5]\").getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.range(0,20)\n",
      "--------------------\n",
      "print(df.rdd.getNumPartitions())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.write.mode(\"overwrite\").csv(\"c:/tmp/partition.csv\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = df.repartition(6)\n",
      "--------------------\n",
      "print(df2.rdd.getNumPartitions())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3 = df.coalesce(2)\n",
      "--------------------\n",
      "print(df3.rdd.getNumPartitions())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df4 = df.groupBy(\"id\").count()\n",
      "--------------------\n",
      "print(df4.rdd.getNumPartitions())\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col, lit\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(spark)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "               .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "               .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.createDataFrame([[\"1\"]],[\"id\"])\n",
      "--------------------\n",
      "df.select(current_date().alias(\"current_date\"), \\\n",
      "--------------------\n",
      "      date_format(current_date(),\"yyyy MM dd\").alias(\"yyyy MM dd\"), \\\n",
      "--------------------\n",
      "      date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\").alias(\"MM/dd/yyyy\"), \\\n",
      "--------------------\n",
      "      date_format(current_timestamp(),\"yyyy MMM dd\").alias(\"yyyy MMMM dd\"), \\\n",
      "--------------------\n",
      "      date_format(current_timestamp(),\"yyyy MMMM dd E\").alias(\"yyyy MMMM dd E\") \\\n",
      "--------------------\n",
      "   ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#SQL\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.sql(\"select current_date() as current_date, \"+\n",
      "--------------------\n",
      "      \"date_format(current_timestamp(),'yyyy MM dd') as yyyy_MM_dd, \"+\n",
      "--------------------\n",
      "      \"date_format(current_timestamp(),'MM/dd/yyyy hh:mm') as MM_dd_yyyy, \"+\n",
      "--------------------\n",
      "      \"date_format(current_timestamp(),'yyyy MMM dd') as yyyy_MMMM_dd, \"+\n",
      "--------------------\n",
      "      \"date_format(current_timestamp(),'yyyy MMMM dd E') as yyyy_MMMM_dd_E\").show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "               .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "               .getOrCreate()\n",
      "--------------------\n",
      "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#current_date()\n",
      "--------------------\n",
      "df.select(current_date().alias(\"current_date\")\n",
      "--------------------\n",
      "  ).show(1)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#date_format()\n",
      "--------------------\n",
      "df.select(col(\"input\"), \n",
      "--------------------\n",
      "    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\") \n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#to_date()\n",
      "--------------------\n",
      "df.select(col(\"input\"), \n",
      "--------------------\n",
      "    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\") \n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#datediff()\n",
      "--------------------\n",
      "df.select(col(\"input\"), \n",
      "--------------------\n",
      "    datediff(current_date(),col(\"input\")).alias(\"datediff\")  \n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#months_between()\n",
      "--------------------\n",
      "df.select(col(\"input\"), \n",
      "--------------------\n",
      "    months_between(current_date(),col(\"input\")).alias(\"months_between\")  \n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#trunc()\n",
      "--------------------\n",
      "df.select(col(\"input\"), \n",
      "--------------------\n",
      "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n",
      "--------------------\n",
      "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n",
      "--------------------\n",
      "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
      "--------------------\n",
      "   ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#add_months() , date_add(), date_sub()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(col(\"input\"), \n",
      "--------------------\n",
      "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
      "--------------------\n",
      "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
      "--------------------\n",
      "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
      "--------------------\n",
      "    date_sub(col(\"input\"),4).alias(\"date_sub\") \n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(col(\"input\"), \n",
      "--------------------\n",
      "     year(col(\"input\")).alias(\"year\"), \n",
      "--------------------\n",
      "     month(col(\"input\")).alias(\"month\"), \n",
      "--------------------\n",
      "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
      "--------------------\n",
      "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(col(\"input\"),  \n",
      "--------------------\n",
      "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
      "--------------------\n",
      "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
      "--------------------\n",
      "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
      "--------------------\n",
      "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#current_timestamp()\n",
      "--------------------\n",
      "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
      "--------------------\n",
      "  ).show(1,truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#to_timestamp()\n",
      "--------------------\n",
      "df2.select(col(\"input\"), \n",
      "--------------------\n",
      "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
      "--------------------\n",
      "  ).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#hour, minute,second\n",
      "--------------------\n",
      "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
      "--------------------\n",
      "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3.select(col(\"input\"), \n",
      "--------------------\n",
      "    hour(col(\"input\")).alias(\"hour\"), \n",
      "--------------------\n",
      "    minute(col(\"input\")).alias(\"minute\"),\n",
      "--------------------\n",
      "    second(col(\"input\")).alias(\"second\") \n",
      "--------------------\n",
      "  ).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "data = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(\n",
      "--------------------\n",
      "      col(\"date\"),\n",
      "--------------------\n",
      "      current_date().alias(\"current_date\"),\n",
      "--------------------\n",
      "      datediff(current_date(),col(\"date\")).alias(\"datediff\")\n",
      "--------------------\n",
      "    ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"datesDiff\", datediff(current_date(),col(\"date\"))) \\\n",
      "--------------------\n",
      "  .withColumn(\"montsDiff\", months_between(current_date(),col(\"date\"))) \\\n",
      "--------------------\n",
      "  .withColumn(\"montsDiff_round\",round(months_between(current_date(),col(\"date\")),2)) \\\n",
      "--------------------\n",
      "  .withColumn(\"yearsDiff\",months_between(current_date(),col(\"date\"))/lit(12)) \\\n",
      "--------------------\n",
      "  .withColumn(\"yearsDiff_round\",round(months_between(current_date(),col(\"date\"))/lit(12),2)) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data2 = [(\"1\",\"07-01-2019\"),(\"2\",\"06-24-2019\"),(\"3\",\"08-24-2019\")]  \n",
      "--------------------\n",
      "df2=spark.createDataFrame(data=data2,schema=[\"id\",\"date\"])\n",
      "--------------------\n",
      "df2.select(\n",
      "--------------------\n",
      "    to_date(col(\"date\"),\"MM-dd-yyyy\").alias(\"date\"),\n",
      "--------------------\n",
      "    current_date().alias(\"endDate\")\n",
      "--------------------\n",
      "    )\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#SQL\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.sql(\"select round(months_between('2019-07-01',current_date())/12,2) as years_diff\").show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import expr\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\", \"Sales\", 3000), \\\n",
      "--------------------\n",
      "    (\"Michael\", \"Sales\", 4600), \\\n",
      "--------------------\n",
      "    (\"Robert\", \"Sales\", 4100), \\\n",
      "--------------------\n",
      "    (\"Maria\", \"Finance\", 3000), \\\n",
      "--------------------\n",
      "    (\"James\", \"Sales\", 3000), \\\n",
      "--------------------\n",
      "    (\"Scott\", \"Finance\", 3300), \\\n",
      "--------------------\n",
      "    (\"Jen\", \"Finance\", 3900), \\\n",
      "--------------------\n",
      "    (\"Jeff\", \"Marketing\", 3000), \\\n",
      "--------------------\n",
      "    (\"Kumar\", \"Marketing\", 2000), \\\n",
      "--------------------\n",
      "    (\"Saif\", \"Sales\", 4100) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "columns= [\"employee_name\", \"department\", \"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "distinctDF = df.distinct()\n",
      "--------------------\n",
      "print(\"Distinct count: \"+str(distinctDF.count()))\n",
      "--------------------\n",
      "distinctDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = df.dropDuplicates()\n",
      "--------------------\n",
      "print(\"Distinct count: \"+str(df2.count()))\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
      "--------------------\n",
      "print(\"Distinct count of department salary : \"+str(dropDisDF.count()))\n",
      "--------------------\n",
      "dropDisDF.show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "simpleData = ((\"James\",\"\",\"Smith\",\"36636\",\"NewYork\",3100), \\\n",
      "--------------------\n",
      "    (\"Michael\",\"Rose\",\"\",\"40288\",\"California\",4300), \\\n",
      "--------------------\n",
      "    (\"Robert\",\"\",\"Williams\",\"42114\",\"Florida\",1400), \\\n",
      "--------------------\n",
      "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"Florida\",5500), \\\n",
      "--------------------\n",
      "    (\"Jen\",\"Mary\",\"Brown\",\"34561\",\"NewYork\",3000) \\\n",
      "--------------------\n",
      "  )\n",
      "--------------------\n",
      "columns= [\"firstname\",\"middlename\",\"lastname\",\"id\",\"location\",\"salary\"]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.drop(\"firstname\") \\\n",
      "--------------------\n",
      "  .printSchema()\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "df.drop(col(\"firstname\")) \\\n",
      "--------------------\n",
      "  .printSchema()  \n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "df.drop(df.firstname) \\\n",
      "--------------------\n",
      "  .printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.drop(\"firstname\",\"middlename\",\"lastname\") \\\n",
      "--------------------\n",
      "    .printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "cols = (\"firstname\",\"middlename\",\"lastname\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.drop(*cols) \\\n",
      "--------------------\n",
      "   .printSchema()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark: SparkSession = SparkSession.builder \\\n",
      "--------------------\n",
      "    .master(\"local[1]\") \\\n",
      "--------------------\n",
      "    .appName(\"SparkByExamples.com\") \\\n",
      "--------------------\n",
      "    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "filePath=\"resources/small_zipcode.csv\"\n",
      "--------------------\n",
      "df = spark.read.options(header='true', inferSchema='true') \\\n",
      "--------------------\n",
      "          .csv(filePath)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.na.drop().show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.na.drop(how=\"any\").show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.na.drop(subset=[\"population\",\"type\"]) \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.dropna().show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "Created on Sat Jan 11 19:38:27 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: sparkbyexamples.com\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "  StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "  StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "  StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "  ])\n",
      "--------------------\n",
      "df = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df1 = spark.sparkContext.parallelize([]).toDF(schema)\n",
      "--------------------\n",
      "df1.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = spark.createDataFrame([], schema)\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3 = spark.emptyDataFrame()\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Thu Oct 24 22:42:50 2019\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: Naveen\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "arrayData = [\n",
      "--------------------\n",
      "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
      "--------------------\n",
      "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
      "--------------------\n",
      "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
      "--------------------\n",
      "        ('Washington',None,None),\n",
      "--------------------\n",
      "        ('Jefferson',['1','2'],{})\n",
      "--------------------\n",
      "        ]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import explode\n",
      "--------------------\n",
      "df2 = df.select(df.name,explode(df.knownLanguages))\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import explode\n",
      "--------------------\n",
      "df3 = df.select(df.name,explode(df.properties))\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "df3.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import explode_outer\n",
      "--------------------\n",
      "\"\"\" with array \"\"\"\n",
      "--------------------\n",
      "df.select(df.name,explode_outer(df.knownLanguages)).show()\n",
      "--------------------\n",
      "\"\"\" with map \"\"\"\n",
      "--------------------\n",
      "df.select(df.name,explode_outer(df.properties)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import posexplode\n",
      "--------------------\n",
      "\"\"\" with array \"\"\"\n",
      "--------------------\n",
      "df.select(df.name,posexplode(df.knownLanguages)).show()\n",
      "--------------------\n",
      "\"\"\" with map \"\"\"\n",
      "--------------------\n",
      "df.select(df.name,posexplode(df.properties)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import posexplode_outer\n",
      "--------------------\n",
      "\"\"\" with array \"\"\"\n",
      "--------------------\n",
      "df.select(df.name,posexplode_outer(df.knownLanguages)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\" with map \"\"\"\n",
      "--------------------\n",
      "df.select(df.name,posexplode_outer(df.properties)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\"END\"\"\"\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Thu Oct 24 22:42:50 2019\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: Naveen\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import explode, flatten\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "arrayArrayData = [\n",
      "--------------------\n",
      "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
      "--------------------\n",
      "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
      "--------------------\n",
      "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\" \"\"\"\n",
      "--------------------\n",
      "df.select(df.name,explode(df.subjects)).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\" creates a single array from an array of arrays. \"\"\"\n",
      "--------------------\n",
      "df.select(df.name,flatten(df.subjects)).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\"END\"\"\"\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import expr\n",
      "--------------------\n",
      "#Concatenate columns\n",
      "--------------------\n",
      "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \n",
      "--------------------\n",
      "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
      "--------------------\n",
      "df.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Using CASE WHEN sql expression\n",
      "--------------------\n",
      "data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n",
      "--------------------\n",
      "columns = [\"name\",\"gender\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "df2 = df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
      "--------------------\n",
      "           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Add months from a value of another column\n",
      "--------------------\n",
      "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
      "--------------------\n",
      "df=spark.createDataFrame(data).toDF(\"date\",\"increment\") \n",
      "--------------------\n",
      "df.select(df.date,df.increment,\n",
      "--------------------\n",
      "     expr(\"add_months(date,increment)\")\n",
      "--------------------\n",
      "  .alias(\"inc_date\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Providing alias using 'as'\n",
      "--------------------\n",
      "df.select(df.date,df.increment,\n",
      "--------------------\n",
      "     expr(\"\"\"add_months(date,increment) as inc_date\"\"\")\n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Add\n",
      "--------------------\n",
      "df.select(df.date,df.increment,\n",
      "--------------------\n",
      "     expr(\"increment + 5 as new_increment\")\n",
      "--------------------\n",
      "  ).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n",
      "--------------------\n",
      "  .printSchema()\n",
      "--------------------\n",
      "#Use expr()  to filter the rows\n",
      "--------------------\n",
      "data=[(100,2),(200,3000),(500,500)] \n",
      "--------------------\n",
      "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
      "--------------------\n",
      "df.filter(expr(\"col1 == col2\")).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "spark: SparkSession = SparkSession.builder \\\n",
      "--------------------\n",
      "    .master(\"local[1]\") \\\n",
      "--------------------\n",
      "    .appName(\"SparkByExamples.com\") \\\n",
      "--------------------\n",
      "    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [\n",
      "--------------------\n",
      "    (\"James\",None,\"M\"),\n",
      "--------------------\n",
      "    (\"Anna\",\"NY\",\"F\"),\n",
      "--------------------\n",
      "    (\"Julia\",None,None)\n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"name\",\"state\",\"gender\"]\n",
      "--------------------\n",
      "df =spark.createDataFrame(data,columns)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter(\"state is NULL\").show()\n",
      "--------------------\n",
      "df.filter(df.state.isNull()).show()\n",
      "--------------------\n",
      "df.filter(col(\"state\").isNull()).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter(\"state IS NULL AND gender IS NULL\").show()\n",
      "--------------------\n",
      "df.filter(df.state.isNull() & df.gender.isNull()).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter(\"state is not NULL\").show()\n",
      "--------------------\n",
      "df.filter(\"NOT state is NULL\").show()\n",
      "--------------------\n",
      "df.filter(df.state.isNotNull()).show()\n",
      "--------------------\n",
      "df.filter(col(\"state\").isNotNull()).show()\n",
      "--------------------\n",
      "df.na.drop(subset=[\"state\"]).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"DATA\")\n",
      "--------------------\n",
      "spark.sql(\"SELECT * FROM DATA where STATE IS NULL\").show()\n",
      "--------------------\n",
      "spark.sql(\"SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL\").show()\n",
      "--------------------\n",
      "spark.sql(\"SELECT * FROM DATA where STATE IS NOT NULL\").show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sat Jun 13 21:08:30 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: NNK\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,array_contains\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "arrayStructureData = [\n",
      "--------------------\n",
      "        ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
      "--------------------\n",
      "        ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
      "--------------------\n",
      "        ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
      "--------------------\n",
      "        ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
      "--------------------\n",
      "        ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
      "--------------------\n",
      "        ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
      "--------------------\n",
      "        ]\n",
      "--------------------\n",
      "        \n",
      "--------------------\n",
      "arrayStructureSchema = StructType([\n",
      "--------------------\n",
      "        StructField('name', StructType([\n",
      "--------------------\n",
      "             StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "             StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "             StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "             ])),\n",
      "--------------------\n",
      "         StructField('languages', ArrayType(StringType()), True),\n",
      "--------------------\n",
      "         StructField('state', StringType(), True),\n",
      "--------------------\n",
      "         StructField('gender', StringType(), True)\n",
      "--------------------\n",
      "         ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = arrayStructureData, schema = arrayStructureSchema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Equals\n",
      "--------------------\n",
      "df.filter(df.state == \"OH\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Not equals\n",
      "--------------------\n",
      "df.filter(~(df.state == \"OH\")) \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "df.filter(df.state != \"OH\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)    \n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "df.filter(col(\"state\") == \"OH\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)    \n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "df.filter(\"gender  == 'M'\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)    \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter(\"gender  <> 'M'\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)    \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#IS IN\n",
      "--------------------\n",
      "li=[\"OH\",\"CA\",\"DE\"]\n",
      "--------------------\n",
      "df.filter(df.state.isin(li)).show()\n",
      "--------------------\n",
      "#IS NOT IN\n",
      "--------------------\n",
      "df.filter(~df.state.isin(li)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n",
      "--------------------\n",
      "    .show(truncate=False)        \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter(array_contains(df.languages,\"Java\")) \\\n",
      "--------------------\n",
      "    .show(truncate=False)        \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter(df.name.lastname == \"Williams\") \\\n",
      "--------------------\n",
      "    .show(truncate=False) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter(df.state.startswith(\"N\")).show()\n",
      "--------------------\n",
      "df.filter(df.state.endswith(\"H\")).show()\n",
      "--------------------\n",
      "df.filter(df.state.like(\"N%\")).show()\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data2 = [(1,\"James Smith\"), (2,\"Michael Rose\"),\n",
      "--------------------\n",
      "    (3,\"Robert Williams\"), (4,\"Rames Rose\"),(5,\"Rames rose\")\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2.filter(df2.name.like(\"%rose%\")).show()\n",
      "--------------------\n",
      "df2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "    .master(\"local[1]\") \\\n",
      "--------------------\n",
      "    .appName(\"SparkByExamples.com\") \\\n",
      "--------------------\n",
      "    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [\n",
      "--------------------\n",
      "    (\"James\",None,\"M\"),\n",
      "--------------------\n",
      "    (\"Anna\",\"NY\",\"F\"),\n",
      "--------------------\n",
      "    (\"Julia\",None,None)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"name\",\"state\",\"gender\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data,columns)\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.filter(\"state is NULL\").show()\n",
      "--------------------\n",
      "df.filter(df.state.isNull()).show()\n",
      "--------------------\n",
      "df.filter(col(\"state\").isNull()).show() \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.na.drop(\"state\").show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,sum,avg,max\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
      "--------------------\n",
      "    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n",
      "--------------------\n",
      "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
      "--------------------\n",
      "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
      "--------------------\n",
      "    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n",
      "--------------------\n",
      "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
      "--------------------\n",
      "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
      "--------------------\n",
      "    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n",
      "--------------------\n",
      "    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.groupBy(\"state\").sum(\"salary\").show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfGroup=df.groupBy(\"state\") \\\n",
      "--------------------\n",
      "          .agg(sum(\"salary\").alias(\"sum_salary\"))\n",
      "--------------------\n",
      "          \n",
      "--------------------\n",
      "dfGroup.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfFilter=dfGroup.filter(dfGroup.sum_salary > 100000)\n",
      "--------------------\n",
      "dfFilter.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import asc\n",
      "--------------------\n",
      "dfFilter.sort(\"sum_salary\").show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import desc\n",
      "--------------------\n",
      "dfFilter.sort(desc(\"sum_salary\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.groupBy(\"state\") \\\n",
      "--------------------\n",
      "  .agg(sum(\"salary\").alias(\"sum_salary\")) \\\n",
      "--------------------\n",
      "  .filter(col(\"sum_salary\") > 100000)  \\\n",
      "--------------------\n",
      "  .sort(desc(\"sum_salary\")) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"EMP\")\n",
      "--------------------\n",
      "spark.sql(\"select state, sum(salary) as sum_salary from EMP \" +\n",
      "--------------------\n",
      "          \"group by state having sum_salary > 100000 \" + \n",
      "--------------------\n",
      "          \"order by sum_salary desc\").show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.groupBy(\"state\") \\\n",
      "--------------------\n",
      "  .sum(\"salary\") \\\n",
      "--------------------\n",
      "  .withColumnRenamed(\"sum(salary)\", \"sum_salary\") \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.groupBy(\"state\") \\\n",
      "--------------------\n",
      "  .sum(\"salary\") \\\n",
      "--------------------\n",
      "  .select(col(\"state\"),col(\"sum(salary)\").alias(\"sum_salary\")) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: prabha\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,sum,avg,max\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
      "--------------------\n",
      "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
      "--------------------\n",
      "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
      "--------------------\n",
      "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
      "--------------------\n",
      "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
      "--------------------\n",
      "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
      "--------------------\n",
      "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
      "--------------------\n",
      "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
      "--------------------\n",
      "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.groupBy(\"department\").count().show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.groupBy(\"department\",\"state\") \\\n",
      "--------------------\n",
      "    .sum(\"salary\",\"bonus\") \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.groupBy(\"department\") \\\n",
      "--------------------\n",
      "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
      "--------------------\n",
      "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
      "--------------------\n",
      "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
      "--------------------\n",
      "         max(\"bonus\").alias(\"max_bonus\") \\\n",
      "--------------------\n",
      "     ) \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "df.groupBy(\"department\") \\\n",
      "--------------------\n",
      "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
      "--------------------\n",
      "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
      "--------------------\n",
      "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
      "--------------------\n",
      "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
      "--------------------\n",
      "    .where(col(\"sum_bonus\") >= 50000) \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "#EMP DataFrame\n",
      "--------------------\n",
      "empData = [(1,\"Smith\",10), (2,\"Rose\",20),\n",
      "--------------------\n",
      "    (3,\"Williams\",10), (4,\"Jones\",30)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "empColumns = [\"emp_id\",\"name\",\"emp_dept_id\"]\n",
      "--------------------\n",
      "empDF = spark.createDataFrame(empData,empColumns)\n",
      "--------------------\n",
      "empDF.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#DEPT DataFrame\n",
      "--------------------\n",
      "deptData = [(\"Finance\",10), (\"Marketing\",20),\n",
      "--------------------\n",
      "    (\"Sales\",30),(\"IT\",40)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "deptColumns = [\"dept_name\",\"dept_id\"]\n",
      "--------------------\n",
      "deptDF=spark.createDataFrame(deptData,deptColumns)  \n",
      "--------------------\n",
      "deptDF.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Address DataFrame\n",
      "--------------------\n",
      "addData=[(1,\"1523 Main St\",\"SFO\",\"CA\"),\n",
      "--------------------\n",
      "    (2,\"3453 Orange St\",\"SFO\",\"NY\"),\n",
      "--------------------\n",
      "    (3,\"34 Warner St\",\"Jersey\",\"NJ\"),\n",
      "--------------------\n",
      "    (4,\"221 Cavalier St\",\"Newark\",\"DE\"),\n",
      "--------------------\n",
      "    (5,\"789 Walnut St\",\"Sandiago\",\"CA\")\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "addColumns = [\"emp_id\",\"addline1\",\"city\",\"state\"]\n",
      "--------------------\n",
      "addDF = spark.createDataFrame(addData,addColumns)\n",
      "--------------------\n",
      "addDF.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Join two DataFrames\n",
      "--------------------\n",
      "empDF.join(addDF,empDF[\"emp_id\"] == addDF[\"emp_id\"]).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Drop duplicate column\n",
      "--------------------\n",
      "empDF.join(addDF,[\"emp_id\"]).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Join Multiple DataFrames\n",
      "--------------------\n",
      "empDF.join(addDF,[\"emp_id\"]) \\\n",
      "--------------------\n",
      "     .join(deptDF,empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
      "--------------------\n",
      "     .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Using Where for Join Condition\n",
      "--------------------\n",
      "empDF.join(deptDF).where(empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
      "--------------------\n",
      "    .join(addDF).where(empDF[\"emp_id\"] == addDF[\"emp_id\"]) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "#SQL\n",
      "--------------------\n",
      "empDF.createOrReplaceTempView(\"EMP\")\n",
      "--------------------\n",
      "deptDF.createOrReplaceTempView(\"DEPT\")\n",
      "--------------------\n",
      "addDF.createOrReplaceTempView(\"ADD\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.sql(\"select * from EMP e, DEPT d, ADD a \" + \\\n",
      "--------------------\n",
      "    \"where e.emp_dept_id == d.dept_id and e.emp_id == a.emp_id\") \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "#\n",
      "--------------------\n",
      "df1 = spark.createDataFrame(\n",
      "--------------------\n",
      "    [(1, \"A\"), (2, \"B\"), (3, \"C\")],\n",
      "--------------------\n",
      "    [\"A1\", \"A2\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = spark.createDataFrame(\n",
      "--------------------\n",
      "    [(1, \"F\"), (2, \"B\")], \n",
      "--------------------\n",
      "    [\"B1\", \"B2\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = df1.join(df2, (df1.A1 == df2.B1) & (df1.A2 == df2.B2))\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: prabha\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
      "--------------------\n",
      "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
      "--------------------\n",
      "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
      "--------------------\n",
      "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
      "--------------------\n",
      "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
      "--------------------\n",
      "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
      "--------------------\n",
      "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
      "--------------------\n",
      "empDF.printSchema()\n",
      "--------------------\n",
      "empDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dept = [(\"Finance\",10), \\\n",
      "--------------------\n",
      "    (\"Marketing\",20), \\\n",
      "--------------------\n",
      "    (\"Sales\",30), \\\n",
      "--------------------\n",
      "    (\"IT\",40) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "deptColumns = [\"dept_name\",\"dept_id\"]\n",
      "--------------------\n",
      "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
      "--------------------\n",
      "deptDF.printSchema()\n",
      "--------------------\n",
      "deptDF.show(truncate=False)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
      "--------------------\n",
      "     .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "   \n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "   \n",
      "--------------------\n",
      "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
      "--------------------\n",
      "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
      "--------------------\n",
      "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
      "--------------------\n",
      "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
      "--------------------\n",
      "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "empDF.createOrReplaceTempView(\"EMP\")\n",
      "--------------------\n",
      "deptDF.createOrReplaceTempView(\"DEPT\")\n",
      "--------------------\n",
      "   \n",
      "--------------------\n",
      "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "   \n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('machinelearninggeeks.com').getOrCreate()\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
      "--------------------\n",
      "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
      "--------------------\n",
      "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
      "--------------------\n",
      "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
      "--------------------\n",
      "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
      "--------------------\n",
      "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
      "--------------------\n",
      "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
      "--------------------\n",
      "empDF.printSchema()\n",
      "--------------------\n",
      "empDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dept = [(\"Finance\",10), \\\n",
      "--------------------\n",
      "    (\"Marketing\",20), \\\n",
      "--------------------\n",
      "    (\"Sales\",30), \\\n",
      "--------------------\n",
      "    (\"IT\",40) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "deptColumns = [\"dept_name\",\"dept_id\"]\n",
      "--------------------\n",
      "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
      "--------------------\n",
      "deptDF.printSchema()\n",
      "--------------------\n",
      "deptDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show(truncate=False)\n",
      "--------------------\n",
      "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "empDF.createOrReplaceTempView(\"EMP\")\n",
      "--------------------\n",
      "deptDF.createOrReplaceTempView(\"DEPT\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "joinDF2 = spark.sql(\"SELECT e.* FROM EMP e LEFT ANTI JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
      "--------------------\n",
      "  .show(truncate=False)    \n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
      "--------------------\n",
      "columns= [\"EmpId\",\"Salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,lit\n",
      "--------------------\n",
      "df2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import when\n",
      "--------------------\n",
      "df3 = df2.withColumn(\"lit_value2\", when(col(\"Salary\") >=40000 & col(\"Salary\") <= 50000,lit(\"100\")).otherwise(lit(\"200\")))\n",
      "--------------------\n",
      "df3.show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [('James','Smith','M',30),\n",
      "--------------------\n",
      "  ('Anna','Rose','F',41),\n",
      "--------------------\n",
      "  ('Robert','Williams','M',62), \n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data, schema = columns)\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import concat_ws,col,lit\n",
      "--------------------\n",
      "df.select(concat_ws(\",\",df.firstname,df.lastname).alias(\"name\"), \\\n",
      "--------------------\n",
      "          df.gender,lit(df.salary*2).alias(\"new_salary\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(df.collect())\n",
      "--------------------\n",
      "rdd=df.rdd.map(lambda x: \n",
      "--------------------\n",
      "    (x[0]+\",\"+x[1],x[2],x[3]*2)\n",
      "--------------------\n",
      "    )  \n",
      "--------------------\n",
      "df2=rdd.toDF([\"name\",\"gender\",\"new_salary\"]   )\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Referring Column Names\n",
      "--------------------\n",
      "rdd2=df.rdd.map(lambda x: \n",
      "--------------------\n",
      "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
      "--------------------\n",
      "    ) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Referring Column Names\n",
      "--------------------\n",
      "rdd2=df.rdd.map(lambda x: \n",
      "--------------------\n",
      "    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n",
      "--------------------\n",
      "    ) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "def func1(x):\n",
      "--------------------\n",
      "    firstName=x.firstname\n",
      "--------------------\n",
      "    lastName=x.lastName\n",
      "--------------------\n",
      "    name=firstName+\",\"+lastName\n",
      "--------------------\n",
      "    gender=x.gender.lower()\n",
      "--------------------\n",
      "    salary=x.salary*2\n",
      "--------------------\n",
      "    return (name,gender,salary)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd2=df.rdd.map(lambda x: func1(x))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Foeeach example\n",
      "--------------------\n",
      "def f(x): print(x)\n",
      "--------------------\n",
      "df.rdd.foreach(f)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.rdd.foreach(lambda x: \n",
      "--------------------\n",
      "    print(\"Data ==>\"+x[\"firstname\"]+\",\"+x[\"lastname\"]+\",\"+x[\"gender\"]+\",\"+str(x[\"salary\"]*2))\n",
      "--------------------\n",
      "    ) \n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "#Iterate collected data\n",
      "--------------------\n",
      "dataCollect = df.collect()\n",
      "--------------------\n",
      "for row in dataCollect:\n",
      "--------------------\n",
      "    print(row['firstname'] + \",\" +row['lastname'])\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "#Convert to Pandas and Iterate\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataCollect=df.rdd.toLocalIterator()\n",
      "--------------------\n",
      "for row in dataCollect:\n",
      "--------------------\n",
      "    print(row['firstname'] + \",\" +row['lastname'])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pandas as pd\n",
      "--------------------\n",
      "pandasDF = df.toPandas()\n",
      "--------------------\n",
      "for index, row in pandasDF.iterrows():\n",
      "--------------------\n",
      "    print(row['firstname'], row['gender'])\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "data = [('James','Smith','M',3000),\n",
      "--------------------\n",
      "  ('Anna','Rose','F',4100),\n",
      "--------------------\n",
      "  ('Robert','Williams','M',6200), \n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data, schema = columns)\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Example 1 mapPartitions()\n",
      "--------------------\n",
      "def reformat(partitionData):\n",
      "--------------------\n",
      "    for row in partitionData:\n",
      "--------------------\n",
      "        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\n",
      "--------------------\n",
      "df.rdd.mapPartitions(reformat).toDF().show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Example 2 mapPartitions()\n",
      "--------------------\n",
      "def reformat2(partitionData):\n",
      "--------------------\n",
      "  updatedData = []\n",
      "--------------------\n",
      "  for row in partitionData:\n",
      "--------------------\n",
      "    name=row.firstname+\",\"+row.lastname\n",
      "--------------------\n",
      "    bonus=row.salary*10/100\n",
      "--------------------\n",
      "    updatedData.append([name,bonus])\n",
      "--------------------\n",
      "  return iter(updatedData)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2=df.rdd.mapPartitions(reformat2).toDF(\"name\",\"bonus\")\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataDictionary = [\n",
      "--------------------\n",
      "        ('James',{'hair':'black','eye':'brown'}),\n",
      "--------------------\n",
      "        ('Michael',{'hair':'brown','eye':None}),\n",
      "--------------------\n",
      "        ('Robert',{'hair':'red','eye':'black'}),\n",
      "--------------------\n",
      "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
      "--------------------\n",
      "        ('Jefferson',{'hair':'brown','eye':''})\n",
      "--------------------\n",
      "        ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using StructType schema\n",
      "--------------------\n",
      "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "    StructField('name', StringType(), True),\n",
      "--------------------\n",
      "    StructField('properties', MapType(StringType(),StringType()),True)\n",
      "--------------------\n",
      "])\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3=df.rdd.map(lambda x: \\\n",
      "--------------------\n",
      "    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n",
      "--------------------\n",
      "    .toDF([\"name\",\"hair\",\"eye\"])\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "df3.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n",
      "--------------------\n",
      "  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
      "--------------------\n",
      "  .drop(\"properties\") \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n",
      "--------------------\n",
      "  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n",
      "--------------------\n",
      "  .drop(\"properties\") \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import explode\n",
      "--------------------\n",
      "df.select(df.name,explode(df.properties)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import map_keys\n",
      "--------------------\n",
      "df.select(df.name,map_keys(df.properties)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import map_values\n",
      "--------------------\n",
      "df.select(df.name,map_values(df.properties)).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#from pyspark.sql.functions import explode,map_keys\n",
      "--------------------\n",
      "#keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
      "--------------------\n",
      "#keysList = keysDF.rdd.map(lambda x:x[0]).collect()\n",
      "--------------------\n",
      "#print(keysList)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,sum,avg,max\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
      "--------------------\n",
      "    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n",
      "--------------------\n",
      "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
      "--------------------\n",
      "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
      "--------------------\n",
      "    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n",
      "--------------------\n",
      "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
      "--------------------\n",
      "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
      "--------------------\n",
      "    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n",
      "--------------------\n",
      "    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfSort=df.sort(df.state,df.salary).groupBy(df.state).agg(sum(df.salary))\n",
      "--------------------\n",
      "dfSort.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sat Jun 20 07:45:04 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: NNK\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col, asc,desc\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
      "--------------------\n",
      "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
      "--------------------\n",
      "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
      "--------------------\n",
      "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
      "--------------------\n",
      "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
      "--------------------\n",
      "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
      "--------------------\n",
      "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
      "--------------------\n",
      "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
      "--------------------\n",
      "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.sort(\"department\",\"state\").show(truncate=False)\n",
      "--------------------\n",
      "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
      "--------------------\n",
      "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
      "--------------------\n",
      "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
      "--------------------\n",
      "df.orderBy(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.sort(df.department.asc(),df.state.desc()).show(truncate=False)\n",
      "--------------------\n",
      "df.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
      "--------------------\n",
      "df.orderBy(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"EMP\")\n",
      "--------------------\n",
      "df.select(\"employee_name\",asc(\"department\"),desc(\"state\"),\"salary\",\"age\",\"bonus\").show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rddCollect = rdd.collect()\n",
      "--------------------\n",
      "print(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\n",
      "--------------------\n",
      "print(\"Action: First element: \"+str(rdd.first()))\n",
      "--------------------\n",
      "print(rddCollect)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "emptyRDD = spark.sparkContext.emptyRDD()\n",
      "--------------------\n",
      "emptyRDD2 = rdd=spark.sparkContext.parallelize([])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"\"+str(emptyRDD2.isEmpty()))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.read.option(\"header\",True) \\\n",
      "--------------------\n",
      "        .csv(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/simple-zipcodes.csv\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "          \n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "print(df.rdd.getNumPartitions())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.write.option(\"header\",True) \\\n",
      "--------------------\n",
      "        .partitionBy(\"state\") \\\n",
      "--------------------\n",
      "        .mode(\"overwrite\") \\\n",
      "--------------------\n",
      "        .csv(\"c:/tmp/zipcodes-state\")\n",
      "--------------------\n",
      "        \n",
      "--------------------\n",
      "df.write.option(\"header\",True) \\\n",
      "--------------------\n",
      "        .partitionBy(\"state\",\"city\") \\\n",
      "--------------------\n",
      "        .mode(\"overwrite\") \\\n",
      "--------------------\n",
      "        .csv(\"c:/tmp/zipcodes-state-city\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=df.repartition(2)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(df.rdd.getNumPartitions())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.write.option(\"header\",True) \\\n",
      "--------------------\n",
      "        .partitionBy(\"state\") \\\n",
      "--------------------\n",
      "        .mode(\"overwrite\") \\\n",
      "--------------------\n",
      "        .csv(\"c:/tmp/zipcodes-state-more\")\n",
      "--------------------\n",
      "        \n",
      "--------------------\n",
      "dfPartition=spark.read.option(\"header\",True)\\\n",
      "--------------------\n",
      "                 .csv(\"c:/tmp/zipcodes-state\")\n",
      "--------------------\n",
      "dfPartition.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfSinglePart=spark.read.option(\"header\",True) \\\n",
      "--------------------\n",
      "                  .csv(\"c:/tmp/zipcodes-state/state=AL/city=SPRINGVILLE\")\n",
      "--------------------\n",
      "dfSinglePart.printSchema()\n",
      "--------------------\n",
      "dfSinglePart.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "parqDF = spark.read.option(\"header\",True) \\\n",
      "--------------------\n",
      "                  .csv(\"c:/tmp/zipcodes-state\")\n",
      "--------------------\n",
      "parqDF.createOrReplaceTempView(\"ZIPCODE\")\n",
      "--------------------\n",
      "spark.sql(\"select * from ZIPCODE  where state='AL' and city = 'SPRINGVILLE'\") \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.write.option(\"header\",True) \\\n",
      "--------------------\n",
      "        .option(\"maxRecordsPerFile\", 2) \\\n",
      "--------------------\n",
      "        .partitionBy(\"state\") \\\n",
      "--------------------\n",
      "        .mode(\"overwrite\") \\\n",
      "--------------------\n",
      "        .csv(\"/tmp/zipcodes-state-maxrecords\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import expr\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
      "--------------------\n",
      "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
      "--------------------\n",
      "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
      "--------------------\n",
      "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns= [\"Product\",\"Amount\",\"Country\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
      "--------------------\n",
      "pivotDF.printSchema()\n",
      "--------------------\n",
      "pivotDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "pivotDF = df.groupBy(\"Product\",\"Country\") \\\n",
      "--------------------\n",
      "      .sum(\"Amount\") \\\n",
      "--------------------\n",
      "      .groupBy(\"Product\") \\\n",
      "--------------------\n",
      "      .pivot(\"Country\") \\\n",
      "--------------------\n",
      "      .sum(\"sum(Amount)\")\n",
      "--------------------\n",
      "pivotDF.printSchema()\n",
      "--------------------\n",
      "pivotDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\" unpivot \"\"\"\n",
      "--------------------\n",
      "\"\"\" unpivot \"\"\"\n",
      "--------------------\n",
      "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
      "--------------------\n",
      "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
      "--------------------\n",
      "    .where(\"Total is not null\")\n",
      "--------------------\n",
      "unPivotDF.show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dept = [(\"Finance\",10), \\\n",
      "--------------------\n",
      "    (\"Marketing\",20), \\\n",
      "--------------------\n",
      "    (\"Sales\",30), \\\n",
      "--------------------\n",
      "    (\"IT\",40) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize(dept)\n",
      "--------------------\n",
      "print(rdd)\n",
      "--------------------\n",
      "dataColl=rdd.collect()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "for row in dataColl:\n",
      "--------------------\n",
      "    print(row[0] + \",\" +str(row[1]))\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "deptColumns = [\"dept_name\",\"dept_id\"]\n",
      "--------------------\n",
      "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
      "--------------------\n",
      "deptDF.printSchema()\n",
      "--------------------\n",
      "deptDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataCollect = deptDF.collect()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(dataCollect)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataCollect2 = deptDF.select(\"dept_name\").collect()\n",
      "--------------------\n",
      "print(dataCollect2)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "for row in dataCollect:\n",
      "--------------------\n",
      "    print(row['dept_name'] + \",\" +str(row['dept_id']))\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sat Jun 13 21:08:30 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: NNK\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
      "--------------------\n",
      "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
      "--------------------\n",
      "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
      "--------------------\n",
      "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
      "--------------------\n",
      "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "pysparkDF.printSchema()\n",
      "--------------------\n",
      "pysparkDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "pandasDF = pysparkDF.toPandas()\n",
      "--------------------\n",
      "print(pandasDF)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Nested structure elements\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
      "--------------------\n",
      "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
      "--------------------\n",
      "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
      "--------------------\n",
      "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
      "--------------------\n",
      "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
      "--------------------\n",
      "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schemaStruct = StructType([\n",
      "--------------------\n",
      "        StructField('name', StructType([\n",
      "--------------------\n",
      "             StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "             StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "             StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "             ])),\n",
      "--------------------\n",
      "          StructField('dob', StringType(), True),\n",
      "--------------------\n",
      "         StructField('gender', StringType(), True),\n",
      "--------------------\n",
      "         StructField('salary', StringType(), True)\n",
      "--------------------\n",
      "         ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "pandasDF2 = df.toPandas()\n",
      "--------------------\n",
      "print(pandasDF2)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "          \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(1,10),(2,20),(3,10),(4,20),(5,10),\n",
      "--------------------\n",
      "    (6,30),(7,50),(8,50),(9,50),(10,30),\n",
      "--------------------\n",
      "    (11,10),(12,10),(13,40),(14,40),(15,40),\n",
      "--------------------\n",
      "    (16,40),(17,50),(18,10),(19,40),(20,40)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.createDataFrame(data,[\"id\",\"value\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.repartition(3,\"value\").explain(True)        \n",
      "--------------------\n",
      "df.repartition(\"value\") \\\n",
      "--------------------\n",
      "  .write.option(\"header\",True) \\\n",
      "--------------------\n",
      "  .mode(\"overwrite\") \\\n",
      "--------------------\n",
      "  .csv(\"c:/tmp/range-partition\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.repartitionByRange(\"value\").explain(True)\n",
      "--------------------\n",
      "df.repartitionByRange(3,\"value\").explain(True)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.repartitionByRange(3,\"value\") \\\n",
      "--------------------\n",
      "  .write.option(\"header\",True) \\\n",
      "--------------------\n",
      "  .mode(\"overwrite\") \\\n",
      "--------------------\n",
      "  .csv(\"c:/tmp/range-partition-count\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
      "--------------------\n",
      "inputRDD = spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#aggregate\n",
      "--------------------\n",
      "seqOp = (lambda x, y: x + y)\n",
      "--------------------\n",
      "combOp = (lambda x, y: x + y)\n",
      "--------------------\n",
      "agg=listRdd.aggregate(0, seqOp, combOp)\n",
      "--------------------\n",
      "print(agg) # output 20\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#aggregate 2\n",
      "--------------------\n",
      "seqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      "--------------------\n",
      "combOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      "--------------------\n",
      "agg2=listRdd.aggregate((0, 0), seqOp2, combOp2)\n",
      "--------------------\n",
      "print(agg2) # output (20,7)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "agg2=listRdd.treeAggregate(0,seqOp, combOp)\n",
      "--------------------\n",
      "print(agg2) # output 20\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#fold\n",
      "--------------------\n",
      "from operator import add\n",
      "--------------------\n",
      "foldRes=listRdd.fold(0, add)\n",
      "--------------------\n",
      "print(foldRes) # output 20\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#reduce\n",
      "--------------------\n",
      "redRes=listRdd.reduce(add)\n",
      "--------------------\n",
      "print(redRes) # output 20\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#treeReduce. This is similar to reduce\n",
      "--------------------\n",
      "add = lambda x, y: x + y\n",
      "--------------------\n",
      "redRes=listRdd.treeReduce(add)\n",
      "--------------------\n",
      "print(redRes) # output 20\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Collect\n",
      "--------------------\n",
      "data = listRdd.collect()\n",
      "--------------------\n",
      "print(data)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#count, countApprox, countApproxDistinct\n",
      "--------------------\n",
      "print(\"Count : \"+str(listRdd.count()))\n",
      "--------------------\n",
      "#Output: Count : 20\n",
      "--------------------\n",
      "print(\"countApprox : \"+str(listRdd.countApprox(1200)))\n",
      "--------------------\n",
      "#Output: countApprox : (final: [7.000, 7.000])\n",
      "--------------------\n",
      "print(\"countApproxDistinct : \"+str(listRdd.countApproxDistinct()))\n",
      "--------------------\n",
      "#Output: countApproxDistinct : 5\n",
      "--------------------\n",
      "print(\"countApproxDistinct : \"+str(inputRDD.countApproxDistinct()))\n",
      "--------------------\n",
      "#Output: countApproxDistinct : 5\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#countByValue, countByValueApprox\n",
      "--------------------\n",
      "print(\"countByValue :  \"+str(listRdd.countByValue()))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#first\n",
      "--------------------\n",
      "print(\"first :  \"+str(listRdd.first()))\n",
      "--------------------\n",
      "#Output: first :  1\n",
      "--------------------\n",
      "print(\"first :  \"+str(inputRDD.first()))\n",
      "--------------------\n",
      "#Output: first :  (Z,1)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#top\n",
      "--------------------\n",
      "print(\"top : \"+str(listRdd.top(2)))\n",
      "--------------------\n",
      "#Output: take : 5,4\n",
      "--------------------\n",
      "print(\"top : \"+str(inputRDD.top(2)))\n",
      "--------------------\n",
      "#Output: take : (Z,1),(C,40)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#min\n",
      "--------------------\n",
      "print(\"min :  \"+str(listRdd.min()))\n",
      "--------------------\n",
      "#Output: min :  1\n",
      "--------------------\n",
      "print(\"min :  \"+str(inputRDD.min()))\n",
      "--------------------\n",
      "#Output: min :  (A,20)  \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#max\n",
      "--------------------\n",
      "print(\"max :  \"+str(listRdd.max()))\n",
      "--------------------\n",
      "#Output: max :  5\n",
      "--------------------\n",
      "print(\"max :  \"+str(inputRDD.max()))\n",
      "--------------------\n",
      "#Output: max :  (Z,1)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#take, takeOrdered, takeSample\n",
      "--------------------\n",
      "print(\"take : \"+str(listRdd.take(2)))\n",
      "--------------------\n",
      "#Output: take : 1,2\n",
      "--------------------\n",
      "print(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))\n",
      "--------------------\n",
      "#Output: takeOrdered : 1,2\n",
      "--------------------\n",
      "print(\"take : \"+str(listRdd.takeSample()))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "Created on Sat Jan 11 19:38:27 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: sparkbyexamples.com\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
      "--------------------\n",
      "broadcastStates = spark.sparkContext.broadcast(states)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
      "--------------------\n",
      "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
      "--------------------\n",
      "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
      "--------------------\n",
      "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd = spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "def state_convert(code):\n",
      "--------------------\n",
      "    return broadcastStates.value[code]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
      "--------------------\n",
      "print(result)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [\"Project Gutenberg’s\",\n",
      "--------------------\n",
      "        \"Alice’s Adventures in Wonderland\",\n",
      "--------------------\n",
      "        \"Project Gutenberg’s\",\n",
      "--------------------\n",
      "        \"Adventures in Wonderland\",\n",
      "--------------------\n",
      "        \"Project Gutenberg’s\"]\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "for element in rdd.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Flatmap    \n",
      "--------------------\n",
      "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
      "--------------------\n",
      "for element in rdd2.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [\"Project\",\n",
      "--------------------\n",
      "\"Gutenberg’s\",\n",
      "--------------------\n",
      "\"Alice’s\",\n",
      "--------------------\n",
      "\"Adventures\",\n",
      "--------------------\n",
      "\"in\",\n",
      "--------------------\n",
      "\"Wonderland\",\n",
      "--------------------\n",
      "\"Project\",\n",
      "--------------------\n",
      "\"Gutenberg’s\",\n",
      "--------------------\n",
      "\"Adventures\",\n",
      "--------------------\n",
      "\"in\",\n",
      "--------------------\n",
      "\"Wonderland\",\n",
      "--------------------\n",
      "\"Project\",\n",
      "--------------------\n",
      "\"Gutenberg’s\"]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd2=rdd.map(lambda x: (x,1))\n",
      "--------------------\n",
      "for element in rdd2.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "data = [('James','Smith','M',30),\n",
      "--------------------\n",
      "  ('Anna','Rose','F',41),\n",
      "--------------------\n",
      "  ('Robert','Williams','M',62), \n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data, schema = columns)\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd2=df.rdd.map(lambda x: \n",
      "--------------------\n",
      "    (x[0]+\",\"+x[1],x[2],x[3]*2)\n",
      "--------------------\n",
      "    )  \n",
      "--------------------\n",
      "df2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Referring Column Names\n",
      "--------------------\n",
      "rdd2=df.rdd.map(lambda x: \n",
      "--------------------\n",
      "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
      "--------------------\n",
      "    ) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Referring Column Names\n",
      "--------------------\n",
      "rdd2=df.rdd.map(lambda x: \n",
      "--------------------\n",
      "    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n",
      "--------------------\n",
      "    ) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "def func1(x):\n",
      "--------------------\n",
      "    firstName=x.firstname\n",
      "--------------------\n",
      "    lastName=x.lastname\n",
      "--------------------\n",
      "    name=firstName+\",\"+lastName\n",
      "--------------------\n",
      "    gender=x.gender.lower()\n",
      "--------------------\n",
      "    salary=x.salary*2\n",
      "--------------------\n",
      "    return (name,gender,salary)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd2=df.rdd.map(lambda x: func1(x)).toDF().show()\n",
      "--------------------\n",
      "rdd2=df.rdd.map(func1).toDF().show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [('Project', 1),\n",
      "--------------------\n",
      "('Gutenberg’s', 1),\n",
      "--------------------\n",
      "('Alice’s', 1),\n",
      "--------------------\n",
      "('Adventures', 1),\n",
      "--------------------\n",
      "('in', 1),\n",
      "--------------------\n",
      "('Wonderland', 1),\n",
      "--------------------\n",
      "('Project', 1),\n",
      "--------------------\n",
      "('Gutenberg’s', 1),\n",
      "--------------------\n",
      "('Adventures', 1),\n",
      "--------------------\n",
      "('in', 1),\n",
      "--------------------\n",
      "('Wonderland', 1),\n",
      "--------------------\n",
      "('Project', 1),\n",
      "--------------------\n",
      "('Gutenberg’s', 1)]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd2=rdd.reduceByKey(lambda a,b: a+b)\n",
      "--------------------\n",
      "for element in rdd2.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "Created on Sat Jan 11 19:38:27 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: sparkbyexamples.com\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dept = [(\"Finance\",10), \n",
      "--------------------\n",
      "        (\"Marketing\",20), \n",
      "--------------------\n",
      "        (\"Sales\",30), \n",
      "--------------------\n",
      "        (\"IT\",40) \n",
      "--------------------\n",
      "      ]\n",
      "--------------------\n",
      "rdd = spark.sparkContext.parallelize(dept)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = rdd.toDF()\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "deptColumns = [\"dept_name\",\"dept_id\"]\n",
      "--------------------\n",
      "df2 = rdd.toDF(deptColumns)\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
      "--------------------\n",
      "deptDF.printSchema()\n",
      "--------------------\n",
      "deptDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType\n",
      "--------------------\n",
      "deptSchema = StructType([       \n",
      "--------------------\n",
      "    StructField('dept_name', StringType(), True),\n",
      "--------------------\n",
      "    StructField('dept_id', StringType(), True)\n",
      "--------------------\n",
      "])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "deptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\n",
      "--------------------\n",
      "deptDF1.printSchema()\n",
      "--------------------\n",
      "deptDF1.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "data = [\"Project Gutenberg’s\",\n",
      "--------------------\n",
      "        \"Alice’s Adventures in Wonderland\",\n",
      "--------------------\n",
      "        \"Project Gutenberg’s\",\n",
      "--------------------\n",
      "        \"Adventures in Wonderland\",\n",
      "--------------------\n",
      "        \"Project Gutenberg’s\"]\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "for element in rdd.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Flatmap    \n",
      "--------------------\n",
      "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
      "--------------------\n",
      "for element in rdd2.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "#map\n",
      "--------------------\n",
      "rdd3=rdd2.map(lambda x: (x,1))\n",
      "--------------------\n",
      "for element in rdd3.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "#reduceByKey\n",
      "--------------------\n",
      "rdd4=rdd3.reduceByKey(lambda a,b: a+b)\n",
      "--------------------\n",
      "for element in rdd4.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "#map\n",
      "--------------------\n",
      "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
      "--------------------\n",
      "for element in rdd5.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "#filter\n",
      "--------------------\n",
      "rdd6 = rdd5.filter(lambda x : 'a' in x[1])\n",
      "--------------------\n",
      "for element in rdd6.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,expr\n",
      "--------------------\n",
      "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
      "--------------------\n",
      "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
      "--------------------\n",
      "    .select(col(\"date\"),col(\"increment\"), \\\n",
      "--------------------\n",
      "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "rdd = spark.sparkContext.textFile(\"/apps/sparkbyexamples/src/pyspark-examples/data.txt\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "for element in rdd.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Flatmap    \n",
      "--------------------\n",
      "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
      "--------------------\n",
      "for element in rdd2.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "#map\n",
      "--------------------\n",
      "rdd3=rdd2.map(lambda x: (x,1))\n",
      "--------------------\n",
      "for element in rdd3.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "#reduceByKey\n",
      "--------------------\n",
      "rdd4=rdd3.reduceByKey(lambda a,b: a+b)\n",
      "--------------------\n",
      "for element in rdd4.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "#map\n",
      "--------------------\n",
      "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
      "--------------------\n",
      "for element in rdd5.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "#filter\n",
      "--------------------\n",
      "rdd6 = rdd5.filter(lambda x : 'a' in x[1])\n",
      "--------------------\n",
      "for element in rdd6.collect():\n",
      "--------------------\n",
      "    print(element)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(spark)\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize([1,2,3,4,56])\n",
      "--------------------\n",
      "print(\"RDD count :\"+str(rdd.count()))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd = spark.sparkContext.emptyRDD\n",
      "--------------------\n",
      "print(rdd)\n",
      "--------------------\n",
      "rdd2 = spark.sparkContext.parallelize([])\n",
      "--------------------\n",
      "print(rdd2)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sat Jun 13 21:08:30 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: NNK\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
      "--------------------\n",
      "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,array_contains\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.read.csv(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/zipcodes.csv\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = spark.read.option(\"header\",True) \\\n",
      "--------------------\n",
      "     .csv(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/zipcodes.csv\")\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "   \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3 = spark.read.options(header='True', delimiter=',') \\\n",
      "--------------------\n",
      "  .csv(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/zipcodes.csv\")\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType() \\\n",
      "--------------------\n",
      "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Zipcode\",IntegerType(),True) \\\n",
      "--------------------\n",
      "      .add(\"ZipCodeType\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"City\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"State\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"LocationType\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Lat\",DoubleType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Long\",DoubleType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Xaxis\",IntegerType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Yaxis\",DoubleType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Zaxis\",DoubleType(),True) \\\n",
      "--------------------\n",
      "      .add(\"WorldRegion\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Country\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"LocationText\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Location\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Decommisioned\",BooleanType(),True) \\\n",
      "--------------------\n",
      "      .add(\"TaxReturnsFiled\",StringType(),True) \\\n",
      "--------------------\n",
      "      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n",
      "--------------------\n",
      "      .add(\"TotalWages\",IntegerType(),True) \\\n",
      "--------------------\n",
      "      .add(\"Notes\",StringType(),True)\n",
      "--------------------\n",
      "      \n",
      "--------------------\n",
      "df_with_schema = spark.read.format(\"csv\") \\\n",
      "--------------------\n",
      "      .option(\"header\", True) \\\n",
      "--------------------\n",
      "      .schema(schema) \\\n",
      "--------------------\n",
      "      .load(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/zipcodes.csv\")\n",
      "--------------------\n",
      "df_with_schema.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2.write.option(\"header\",True) \\\n",
      "--------------------\n",
      " .csv(\"/tmp/spark_output/zipcodes123\")\n",
      "--------------------\n",
      " \n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,BooleanType,DoubleType\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "    .master(\"local[1]\") \\\n",
      "--------------------\n",
      "    .appName(\"SparkByExamples.com\") \\\n",
      "--------------------\n",
      "    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Read JSON file into dataframe    \n",
      "--------------------\n",
      "df = spark.read.json(\"resources/zipcodes.json\")\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Read multiline json file\n",
      "--------------------\n",
      "multiline_df = spark.read.option(\"multiline\",\"true\") \\\n",
      "--------------------\n",
      "      .json(\"resources/multiline-zipcode.json\")\n",
      "--------------------\n",
      "multiline_df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Read multiple files\n",
      "--------------------\n",
      "df2 = spark.read.json(\n",
      "--------------------\n",
      "    ['resources/zipcode2.json','resources/zipcode1.json'])\n",
      "--------------------\n",
      "df2.show()    \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Read All JSON files from a directory\n",
      "--------------------\n",
      "df3 = spark.read.json(\"resources/*.json\")\n",
      "--------------------\n",
      "df3.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Define custom schema\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "      StructField(\"RecordNumber\",IntegerType(),True),\n",
      "--------------------\n",
      "      StructField(\"Zipcode\",IntegerType(),True),\n",
      "--------------------\n",
      "      StructField(\"ZipCodeType\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"City\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"State\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"LocationType\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"Lat\",DoubleType(),True),\n",
      "--------------------\n",
      "      StructField(\"Long\",DoubleType(),True),\n",
      "--------------------\n",
      "      StructField(\"Xaxis\",IntegerType(),True),\n",
      "--------------------\n",
      "      StructField(\"Yaxis\",DoubleType(),True),\n",
      "--------------------\n",
      "      StructField(\"Zaxis\",DoubleType(),True),\n",
      "--------------------\n",
      "      StructField(\"WorldRegion\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"Country\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"LocationText\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"Location\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"Decommisioned\",BooleanType(),True),\n",
      "--------------------\n",
      "      StructField(\"TaxReturnsFiled\",StringType(),True),\n",
      "--------------------\n",
      "      StructField(\"EstimatedPopulation\",IntegerType(),True),\n",
      "--------------------\n",
      "      StructField(\"TotalWages\",IntegerType(),True),\n",
      "--------------------\n",
      "      StructField(\"Notes\",StringType(),True)\n",
      "--------------------\n",
      "  ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df_with_schema = spark.read.schema(schema) \\\n",
      "--------------------\n",
      "        .json(\"resources/zipcodes.json\")\n",
      "--------------------\n",
      "df_with_schema.printSchema()\n",
      "--------------------\n",
      "df_with_schema.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create a table from Parquet File\n",
      "--------------------\n",
      "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW zipcode3 USING json OPTIONS\" + \n",
      "--------------------\n",
      "      \" (path 'resources/zipcodes.json')\")\n",
      "--------------------\n",
      "spark.sql(\"select * from zipcode3\").show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# PySpark write Parquet File\n",
      "--------------------\n",
      "df2.write.mode('Overwrite').json(\"/tmp/spark_output/zipcodes.json\")\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "Created on Sat Jan 11 19:38:27 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: sparkbyexamples.com\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
      "--------------------\n",
      "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
      "--------------------\n",
      "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
      "--------------------\n",
      "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
      "--------------------\n",
      "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "        StructField('name', StructType([\n",
      "--------------------\n",
      "             StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "             StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "             StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "             ])),\n",
      "--------------------\n",
      "         StructField('dob', StringType(), True),\n",
      "--------------------\n",
      "         StructField('gender', StringType(), True),\n",
      "--------------------\n",
      "         StructField('salary', IntegerType(), True)\n",
      "--------------------\n",
      "         ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "''' Example 1 '''\n",
      "--------------------\n",
      "df.withColumnRenamed(\"dob\",\"DateOfBirth\").printSchema()\n",
      "--------------------\n",
      "''' Example 2 '''    \n",
      "--------------------\n",
      "df2 = df.withColumnRenamed(\"dob\",\"DateOfBirth\") \\\n",
      "--------------------\n",
      "    .withColumnRenamed(\"salary\",\"salary_amount\")\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "''' Example 3 '''\n",
      "--------------------\n",
      "schema2 = StructType([\n",
      "--------------------\n",
      "    StructField(\"fname\",StringType()),\n",
      "--------------------\n",
      "    StructField(\"middlename\",StringType()),\n",
      "--------------------\n",
      "    StructField(\"lname\",StringType())])\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "df.select(col(\"name\").cast(schema2),\n",
      "--------------------\n",
      "  col(\"dob\"),\n",
      "--------------------\n",
      "  col(\"gender\"),\n",
      "--------------------\n",
      "  col(\"salary\")) \\\n",
      "--------------------\n",
      "    .printSchema()    \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "''' Example 4 '''\n",
      "--------------------\n",
      "df.select(col(\"name.firstname\").alias(\"fname\"),\n",
      "--------------------\n",
      "  col(\"name.middlename\").alias(\"mname\"),\n",
      "--------------------\n",
      "  col(\"name.lastname\").alias(\"lname\"),\n",
      "--------------------\n",
      "  col(\"dob\"),col(\"gender\"),col(\"salary\")) \\\n",
      "--------------------\n",
      "  .printSchema()\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "''' Example 5 '''  \n",
      "--------------------\n",
      "df4 = df.withColumn(\"fname\",col(\"name.firstname\")) \\\n",
      "--------------------\n",
      "      .withColumn(\"mname\",col(\"name.middlename\")) \\\n",
      "--------------------\n",
      "      .withColumn(\"lname\",col(\"name.lastname\")) \\\n",
      "--------------------\n",
      "      .drop(\"name\")\n",
      "--------------------\n",
      "df4.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "''' Example 6\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "not working\n",
      "--------------------\n",
      "val old_columns = Seq(\"dob\",\"gender\",\"salary\",\"fname\",\"mname\",\"lname\")\n",
      "--------------------\n",
      "    val new_columns = Seq(\"DateOfBirth\",\"Sex\",\"salary\",\"firstName\",\"middleName\",\"lastName\")\n",
      "--------------------\n",
      "    val columnsList = old_columns.zip(new_columns).map(f=&gt;{col(f._1).as(f._2)})\n",
      "--------------------\n",
      "    val df5 = df4.select(columnsList:_*)\n",
      "--------------------\n",
      "    df5.printSchema()\n",
      "--------------------\n",
      "'''\n",
      "--------------------\n",
      "''' Example 7\n",
      "--------------------\n",
      "not working \n",
      "--------------------\n",
      "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
      "--------------------\n",
      "df.toDF(newColumns) \\\n",
      "--------------------\n",
      ".printSchema()\n",
      "--------------------\n",
      "'''        \n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "    .master(\"local[1]\") \\\n",
      "--------------------\n",
      "    .appName(\"SparkByExamples.com\") \\\n",
      "--------------------\n",
      "    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "filePath=\"resources/small_zipcode.csv\"\n",
      "--------------------\n",
      "df = spark.read.options(header='true', inferSchema='true') \\\n",
      "--------------------\n",
      "          .csv(filePath)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.fillna(value=0).show()\n",
      "--------------------\n",
      "df.fillna(value=0,subset=[\"population\"]).show()\n",
      "--------------------\n",
      "df.na.fill(value=0).show()\n",
      "--------------------\n",
      "df.na.fill(value=0,subset=[\"population\"]).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.fillna(value=\"\").show()\n",
      "--------------------\n",
      "df.na.fill(value=\"\").show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.fillna(\"unknown\",[\"city\"]) \\\n",
      "--------------------\n",
      "    .fillna(\"\",[\"type\"]).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.fillna({\"city\": \"unknown\", \"type\": \"\"}) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.na.fill(\"unknown\",[\"city\"]) \\\n",
      "--------------------\n",
      "    .na.fill(\"\",[\"type\"]).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.na.fill({\"city\": \"unknown\", \"type\": \"\"}) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.read.option(\"header\",True) \\\n",
      "--------------------\n",
      "        .csv(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/simple-zipcodes.csv\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "newDF=df.repartition(3)\n",
      "--------------------\n",
      "print(newDF.rdd.getNumPartitions())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "newDF.write.option(\"header\",True).mode(\"overwrite\") \\\n",
      "--------------------\n",
      "        .csv(\"/tmp/zipcodes-state\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2=df.repartition(3,\"state\")\n",
      "--------------------\n",
      "df2.write.option(\"header\",True).mode(\"overwrite\") \\\n",
      "--------------------\n",
      "   .csv(\"/tmp/zipcodes-state-3states\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3=df.repartition(\"state\")\n",
      "--------------------\n",
      "df3.write.option(\"header\",True).mode(\"overwrite\") \\\n",
      "--------------------\n",
      "   .csv(\"/tmp/zipcodes-state-allstates\")\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "        .master(\"local[5]\").getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.range(0,20)\n",
      "--------------------\n",
      "print(df.rdd.getNumPartitions())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd = spark.sparkContext.parallelize((0,20))\n",
      "--------------------\n",
      "print(\"From local[5]\"+str(rdd.getNumPartitions()))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd1 = spark.sparkContext.parallelize((0,25), 6)\n",
      "--------------------\n",
      "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\"rddFromFile = spark.sparkContext.textFile(\"src/main/resources/test.txt\",10)\n",
      "--------------------\n",
      "print(\"TextFile : \"+str(rddFromFile.getNumPartitions())) \"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd1.saveAsTextFile(\"c://tmp/partition2\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd2 = rdd1.repartition(4)\n",
      "--------------------\n",
      "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
      "--------------------\n",
      "rdd2.saveAsTextFile(\"c://tmp/re-partition2\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd3 = rdd1.coalesce(4)\n",
      "--------------------\n",
      "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
      "--------------------\n",
      "rdd3.saveAsTextFile(\"c:/tmp/coalesce2\")\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession, Row\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "row=Row(\"James\",40)\n",
      "--------------------\n",
      "print(row[0] +\",\"+str(row[1]))\n",
      "--------------------\n",
      "row2=Row(name=\"Alice\", age=11)\n",
      "--------------------\n",
      "print(row2.name)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "Person = Row(\"name\", \"age\")\n",
      "--------------------\n",
      "p1=Person(\"James\", 40)\n",
      "--------------------\n",
      "p2=Person(\"Alice\", 35)\n",
      "--------------------\n",
      "print(p1.name +\",\"+p2.name)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#PySpark Example\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "rdd2 = spark.sparkContext.parallelize([],10)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
      "--------------------\n",
      "    Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
      "--------------------\n",
      "    Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#RDD Example 1\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "collData=rdd.collect()\n",
      "--------------------\n",
      "print(collData)\n",
      "--------------------\n",
      "for row in collData:\n",
      "--------------------\n",
      "    print(row.name + \",\" +str(row.lang))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# RDD Example 2\n",
      "--------------------\n",
      "Person=Row(\"name\",\"lang\",\"state\")\n",
      "--------------------\n",
      "data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
      "--------------------\n",
      "    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
      "--------------------\n",
      "    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
      "--------------------\n",
      "rdd=spark.sparkContext.parallelize(data)\n",
      "--------------------\n",
      "collData=rdd.collect()\n",
      "--------------------\n",
      "print(collData)\n",
      "--------------------\n",
      "for person in collData:\n",
      "--------------------\n",
      "    print(person.name + \",\" +str(person.lang))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#DataFrame Example 1\n",
      "--------------------\n",
      "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "collData=df.collect()\n",
      "--------------------\n",
      "print(collData)\n",
      "--------------------\n",
      "for row in collData:\n",
      "--------------------\n",
      "    print(row.name + \",\" +str(row.lang))\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "#DataFrame Example 2\n",
      "--------------------\n",
      "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
      "--------------------\n",
      "(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
      "--------------------\n",
      "(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
      "--------------------\n",
      "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data).toDF(*columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "for row in df.collect():\n",
      "--------------------\n",
      "    print(row.name)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "    .master(\"local[1]\") \\\n",
      "--------------------\n",
      "    .appName(\"SparkByExamples.com\") \\\n",
      "--------------------\n",
      "    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.range(100)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "'''sample() '''\n",
      "--------------------\n",
      "print(df.sample(0.06).collect())\n",
      "--------------------\n",
      "print(df.sample(0.1,123).collect())\n",
      "--------------------\n",
      "print(df.sample(0.1,123).collect())\n",
      "--------------------\n",
      "print(df.sample(0.1,456).collect())\n",
      "--------------------\n",
      "print(\"withReplacement Examples\")\n",
      "--------------------\n",
      "print(df.sample(True,0.3,123).collect())\n",
      "--------------------\n",
      "print(df.sample(0.3,123).collect())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "'''sampleBy() '''\n",
      "--------------------\n",
      "print(\"sampleBy Examples\")\n",
      "--------------------\n",
      "df2=df.select((df.id % 3).alias(\"key\"))\n",
      "--------------------\n",
      "print(df2.sampleBy(\"key\", {0: 0.1, 1: 0.2},0).collect())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"RDD Examples\")\n",
      "--------------------\n",
      "'''RDD'''\n",
      "--------------------\n",
      "rdd = spark.sparkContext.range(0,100)\n",
      "--------------------\n",
      "print(rdd.sample(False,0.1,0).collect())\n",
      "--------------------\n",
      "print(rdd.sample(True,0.3,123).collect())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "''' RDD takeSample() '''\n",
      "--------------------\n",
      "print(rdd.takeSample(False,10,0))\n",
      "--------------------\n",
      "print(rdd.takeSample(True,30,123))\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sat Jun 13 21:08:30 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: NNK\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
      "--------------------\n",
      "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
      "--------------------\n",
      "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
      "--------------------\n",
      "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(\"firstname\",\"lastname\").show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Using Dataframe object name\n",
      "--------------------\n",
      "df.select(df.firstname,df.lastname).show()\n",
      "--------------------\n",
      "df.select(df[\"firstname\"],df[\"lastname\"]).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using col function\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "df.select(col(\"firstname\").alias(\"fname\"),col(\"lastname\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Show all columns\n",
      "--------------------\n",
      "df.select(\"*\").show()\n",
      "--------------------\n",
      "df.select([col for col in df.columns]).show()\n",
      "--------------------\n",
      "df.select(*columns).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(df.columns[:3]).show(3)\n",
      "--------------------\n",
      "df.select(df.columns[2:4]).show(3)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(df.colRegex(\"`^.*name*`\")).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [\n",
      "--------------------\n",
      "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
      "--------------------\n",
      "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
      "--------------------\n",
      "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
      "--------------------\n",
      "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
      "--------------------\n",
      "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
      "--------------------\n",
      "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
      "--------------------\n",
      "        ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType        \n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "    StructField('name', StructType([\n",
      "--------------------\n",
      "         StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "         StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "         StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "         ])),\n",
      "--------------------\n",
      "     StructField('state', StringType(), True),\n",
      "--------------------\n",
      "     StructField('gender', StringType(), True)\n",
      "--------------------\n",
      "     ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = spark.createDataFrame(data = data, schema = schema)\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show(truncate=False) # shows all columns\n",
      "--------------------\n",
      "df2.select(\"name\").show(truncate=False)\n",
      "--------------------\n",
      "df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)\n",
      "--------------------\n",
      "df2.select(\"name.*\").show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "    .master(\"local[1]\") \\\n",
      "--------------------\n",
      "    .appName(\"SparkByExamples.com\") \\\n",
      "--------------------\n",
      "    .getOrCreate()\n",
      "--------------------\n",
      "data = [('Scott', 50), ('Jeff', 45), ('Thomas', 54),('Ann',34)] \n",
      "--------------------\n",
      "sparkDF=spark.createDataFrame(data,[\"name\",\"age\"]) \n",
      "--------------------\n",
      "sparkDF.printSchema()\n",
      "--------------------\n",
      "sparkDF.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print((sparkDF.count(), len(sparkDF.columns)))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "def sparkShape(dataFrame):\n",
      "--------------------\n",
      "    return (dataFrame.count(), len(dataFrame.columns))\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "pyspark.sql.dataframe.DataFrame.shape = sparkShape\n",
      "--------------------\n",
      "print(sparkDF.shape())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pandas as pd    \n",
      "--------------------\n",
      "pandasDF=sparkDF.toPandas()\n",
      "--------------------\n",
      "print(pandasDF.shape)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\",34),(\"Ann\",34),\n",
      "--------------------\n",
      "    (\"Michael\",33),(\"Scott\",53),\n",
      "--------------------\n",
      "    (\"Robert\",37),(\"Chad\",27)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"age\",]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "#Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "--------------------\n",
      "# Internally calls limit and collect\n",
      "--------------------\n",
      "#Action, Return Array[T]\n",
      "--------------------\n",
      "print(df.take(2))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "--------------------\n",
      "#Running tail requires moving data into the application's driver process, and doing so with\n",
      "--------------------\n",
      "#a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      "--------------------\n",
      "#Return Array[T]\n",
      "--------------------\n",
      "print(df.tail(2))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\"Returns the first ``n`` rows.\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      ".. note:: This method should only be used if the resulting array is expected\n",
      "--------------------\n",
      "    to be small, as all the data is loaded into the driver's memory.\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      ":param n: int, default 1. Number of rows to return.\n",
      "--------------------\n",
      ":return: If n is greater than 1, return a list of :class:`Row`.\n",
      "--------------------\n",
      "    If n is 1, return a single Row.\"\"\"\n",
      "--------------------\n",
      "#Return Array[T]\n",
      "--------------------\n",
      "print(df.head(2))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Returns the first row, same as df.head(1)\n",
      "--------------------\n",
      "print(df.first())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Returns all the records as a list of :class:`Row`.\n",
      "--------------------\n",
      "#Action, Return Array[T]\n",
      "--------------------\n",
      "print(df.collect())\n",
      "--------------------\n",
      "#\"Limits the result count to the number specified.\n",
      "--------------------\n",
      "#Returns a new Dataset by taking the first n rows.\n",
      "--------------------\n",
      "pandasDF=df.limit(3).toPandas()\n",
      "--------------------\n",
      "print(pandasDF)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.master(\"local[1]\") \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"First SparkContext:\");\n",
      "--------------------\n",
      "print(\"APP Name :\"+spark.sparkContext.appName);\n",
      "--------------------\n",
      "print(\"Master :\"+spark.sparkContext.master);\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "sparkSession2 = SparkSession.builder \\\n",
      "--------------------\n",
      "      .master(\"local[1]\") \\\n",
      "--------------------\n",
      "      .appName(\"SparkByExample-test\") \\\n",
      "--------------------\n",
      "      .getOrCreate();\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"Second SparkContext:\")\n",
      "--------------------\n",
      "print(\"APP Name :\"+sparkSession2.sparkContext.appName);\n",
      "--------------------\n",
      "print(\"Master :\"+sparkSession2.sparkContext.master);\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "sparkSession3 = SparkSession.newSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(\"Second SparkContext:\")\n",
      "--------------------\n",
      "print(\"APP Name :\"+sparkSession3.sparkContext.appName);\n",
      "--------------------\n",
      "print(\"Master :\"+sparkSession3.sparkContext.master);\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import split, col\n",
      "--------------------\n",
      "spark=SparkSession.builder.appName(\"sparkbyexamples\").getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data=data = [('James','','Smith','1991-04-01'),\n",
      "--------------------\n",
      "  ('Michael','Rose','','2000-05-19'),\n",
      "--------------------\n",
      "  ('Robert','','Williams','1978-09-05'),\n",
      "--------------------\n",
      "  ('Maria','Anne','Jones','1967-12-01'),\n",
      "--------------------\n",
      "  ('Jen','Mary','Brown','1980-02-17')\n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\"]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data,columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "df1 = df.withColumn('year', split(df['dob'], '-').getItem(0)) \\\n",
      "--------------------\n",
      "       .withColumn('month', split(df['dob'], '-').getItem(1)) \\\n",
      "--------------------\n",
      "       .withColumn('day', split(df['dob'], '-').getItem(2))\n",
      "--------------------\n",
      "df1.printSchema()\n",
      "--------------------\n",
      "df1.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      " # Alternatively we can do like below      \n",
      "--------------------\n",
      "split_col = pyspark.sql.functions.split(df['dob'], '-')\n",
      "--------------------\n",
      "df2 = df.withColumn('year', split_col.getItem(0)) \\\n",
      "--------------------\n",
      "       .withColumn('month', split_col.getItem(1)) \\\n",
      "--------------------\n",
      "       .withColumn('day', split_col.getItem(2))\n",
      "--------------------\n",
      "df2.show(truncate=False)      \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using split() function of Column class\n",
      "--------------------\n",
      "split_col = pyspark.sql.functions.split(df['dob'], '-')\n",
      "--------------------\n",
      "df3 = df.select(\"firstname\",\"middlename\",\"lastname\",\"dob\", split_col.getItem(0).alias('year'),split_col.getItem(1).alias('month'),split_col.getItem(2).alias('day'))   \n",
      "--------------------\n",
      "df3.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "df4=spark.createDataFrame([(\"20-13-2012-monday\",)], ['date',])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df4.select(split(df4.date,'^([\\d]+-[\\d]+-[\\d])').alias('date'),\n",
      "--------------------\n",
      "    regexp_replace(split(df4.date,'^([\\d]+-[\\d]+-[\\d]+)').getItem(1),'-','').alias('day')).show()\n",
      "--------------------\n",
      "    \"\"\"\n",
      "--------------------\n",
      "df4 = spark.createDataFrame([('oneAtwoBthree',)], ['str',])\n",
      "--------------------\n",
      "df4.select(split(df4.str, '[AB]').alias('str')).show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df4.select(split(df4.str, '[AB]',2).alias('str')).show()\n",
      "--------------------\n",
      "df4.select(split(df4.str, '[AB]',1).alias('str')).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "data = [(\"James\",\"M\",60000), (\"Michael\",\"M\",70000),\n",
      "--------------------\n",
      "        (\"Robert\",None,400000), (\"Maria\",\"F\",500000),\n",
      "--------------------\n",
      "        (\"Jen\",\"\",None)]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"name\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Using When otherwise\n",
      "--------------------\n",
      "from pyspark.sql.functions import when,col\n",
      "--------------------\n",
      "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
      "--------------------\n",
      "                                 .when(df.gender == \"F\",\"Female\")\n",
      "--------------------\n",
      "                                 .when(df.gender.isNull() ,\"\")\n",
      "--------------------\n",
      "                                 .otherwise(df.gender))\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
      "--------------------\n",
      "                                 .when(df.gender == \"F\",\"Female\")\n",
      "--------------------\n",
      "                                 .when(df.gender.isNull() ,\"\")\n",
      "--------------------\n",
      "                                 .otherwise(df.gender))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2=df.select(col(\"*\"),when(df.gender == \"M\",\"Male\")\n",
      "--------------------\n",
      "                  .when(df.gender == \"F\",\"Female\")\n",
      "--------------------\n",
      "                  .when(df.gender.isNull() ,\"\")\n",
      "--------------------\n",
      "                  .otherwise(df.gender).alias(\"new_gender\"))\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "# Using SQL Case When\n",
      "--------------------\n",
      "from pyspark.sql.functions import expr\n",
      "--------------------\n",
      "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
      "--------------------\n",
      "           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
      "--------------------\n",
      "          \"ELSE gender END\"))\n",
      "--------------------\n",
      "df3.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df4 = df.select(col(\"*\"), expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
      "--------------------\n",
      "           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
      "--------------------\n",
      "           \"ELSE gender END\").alias(\"new_gender\"))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"EMP\")\n",
      "--------------------\n",
      "spark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" + \n",
      "--------------------\n",
      "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
      "--------------------\n",
      "              \"ELSE gender END as new_gender from EMP\").show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "               .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "               .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.createDataFrame([[\"02-03-2013\"],[\"05-06-2023\"]],[\"input\"])\n",
      "--------------------\n",
      "df.select(col(\"input\"),to_date(col(\"input\"),\"MM-dd-yyyy\").alias(\"date\")) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#SQL\n",
      "--------------------\n",
      "spark.sql(\"select to_date('02-03-2013','MM-dd-yyyy') date\").show()\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.createDataFrame(\n",
      "--------------------\n",
      "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
      "--------------------\n",
      "        schema=[\"id\",\"input_timestamp\"])\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Timestamp String to DateType\n",
      "--------------------\n",
      "df.withColumn(\"timestamp\",to_timestamp(\"input_timestamp\")) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "# Using Cast to convert TimestampType to DateType\n",
      "--------------------\n",
      "df.withColumn('timestamp', \\\n",
      "--------------------\n",
      "         to_timestamp('input_timestamp').cast('string')) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(to_timestamp(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#SQL string to TimestampType\n",
      "--------------------\n",
      "spark.sql(\"select to_timestamp('2019-06-24 12:01:19.000') as timestamp\")\n",
      "--------------------\n",
      "#SQL CAST timestamp string to TimestampType\n",
      "--------------------\n",
      "spark.sql(\"select timestamp('2019-06-24 12:01:19.000') as timestamp\")\n",
      "--------------------\n",
      "#SQL Custom string to TimestampType\n",
      "--------------------\n",
      "spark.sql(\"select to_timestamp('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as timestamp\")\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "         .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "         .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
      "--------------------\n",
      "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
      "--------------------\n",
      "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
      "--------------------\n",
      "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
      "--------------------\n",
      "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
      "--------------------\n",
      "            ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df=spark.createDataFrame(data,columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import split, col\n",
      "--------------------\n",
      "df2 = df.select(split(col(\"name\"),\",\").alias(\"NameArray\")) \\\n",
      "--------------------\n",
      "    .drop(\"name\")\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"PERSON\")\n",
      "--------------------\n",
      "spark.sql(\"select SPLIT(name,',') as NameArray from PERSON\") \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "data = [ (\"36636\",\"Finance\",(3000,\"USA\")), \n",
      "--------------------\n",
      "    (\"40288\",\"Finance\",(5000,\"IND\")), \n",
      "--------------------\n",
      "    (\"42114\",\"Sales\",(3900,\"USA\")), \n",
      "--------------------\n",
      "    (\"39192\",\"Marketing\",(2500,\"CAN\")), \n",
      "--------------------\n",
      "    (\"34534\",\"Sales\",(6500,\"USA\")) ]\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "     StructField('id', StringType(), True),\n",
      "--------------------\n",
      "     StructField('dept', StringType(), True),\n",
      "--------------------\n",
      "     StructField('properties', StructType([\n",
      "--------------------\n",
      "         StructField('salary', IntegerType(), True),\n",
      "--------------------\n",
      "         StructField('location', StringType(), True)\n",
      "--------------------\n",
      "         ]))\n",
      "--------------------\n",
      "     ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Convert struct type to Map\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,lit,create_map\n",
      "--------------------\n",
      "df = df.withColumn(\"propertiesMap\",create_map(\n",
      "--------------------\n",
      "        lit(\"salary\"),col(\"properties.salary\"),\n",
      "--------------------\n",
      "        lit(\"location\"),col(\"properties.location\")\n",
      "--------------------\n",
      "        )).drop(\"properties\")\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,struct,when\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.master(\"local[1]\") \\\n",
      "--------------------\n",
      "                    .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "                    .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
      "--------------------\n",
      "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
      "--------------------\n",
      "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
      "--------------------\n",
      "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
      "--------------------\n",
      "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([ \n",
      "--------------------\n",
      "    StructField(\"firstname\",StringType(),True), \n",
      "--------------------\n",
      "    StructField(\"middlename\",StringType(),True), \n",
      "--------------------\n",
      "    StructField(\"lastname\",StringType(),True), \n",
      "--------------------\n",
      "    StructField(\"id\", StringType(), True), \n",
      "--------------------\n",
      "    StructField(\"gender\", StringType(), True), \n",
      "--------------------\n",
      "    StructField(\"salary\", IntegerType(), True) \n",
      "--------------------\n",
      "  ])\n",
      "--------------------\n",
      " \n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "structureData = [\n",
      "--------------------\n",
      "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
      "--------------------\n",
      "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
      "--------------------\n",
      "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
      "--------------------\n",
      "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
      "--------------------\n",
      "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "structureSchema = StructType([\n",
      "--------------------\n",
      "        StructField('name', StructType([\n",
      "--------------------\n",
      "             StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "             StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "             StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "             ])),\n",
      "--------------------\n",
      "         StructField('id', StringType(), True),\n",
      "--------------------\n",
      "         StructField('gender', StringType(), True),\n",
      "--------------------\n",
      "         StructField('salary', IntegerType(), True)\n",
      "--------------------\n",
      "         ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "updatedDF = df2.withColumn(\"OtherInfo\", \n",
      "--------------------\n",
      "    struct(col(\"id\").alias(\"identifier\"),\n",
      "--------------------\n",
      "    col(\"gender\").alias(\"gender\"),\n",
      "--------------------\n",
      "    col(\"salary\").alias(\"salary\"),\n",
      "--------------------\n",
      "    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
      "--------------------\n",
      "      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
      "--------------------\n",
      "      .otherwise(\"High\").alias(\"Salary_Grade\")\n",
      "--------------------\n",
      "  )).drop(\"id\",\"gender\",\"salary\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "updatedDF.printSchema()\n",
      "--------------------\n",
      "updatedDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\" Array & Map\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "arrayStructureSchema = StructType([\n",
      "--------------------\n",
      "    StructField('name', StructType([\n",
      "--------------------\n",
      "       StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "       StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "       StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "       ])),\n",
      "--------------------\n",
      "       StructField('hobbies', ArrayType(StringType()), True),\n",
      "--------------------\n",
      "       StructField('properties', MapType(StringType(),StringType()), True)\n",
      "--------------------\n",
      "    ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dates = [(\"1\",\"2019-07-01 12:01:19.111\"),\n",
      "--------------------\n",
      "    (\"2\",\"2019-06-24 12:01:19.222\"),\n",
      "--------------------\n",
      "    (\"3\",\"2019-11-16 16:44:55.406\"),\n",
      "--------------------\n",
      "    (\"4\",\"2019-11-16 16:50:59.406\")\n",
      "--------------------\n",
      "    ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=dates, schema=[\"id\",\"from_timestamp\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "df2=df.withColumn('from_timestamp',to_timestamp(col('from_timestamp')))\\\n",
      "--------------------\n",
      "  .withColumn('end_timestamp', current_timestamp())\\\n",
      "--------------------\n",
      "  .withColumn('DiffInSeconds',col(\"end_timestamp\").cast(\"long\") - col('from_timestamp').cast(\"long\"))\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn('from_timestamp',to_timestamp(col('from_timestamp')))\\\n",
      "--------------------\n",
      "  .withColumn('end_timestamp', current_timestamp())\\\n",
      "--------------------\n",
      "  .withColumn('DiffInSeconds',unix_timestamp(\"end_timestamp\") - unix_timestamp('from_timestamp')) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2.withColumn('DiffInMinutes',round(col('DiffInSeconds')/60))\\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "df2.withColumn('DiffInHours',round(col('DiffInSeconds')/3600))\\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "#Difference between two timestamps when input has just timestamp\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data= [(\"12:01:19.000\",\"13:01:19.000\"),\n",
      "--------------------\n",
      "    (\"12:01:19.000\",\"12:02:19.000\"),\n",
      "--------------------\n",
      "    (\"16:44:55.406\",\"17:44:55.406\"),\n",
      "--------------------\n",
      "    (\"16:50:59.406\",\"16:44:59.406\")]\n",
      "--------------------\n",
      "df3 = spark.createDataFrame(data=data, schema=[\"from_timestamp\",\"to_timestamp\"])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3.withColumn(\"from_timestamp\",to_timestamp(col(\"from_timestamp\"),\"HH:mm:ss.SSS\")) \\\n",
      "--------------------\n",
      "   .withColumn(\"to_timestamp\",to_timestamp(col(\"to_timestamp\"),\"HH:mm:ss.SSS\")) \\\n",
      "--------------------\n",
      "   .withColumn(\"DiffInSeconds\", col(\"from_timestamp\").cast(\"long\") - col(\"to_timestamp\").cast(\"long\")) \\\n",
      "--------------------\n",
      "   .withColumn(\"DiffInMinutes\",round(col(\"DiffInSeconds\")/60)) \\\n",
      "--------------------\n",
      "   .withColumn(\"DiffInHours\",round(col(\"DiffInSeconds\")/3600)) \\\n",
      "--------------------\n",
      "   .show(truncate=False)\n",
      "--------------------\n",
      "   \n",
      "--------------------\n",
      "#\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3 = spark.createDataFrame(\n",
      "--------------------\n",
      "        data=[(\"1\",\"07-01-2019 12:01:19.406\")], \n",
      "--------------------\n",
      "        schema=[\"id\",\"input_timestamp\"]\n",
      "--------------------\n",
      "        )\n",
      "--------------------\n",
      "df3.withColumn(\"input_timestamp\",to_timestamp(col(\"input_timestamp\"),\"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n",
      "--------------------\n",
      "    .withColumn(\"current_timestamp\",current_timestamp().alias(\"current_timestamp\")) \\\n",
      "--------------------\n",
      "    .withColumn(\"DiffInSeconds\",current_timestamp().cast(\"long\") - col(\"input_timestamp\").cast(\"long\")) \\\n",
      "--------------------\n",
      "    .withColumn(\"DiffInMinutes\",round(col(\"DiffInSeconds\")/60)) \\\n",
      "--------------------\n",
      "    .withColumn(\"DiffInHours\",round(col(\"DiffInSeconds\")/3600)) \\\n",
      "--------------------\n",
      "    .withColumn(\"DiffInDays\",round(col(\"DiffInSeconds\")/24*3600)) \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "#SQL\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.sql(\"select unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19') DiffInSeconds\").show()\n",
      "--------------------\n",
      "spark.sql(\"select (unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19'))/60 DiffInMinutes\").show()\n",
      "--------------------\n",
      "spark.sql(\"select (unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19'))/3600 DiffInHours\").show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.createDataFrame(\n",
      "--------------------\n",
      "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
      "--------------------\n",
      "        schema=[\"id\",\"input_timestamp\"])\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using Cast to convert Timestamp String to DateType\n",
      "--------------------\n",
      "df.withColumn('date_type', col('input_timestamp').cast('date')) \\\n",
      "--------------------\n",
      "       .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using Cast to convert TimestampType to DateType\n",
      "--------------------\n",
      "df.withColumn('date_type', to_timestamp('input_timestamp').cast('date')) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(to_date(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "#Timestamp String to DateType\n",
      "--------------------\n",
      "df.withColumn(\"date_type\",to_date(\"input_timestamp\")) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Timestamp Type to DateType\n",
      "--------------------\n",
      "df.withColumn(\"date_type\",to_date(current_timestamp())) \\\n",
      "--------------------\n",
      "  .show(truncate=False) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"ts\",to_timestamp(col(\"input_timestamp\"))) \\\n",
      "--------------------\n",
      "  .withColumn(\"datetype\",to_date(col(\"ts\"))) \\\n",
      "--------------------\n",
      "  .show(truncate=False)\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "#SQL TimestampType to DateType\n",
      "--------------------\n",
      "spark.sql(\"select to_date(current_timestamp) as date_type\")\n",
      "--------------------\n",
      "#SQL CAST TimestampType to DateType\n",
      "--------------------\n",
      "spark.sql(\"select date(to_timestamp('2019-06-24 12:01:19.000')) as date_type\")\n",
      "--------------------\n",
      "#SQL CAST timestamp string to DateType\n",
      "--------------------\n",
      "spark.sql(\"select date('2019-06-24 12:01:19.000') as date_type\")\n",
      "--------------------\n",
      "#SQL Timestamp String (default format) to DateType\n",
      "--------------------\n",
      "spark.sql(\"select to_date('2019-06-24 12:01:19.000') as date_type\")\n",
      "--------------------\n",
      "#SQL Custom Timeformat to DateType\n",
      "--------------------\n",
      "spark.sql(\"select to_date('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as date_type\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "     \n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.types import DataType\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.types import ArrayType,IntegerType\n",
      "--------------------\n",
      "arrayType = ArrayType(IntegerType(),False)\n",
      "--------------------\n",
      "print(arrayType.jsonValue()) \n",
      "--------------------\n",
      "print(arrayType.simpleString())\n",
      "--------------------\n",
      "print(arrayType.typeName()) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.types import MapType,StringType,IntegerType\n",
      "--------------------\n",
      "mapType = MapType(StringType(),IntegerType())\n",
      "--------------------\n",
      " \n",
      "--------------------\n",
      "print(mapType.keyType)\n",
      "--------------------\n",
      "print(mapType.valueType)\n",
      "--------------------\n",
      "print(mapType.valueContainsNull)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\",\"\",\"Smith\",\"36\",\"M\",3000),\n",
      "--------------------\n",
      "    (\"Michael\",\"Rose\",\"\",\"40\",\"M\",4000),\n",
      "--------------------\n",
      "    (\"Robert\",\"\",\"Williams\",\"42\",\"M\",4000),\n",
      "--------------------\n",
      "    (\"Maria\",\"Anne\",\"Jones\",\"39\",\"F\",4000),\n",
      "--------------------\n",
      "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([ \n",
      "--------------------\n",
      "    StructField(\"firstname\",StringType(),True), \n",
      "--------------------\n",
      "    StructField(\"middlename\",StringType(),True), \n",
      "--------------------\n",
      "    StructField(\"lastname\",StringType(),True), \n",
      "--------------------\n",
      "    StructField(\"age\", StringType(), True), \n",
      "--------------------\n",
      "    StructField(\"gender\", StringType(), True), \n",
      "--------------------\n",
      "    StructField(\"salary\", IntegerType(), True) \n",
      "--------------------\n",
      "  ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=schema)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col, udf\n",
      "--------------------\n",
      "from pyspark.sql.types import StringType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"Seqno\",\"Name\"]\n",
      "--------------------\n",
      "data = [(\"1\", \"john jones\"),\n",
      "--------------------\n",
      "    (\"2\", \"tracey smith\"),\n",
      "--------------------\n",
      "    (\"3\", \"amy sanders\")]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data,schema=columns)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "def convertCase(str):\n",
      "--------------------\n",
      "    resStr=\"\"\n",
      "--------------------\n",
      "    arr = str.split(\" \")\n",
      "--------------------\n",
      "    for x in arr:\n",
      "--------------------\n",
      "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
      "--------------------\n",
      "    return resStr \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\" Converting function to UDF \"\"\"\n",
      "--------------------\n",
      "convertUDF = udf(lambda z: convertCase(z))\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.select(col(\"Seqno\"), \\\n",
      "--------------------\n",
      "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
      "--------------------\n",
      ".show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@udf(returnType=StringType()) \n",
      "--------------------\n",
      "def upperCase(str):\n",
      "--------------------\n",
      "    return str.upper()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "upperCaseUDF = udf(lambda z:upperCase(z),StringType())    \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"Cureated Name\", upperCase(col(\"Name\"))) \\\n",
      "--------------------\n",
      ".show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\" Using UDF on SQL \"\"\"\n",
      "--------------------\n",
      "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
      "--------------------\n",
      "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
      "--------------------\n",
      "     .show(truncate=False)\n",
      "--------------------\n",
      "     \n",
      "--------------------\n",
      "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \\\n",
      "--------------------\n",
      "          \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n",
      "--------------------\n",
      "     .show(truncate=False)  \n",
      "--------------------\n",
      "     \n",
      "--------------------\n",
      "\"\"\" null check \"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"Seqno\",\"Name\"]\n",
      "--------------------\n",
      "data = [(\"1\", \"john jones\"),\n",
      "--------------------\n",
      "    (\"2\", \"tracey smith\"),\n",
      "--------------------\n",
      "    (\"3\", \"amy sanders\"),\n",
      "--------------------\n",
      "    ('4',None)]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = spark.createDataFrame(data=data,schema=columns)\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
      "--------------------\n",
      "     .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
      "--------------------\n",
      "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
      "--------------------\n",
      "     .show(truncate=False)  \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      " \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
      "--------------------\n",
      "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
      "--------------------\n",
      "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
      "--------------------\n",
      "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
      "--------------------\n",
      "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
      "--------------------\n",
      "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
      "--------------------\n",
      "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
      "--------------------\n",
      "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
      "--------------------\n",
      "  ]\n",
      "--------------------\n",
      "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "unionDF = df.union(df2)\n",
      "--------------------\n",
      "unionDF.show(truncate=False)\n",
      "--------------------\n",
      "disDF = df.union(df2).distinct()\n",
      "--------------------\n",
      "disDF.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "unionAllDF = df.unionAll(df2)\n",
      "--------------------\n",
      "unionAllDF.show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "inputData = [(\"2019-07-01 12:01:19\",\n",
      "--------------------\n",
      "            \"07-01-2019 12:01:19\", \n",
      "--------------------\n",
      "            \"07-01-2019\")]\n",
      "--------------------\n",
      "columns=[\"timestamp_1\",\"timestamp_2\",\"timestamp_3\"]\n",
      "--------------------\n",
      "df=spark.createDataFrame(\n",
      "--------------------\n",
      "        data = inputData,\n",
      "--------------------\n",
      "        schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import *\n",
      "--------------------\n",
      "df2 = df.select( \n",
      "--------------------\n",
      "      unix_timestamp(col(\"timestamp_1\")).alias(\"timestamp_1\"), \n",
      "--------------------\n",
      "      unix_timestamp(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"), \n",
      "--------------------\n",
      "      unix_timestamp(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"), \n",
      "--------------------\n",
      "      unix_timestamp().alias(\"timestamp_4\") \n",
      "--------------------\n",
      "   )\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3=df2.select(\n",
      "--------------------\n",
      "    from_unixtime(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n",
      "--------------------\n",
      "    from_unixtime(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n",
      "--------------------\n",
      "    from_unixtime(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"),\n",
      "--------------------\n",
      "    from_unixtime(col(\"timestamp_4\")).alias(\"timestamp_4\")\n",
      "--------------------\n",
      "  )\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "df3.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#SQL\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [('James','Smith','M',3000),('Anna','Rose','F',4100),\n",
      "--------------------\n",
      "  ('Robert','Williams','NA',6200),(None,'Rob','F',6200)\n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data, schema = columns)\n",
      "--------------------\n",
      "df.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2=df.withColumn(\"salary\", df.salary*3)\n",
      "--------------------\n",
      "df2.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import when\n",
      "--------------------\n",
      "df3 = df.withColumn(\"gender\", when(df.gender == \"M\",\"Male\") \\\n",
      "--------------------\n",
      "      .when(df.gender == \"F\",\"Female\") \\\n",
      "--------------------\n",
      "      .otherwise(df.gender))\n",
      "--------------------\n",
      "df3.show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df4=df.withColumn(\"salary\",df.salary.cast(\"String\"))\n",
      "--------------------\n",
      "df4.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.createOrReplaceTempView(\"PER\")\n",
      "--------------------\n",
      "df5=spark.sql(\"select firstname,gender,salary*3 as salary from PER\")\n",
      "--------------------\n",
      "df5.show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sat Jun 13 21:08:30 2020\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: NNK\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
      "--------------------\n",
      "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
      "--------------------\n",
      "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
      "--------------------\n",
      "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
      "--------------------\n",
      "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = data, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using when otherwise\n",
      "--------------------\n",
      "from pyspark.sql.functions import col, when\n",
      "--------------------\n",
      "df2 = df.withColumn(\"new_gender\", when(col(\"gender\") == \"M\",\"Male\")\n",
      "--------------------\n",
      "                                 .when(col(\"gender\") == \"F\",\"Female\")\n",
      "--------------------\n",
      "                                 .otherwise(\"Unknown\"))\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df22=df.select(col(\"*\"), when(col(\"gender\") == \"M\",\"Male\")\n",
      "--------------------\n",
      "      .when(col(\"gender\") == \"F\",\"Female\")\n",
      "--------------------\n",
      "      .otherwise(\"Unknown\").alias(\"new_gender\")).show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Using case when\n",
      "--------------------\n",
      "from pyspark.sql.functions import expr\n",
      "--------------------\n",
      "df3 = df.withColumn(\"new_gender\", expr(\"case when gender = 'M' then 'Male' \" + \n",
      "--------------------\n",
      "                       \"when gender = 'F' then 'Female' \" +\n",
      "--------------------\n",
      "                       \"else 'Unknown' end\"))\n",
      "--------------------\n",
      "df3.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Using case when\n",
      "--------------------\n",
      "df4 = df.select(col(\"*\"), expr(\"case when gender = 'M' then 'Male' \" +\n",
      "--------------------\n",
      "                       \"when gender = 'F' then 'Female' \" +\n",
      "--------------------\n",
      "                       \"else 'Unknown' end\").alias(\"new_gender\"))\n",
      "--------------------\n",
      "df4.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data2 = [(66, \"a\", \"4\"), (67, \"a\", \"0\"), (70, \"b\", \"4\"), (71, \"d\", \"4\")]\n",
      "--------------------\n",
      "df5 = spark.createDataFrame(data = data2, schema = [\"id\", \"code\", \"amt\"])\n",
      "--------------------\n",
      "         \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df5.withColumn(\"new_column\", when(col(\"code\") == \"a\" | col(\"code\") == \"d\", \"A\")\n",
      "--------------------\n",
      "      .when(col(\"code\") == \"b\" & col(\"amt\") == \"4\", \"B\")\n",
      "--------------------\n",
      "      .otherwise(\"A1\")).show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
      "--------------------\n",
      "    (\"Michael\", \"Sales\", 4600),  \\\n",
      "--------------------\n",
      "    (\"Robert\", \"Sales\", 4100),   \\\n",
      "--------------------\n",
      "    (\"Maria\", \"Finance\", 3000),  \\\n",
      "--------------------\n",
      "    (\"James\", \"Sales\", 3000),    \\\n",
      "--------------------\n",
      "    (\"Scott\", \"Finance\", 3300),  \\\n",
      "--------------------\n",
      "    (\"Jen\", \"Finance\", 3900),    \\\n",
      "--------------------\n",
      "    (\"Jeff\", \"Marketing\", 3000), \\\n",
      "--------------------\n",
      "    (\"Kumar\", \"Marketing\", 2000),\\\n",
      "--------------------\n",
      "    (\"Saif\", \"Sales\", 4100) \\\n",
      "--------------------\n",
      "  )\n",
      "--------------------\n",
      " \n",
      "--------------------\n",
      "columns= [\"employee_name\", \"department\", \"salary\"]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.window import Window\n",
      "--------------------\n",
      "from pyspark.sql.functions import row_number\n",
      "--------------------\n",
      "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
      "--------------------\n",
      "    .show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import rank\n",
      "--------------------\n",
      "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import dense_rank\n",
      "--------------------\n",
      "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import percent_rank\n",
      "--------------------\n",
      "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "from pyspark.sql.functions import ntile\n",
      "--------------------\n",
      "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import cume_dist    \n",
      "--------------------\n",
      "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
      "--------------------\n",
      "   .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import lag    \n",
      "--------------------\n",
      "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
      "--------------------\n",
      "      .show()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql.functions import lead    \n",
      "--------------------\n",
      "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
      "--------------------\n",
      "    .show()\n",
      "--------------------\n",
      "    \n",
      "--------------------\n",
      "windowSpecAgg  = Window.partitionBy(\"department\")\n",
      "--------------------\n",
      "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
      "--------------------\n",
      "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
      "--------------------\n",
      "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
      "--------------------\n",
      "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
      "--------------------\n",
      "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
      "--------------------\n",
      "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
      "--------------------\n",
      "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Sun Jun 14 10:20:19 2020\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col, lit\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "data = [('James','','Smith','1991-04-01','M',3000),\n",
      "--------------------\n",
      "  ('Michael','Rose','','2000-05-19','M',4000),\n",
      "--------------------\n",
      "  ('Robert','','Williams','1978-09-05','M',4000),\n",
      "--------------------\n",
      "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
      "--------------------\n",
      "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
      "--------------------\n",
      "df = spark.createDataFrame(data=data, schema = columns)\n",
      "--------------------\n",
      "df.printSchema()\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2 = df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\"))\n",
      "--------------------\n",
      "df2.printSchema()\n",
      "--------------------\n",
      "df2.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3 = df.withColumn(\"salary\",col(\"salary\")*100)\n",
      "--------------------\n",
      "df3.printSchema()\n",
      "--------------------\n",
      "df3.show(truncate=False) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df4 = df.withColumn(\"CopiedColumn\",col(\"salary\")* -1)\n",
      "--------------------\n",
      "df4.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df5 = df.withColumn(\"Country\", lit(\"USA\"))\n",
      "--------------------\n",
      "df5.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df6 = df.withColumn(\"Country\", lit(\"USA\")) \\\n",
      "--------------------\n",
      "   .withColumn(\"anotherColumn\",lit(\"anotherValue\"))\n",
      "--------------------\n",
      "df6.printSchema()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumnRenamed(\"gender\",\"sex\") \\\n",
      "--------------------\n",
      "  .show(truncate=False) \n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "df4.drop(\"CopiedColumn\") \\\n",
      "--------------------\n",
      ".show(truncate=False) \n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
      "--------------------\n",
      "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
      "--------------------\n",
      "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
      "--------------------\n",
      "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
      "--------------------\n",
      "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
      "--------------------\n",
      "]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schemaStruct = StructType([\n",
      "--------------------\n",
      "        StructField('name', StructType([\n",
      "--------------------\n",
      "             StructField('firstname', StringType(), True),\n",
      "--------------------\n",
      "             StructField('middlename', StringType(), True),\n",
      "--------------------\n",
      "             StructField('lastname', StringType(), True)\n",
      "--------------------\n",
      "             ])),\n",
      "--------------------\n",
      "          StructField('dob', StringType(), True),\n",
      "--------------------\n",
      "         StructField('gender', StringType(), True),\n",
      "--------------------\n",
      "         StructField('salary', StringType(), True)\n",
      "--------------------\n",
      "         ])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df7 = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
      "--------------------\n",
      "df7.printSchema()\n",
      "--------------------\n",
      "df7.show(truncate=False)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "columns = [\"name\",\"address\"]\n",
      "--------------------\n",
      "data = [(\"Robert, Smith\", \"1 Main st, Newark, NJ, 92537\"), \\\n",
      "--------------------\n",
      "        (\"Maria, Garcia\",\"3456 Walnut st, Newark, NJ, 94732\")]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dfFromData = spark.createDataFrame(data=data, schema = schema)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "newDF = dfFromData.map(f=>{\n",
      "--------------------\n",
      "nameSplit = f.getAs[String](0).split(\",\")\n",
      "--------------------\n",
      "addSplit = f.getAs[String](1).split(\",\")\n",
      "--------------------\n",
      "      (nameSplit(0),nameSplit(1),addSplit(0),addSplit(1),addSplit(2),addSplit(3))\n",
      "--------------------\n",
      "    })\n",
      "--------------------\n",
      "finalDF = newDF.toDF(\"First Name\",\"Last Name\",\n",
      "--------------------\n",
      "             \"Address Line1\",\"City\",\"State\",\"zipCode\")\n",
      "--------------------\n",
      "finalDF.printSchema()\n",
      "--------------------\n",
      "finalDF.show(false)\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pandas as pd    \n",
      "--------------------\n",
      "data = [[\"James\",\"\",\"Smith\",30,\"M\",60000], \n",
      "--------------------\n",
      "        [\"Michael\",\"Rose\",\"\",50,\"M\",70000], \n",
      "--------------------\n",
      "        [\"Robert\",\"\",\"Williams\",42,\"\",400000], \n",
      "--------------------\n",
      "        [\"Maria\",\"Anne\",\"Jones\",38,\"F\",500000], \n",
      "--------------------\n",
      "        [\"Jen\",\"Mary\",\"Brown\",45,None,0]] \n",
      "--------------------\n",
      "columns = ['First Name', 'Middle Name','Last Name','Age','Gender','Salary']\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create the pandas DataFrame \n",
      "--------------------\n",
      "pandasDF = pd.DataFrame(data=data, columns=columns) \n",
      "--------------------\n",
      "  \n",
      "--------------------\n",
      "# print dataframe. \n",
      "--------------------\n",
      "print(pandasDF)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "#Outputs below data on console\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "pdCount=pandasDF.count()\n",
      "--------------------\n",
      "print(pdCount)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "print(pandasDF.max())\n",
      "--------------------\n",
      "print(pandasDF.mean())\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Thu Oct 24 22:42:50 2019\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: prabha\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "from pyspark.sql.functions import to_timestamp\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,DateType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "            StructField(\"city\", StringType(), True),\n",
      "--------------------\n",
      "            StructField(\"dates\", StringType(), True),\n",
      "--------------------\n",
      "            StructField(\"population\", IntegerType(), True)])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dates = [\"1991-02-25\",\"1998-05-10\", \"1993/03/15\", \"1992/07/17\"]\n",
      "--------------------\n",
      "cities = ['Caracas', 'Ccs', '   São Paulo   ', '~Madrid']\n",
      "--------------------\n",
      "population = [37800000, 19795791, 12341418, 6489162]\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "        # Dataframe:\n",
      "--------------------\n",
      "df = spark.createDataFrame(list(zip(cities, dates, population)), schema=schema)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.show(truncate=False)\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "author SparkByExamples.com\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "# Create SparkSession\n",
      "--------------------\n",
      "spark = SparkSession.builder \\\n",
      "--------------------\n",
      "          .appName('SparkByExamples.com') \\\n",
      "--------------------\n",
      "          .getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df=spark.read.option(\"header\",True) \\\n",
      "--------------------\n",
      "        .csv(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/simple-zipcodes.csv\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "newDF=df.repartition(3)\n",
      "--------------------\n",
      "print(newDF.rdd.getNumPartitions())\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "newDF.write.option(\"header\",True).mode(\"overwrite\") \\\n",
      "--------------------\n",
      "        .csv(\"/tmp/zipcodes-state\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df2=df.repartition(4,\"state\")\n",
      "--------------------\n",
      "df2.write.option(\"header\",True).mode(\"overwrite\") \\\n",
      "--------------------\n",
      "   .csv(\"/tmp/zipcodes-state-3states\")\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df3=df.repartition(\"state\")\n",
      "--------------------\n",
      "df3.write.option(\"header\",True).mode(\"overwrite\") \\\n",
      "--------------------\n",
      "   .csv(\"/tmp/zipcodes-state-allstates\")\n",
      "--------------------\n",
      "# -*- coding: utf-8 -*-\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "Created on Thu Oct 24 22:42:50 2019\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "@author: prabha\n",
      "--------------------\n",
      "\"\"\"\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "import pyspark\n",
      "--------------------\n",
      "from pyspark.sql import SparkSession\n",
      "--------------------\n",
      "from pyspark.sql.functions import col\n",
      "--------------------\n",
      "from pyspark.sql.functions import to_timestamp, current_timestamp\n",
      "--------------------\n",
      "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "schema = StructType([\n",
      "--------------------\n",
      "            StructField(\"input_timestamp\", StringType(), True)])\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "dates = ['2019-07-01 12:01:19.111',\n",
      "--------------------\n",
      "    '2019-06-24 12:01:19.222',\n",
      "--------------------\n",
      "    '2019-11-16 16:44:55.406',\n",
      "--------------------\n",
      "    '2019-11-16 16:50:59.406']\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df = spark.createDataFrame(list( zip(dates)), schema=schema)\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "df.withColumn('input_timestamp',to_timestamp(col('input_timestamp')))\\\n",
      "--------------------\n",
      "  .withColumn('current_timestamp', current_timestamp().alias('current_timestamp'))\\\n",
      "--------------------\n",
      "  .withColumn('DiffInSeconds',current_timestamp().cast(LongType) - col('input_timestamp').cast(LongType))\\\n",
      "--------------------\n",
      "  .withColumn('DiffInMinutes',round(col('DiffInSeconds')/60))\\\n",
      "--------------------\n",
      "  .withColumn('DiffInHours',round(col('DiffInSeconds')/3600))\\\n",
      "--------------------\n",
      "  .withColumn('DiffInDays',round(col('DiffInSeconds')/24*3600))\\\n",
      "--------------------\n",
      "  .show()\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for item in input_file.collect():\n",
    "    print(item)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fa72cd0-779d-47a7-99d5-8969fc7bb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read text file into df\n",
    "df=spark.read.text(\"../data/cover_letter.txt\")\n",
    "\n",
    "\n",
    "# difference bw creating a dataframe vs rdd from a file\n",
    "# dataframe <-- spark.read(\"location\")\n",
    "# rdd <-- sc.textFile(\"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "652e7491-be17-4f38-81de-169312ed9c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Presently a Data Engineer with more than 2 yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               value\n",
       "0  Presently a Data Engineer with more than 2 yea...\n",
       "1                                                   "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9321a-1645-4ba9-a0bd-6709e70b8793",
   "metadata": {},
   "source": [
    "## Spark RDD Transformations with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b99ebec3-17a1-4f8a-ac26-dc5f79e6c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.textFile(\"../data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "951f6bf3-d231-402c-959f-c63222fb4df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Project Gutenberg’s'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdf49c-022d-4037-beb9-b6b03e9a2bb6",
   "metadata": {},
   "source": [
    "## flatMap(), map(), filter(), reducebykey Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=rdd.map(lambda x:x.split(\" \"))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1ead6a2-7fbb-4c0e-922b-09e0c852c16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1=rdd.flatMap(lambda x:x.split(\" \"))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94fad24a-c21e-4dc5-a65e-7569cfae78c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'and']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=rdd1.filter(lambda x:x.startswith(\"a\"))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a7ea29d-b16a-4154-9fe8-e4bdd52ab6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('and', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=rdd2.map(lambda x:(x,1))\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('anyone', 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fbb47cf-d434-4c6e-a206-5f2b23e41fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anyone', 27), ('anywhere', 27), ('at', 27), ('and', 27)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now perform reduceBykey\n",
    "rdd4=rdd3.reduceByKey(lambda x,y:x+y)\n",
    "rdd4.collect()\n",
    "\n",
    "# how reduceByKey works:\n",
    "\n",
    "# It groups the data by keys, so all the values associated with a particular key are grouped together.\n",
    "# For each group of values with the same key, it applies the provided lambda function to reduce (aggregate) them to a single value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c9cf4be-e07d-473a-938d-0ba49e5f800e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 27), ('anyone', 27), ('anywhere', 27), ('at', 27)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e1d3d4-c83d-45fa-aaf8-ae7c9ebb91c1",
   "metadata": {},
   "source": [
    "## Spark RDD Actions with examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0d0a8de-a1d9-45f6-bfe8-820ce3d0fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputRDD = sc.parallelize([(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)],3)\n",
    "listRdd = sc.parallelize([1,2,3,4,5,3,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8606b8ef-6f0f-477c-a3b7-69e0fe69076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 120), ('C', 40), ('A', 20), ('Z', 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reduce\n",
    "inputRDD.reduceByKey(lambda x,y:x+y).collect()\n",
    "\n",
    "# alternative\n",
    "# from operator import add\n",
    "# inputRDD.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12ac94d0-5ab5-45ea-850d-f06ea87291ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRdd.reduce(lambda x,y:x+y)\n",
    "# doesn't return a rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "056f8f2c-28c1-4626-929e-1c680f61460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add = lambda x, y: x + y\n",
    "rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)  # last 10 is the number of partitions\n",
    "rdd.treeAggregate(0, add, add)\n",
    "\n",
    "#TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2761a6f7-fc29-41d8-beec-52f0b1492fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(1, 2), (2, 3)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da54b82f-7439-43f9-9805-9abd0cc8c694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3247e-8b51-47c3-b254-3db9dfe41a12",
   "metadata": {},
   "source": [
    "## Spark Repartition() vs Coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f865f4b-98b3-4d4f-a37a-2fc8f88474b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition :  6\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(0,25), 6)\n",
    "print(\"Repartition : \",rdd1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9033fcf5-b92a-43f0-9b76-b5cb23adfcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition :  4\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd1.repartition(4)\n",
    "print(\"Repartition : \",rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89c037df-73e4-4c86-b7e9-24929bd2c4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coalesce :  3\n"
     ]
    }
   ],
   "source": [
    "rdd3=rdd2.coalesce(3)\n",
    "print(\"coalesce : \",rdd3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaef99b-f1e5-4353-9ca0-c1e0fb325900",
   "metadata": {},
   "source": [
    "## spark suffling and partitions\n",
    "https://sparkbyexamples.com/spark/spark-shuffle-partitions/ <br>\n",
    "\n",
    "Based on your dataset size, number of cores, and memory, Spark shuffling can benefit or harm your jobs. When you dealing with less amount of data, you should typically reduce the shuffle partitions otherwise you will end up with many partitioned files with a fewer number of records in each partition. which results in running many tasks with lesser data to process.\n",
    "\n",
    "On other hand, when you have too much of data and having less number of partitions results in fewer longer running tasks and some times you may also get out of memory error.\n",
    "\n",
    "Getting a right size of the shuffle partition is always tricky and takes many runs with different value to achieve the optimized number. This is one of the key property to look for when you have performance issues on Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97676322-86c7-4196-bf10-886f7809b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count,desc\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "    ]\n",
    "\n",
    "column=[\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "\n",
    "df=spark.createDataFrame(data=simpleData,schema=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f208e21d-f531-4597-bffe-24e0d47dc905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9fb7a53-c941-475c-bf1c-1ba7a0dd0a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before applying action partitions= 5\n"
     ]
    }
   ],
   "source": [
    "print(\"before applying action partitions=\",df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cbad09d7-b8b7-41b2-9f9a-a6c0b3ee76e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|department|total_salary|\n",
      "+----------+------------+\n",
      "|   Finance|      351000|\n",
      "|     Sales|      257000|\n",
      "| Marketing|      171000|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#applying some group by action\n",
    "# df2=df.groupBy(df.department).agg(sum('salary').alias(\"total_salary\")).orderBy(desc('total_salary')).show()\n",
    "df2=df.groupBy(df.department).sum('salary').orderBy(desc(sum('salary'))).withColumnRenamed('sum(salary)','total_salary')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a05cb233-0949-4989-ad33-5f1cebbeb07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 partition= 5\n"
     ]
    }
   ],
   "source": [
    "print(\"df1 partition=\",df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c6242b8d-3f68-4c3d-9a11-4672cd9375df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 partition= 1\n"
     ]
    }
   ],
   "source": [
    "print(\"df2 partition=\",df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c82d3-7ef9-47d9-a334-ebaeff2629b7",
   "metadata": {},
   "source": [
    "## spark.default.parallelism vs spark.sql.shuffle.partitions\n",
    "https://www.learntospark.com/2020/05/persist-and-cache-in-apache-spark.html <br>\n",
    "<spark.default.parallelism > was introduced with RDD hence this property is only applicable to RDD. <br>\n",
    "The default value for this configuration set to the number of all cores on all nodes in a cluster, <br>\n",
    "on local, it is set to the number of cores on your system.\n",
    "\n",
    "\n",
    "DataFrame: Whereas < spark.sql.shuffle.partitions > was introduced with DataFrame and <br>\n",
    "it only works with DataFrame, the default value for this configuration set to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ae7fccfd-a22f-4a30-9539-948c11e5d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556c386-f495-4675-bd63-6867b66d4adf",
   "metadata": {},
   "source": [
    "## Spark – Difference between Cache and Persist?\n",
    "\n",
    "Using cache() and persist() methods, Spark provides an optimization mechanism to store the intermediate computation of an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, Dataframe, and Dataset computation result’s).\n",
    "\n",
    "Both caching and persisting are used to save the Spark RDD, Dataframe, and Dataset’s. But, the difference is, RDD cache() method default saves it to memory (MEMORY_ONLY) whereas persist() method is used to store it to the user-defined storage level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "987df5f2-933f-4ee7-a5ec-7c2bcafb76a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLet us consider the input data as csv file, and we need to read the data as dataframe and \\nuse cache on top of df.\\n\\n\\n\\nSyntax:\\n#cache RDD to store data in MEMORY_ONLY\\nrdd.cache()\\n\\n#cache DF to store data in MEMORY_ONLY\\ndf.cache()\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "\"\"\"\n",
    "Let us consider the input data as csv file, and we need to read the data as dataframe and \n",
    "use cache on top of df.\n",
    "\n",
    "\n",
    "Syntax:\n",
    "#cache RDD to store data in MEMORY_ONLY\n",
    "rdd.cache()\n",
    "\n",
    "#cache DF to store data in MEMORY_ONLY\n",
    "df.cache()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "803a9745-2d2d-40c3-bb4a-9927ae031eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"../data/simple-zipcodes.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7cd6ec4-38a4-4580-b114-227aeefeb221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5abd4c66-7b76-4c83-b366-c573c9661d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.storageLevel.useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc6e896b-0c1f-4f5b-a6df-0625a5e7c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_cache= True\n"
     ]
    }
   ],
   "source": [
    "#cache dataframe\n",
    "df_cached=df.cache()\n",
    "print(\"is_cache=\",df_cached.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8a97d97-76b5-4dbe-b55c-3bbe92e00b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|City               |Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|1           |US     |PARC PARQUE        |704    |PR   |\n",
      "|2           |US     |PASEO COSTA DEL SUR|704    |PR   |\n",
      "|10          |US     |BDA SAN LUIS       |709    |PR   |\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5937ba91-585c-462d-9d44-ad66c5c33e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41974c-caa2-465b-a005-0a45b875e530",
   "metadata": {},
   "source": [
    "<img src=\"https://1.bp.blogspot.com/-d36kJGTjXlA/XskIMtb4NgI/AAAAAAAABWA/qjU2gEmStcYMVR8kkpl-WuirN65ogjpxQCLcBGAsYHQ/s1600/persist_type.JPG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5cb11a99-1ed3-4c4e-9e85-eef50cc0028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(df.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78320f3f-5da6-44bb-aab2-596447ec0e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/12 15:15:35 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# improve persist and cache data\n",
    "import pyspark\n",
    "df_persist=df.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "616ff94f-9347-436e-87ce-1ac7ae3bca00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_persist.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59c10eb4-fe90-4da3-bc32-2aa002d4790b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[RecordNumber: int, Country: string, City: string, Zipcode: int, State: string]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd20831-f29d-4a43-8a10-bde6921a6223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[RecordNumber: int, Country: string, City: string, Zipcode: int, State: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove the cached or persisted data\n",
    "df.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72a23e-9e57-44dc-89a9-475afe40cb55",
   "metadata": {},
   "source": [
    "## PySpark Broadcast Variables\n",
    "https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/ <br>\n",
    "\n",
    "In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39c34c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/navneetsajwan/New Volume2/Data Engineering/Interview Prep stuff/spark_tutorial/Spark_RDD_Tutorial\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9458e3de-4195-4175-97c6-88bc36eff233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/\n",
    "\"\"\"\n",
    "Let me explain with an example when to use broadcast variables,\n",
    "assume you are getting a two-letter country state code in a file \n",
    "and you wanted to transform it to full state name, (for example CA to California, NY to New York e.t.c)\n",
    "by doing a lookup to reference mapping. In some instances, this data could be large\n",
    "and you may have many such lookups (like zip code e.t.c).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "print(broadcastStates.value)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "\n",
    "\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc1c58-442c-4212-a3cd-6ca47e3ebe5e",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e81ec7-7884-43ec-af73-e96f609319ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is not availabe in pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
