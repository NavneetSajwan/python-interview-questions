{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e436a2f-d479-4d6a-b13a-3504e863996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/09 07:21:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/09 07:21:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"spark sql\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02f167b-e4aa-446c-bbbf-586b00fdad6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://worker01:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark sql</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8e6fa99a60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9cc3777-19ec-4581-ba9d-a8ded1c6e8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns = [\"language\",\"users_count\"]\n",
    "\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a45feb6-d8e3-4cae-9ac6-742cb5bad81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cover_letter.txt        README.md            test.txt       zipcodes.csv\n",
      "data.txt                simple-zipcodes.csv  zipcode1.json  zipcodes.json\n",
      "multiline-zipcode.json  small_zipcode.csv    zipcode2.json\n"
     ]
    }
   ],
   "source": [
    "ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f053cea4-c9cc-4707-adf1-e1d6fd3550d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|  Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96|-66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        false|           null|               null|      null| null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96|-66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        false|           null|               null|      null| null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14|-66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           null|               null|      null| null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"../data/zipcodes.csv\",header=True,inferSchema=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b0fb58-3ca2-4439-87bc-5a4f9409b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Zipcode|State|\n",
      "+-------+-----+\n",
      "|    704|   PR|\n",
      "|    704|   PR|\n",
      "|    709|   PR|\n",
      "+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## select columns\n",
    "df.select(\"Zipcode\",\"State\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a418bb9-5b73-4431-8d3d-c8e2af6b25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|    new_column|Zipcode|\n",
      "+--------------+-------+\n",
      "|area_khairthal|    704|\n",
      "|area_khairthal|    704|\n",
      "|area_khairthal|    709|\n",
      "+--------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## add and update columns\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\"\"\"\n",
    "adding new column when condition\n",
    "\"\"\"\n",
    "df.withColumn(\"new_column\",when(df.Zipcode >3000,\"area_north\").otherwise(\"area_khairthal\")). \\\n",
    "    select(\"new_column\",\"Zipcode\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb5f92f1-d0bb-42b1-9e6e-90a4f2698529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|new_column|Zipcode|\n",
      "+----------+-------+\n",
      "| khairthal|    704|\n",
      "| khairthal|    704|\n",
      "| khairthal|    709|\n",
      "+----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "adding new column with lit fucntion\n",
    "\"\"\"\n",
    "df.withColumn(\"new_column\",lit(\"khairthal\")). \\\n",
    "    select(\"new_column\",\"Zipcode\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6c93157-5cf5-4d22-8337-93d78d1c0940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|zipcodes|\n",
      "+--------+\n",
      "|     704|\n",
      "|     704|\n",
      "|     709|\n",
      "+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "rename column\n",
    "\"\"\"\n",
    "df.withColumnRenamed(\"Zipcode\",\"zipcodes\").select(\"zipcodes\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16d89453-1f79-47bd-be1c-476fe1a424c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|language|users_counts|\n",
      "+--------+------------+\n",
      "|    Java|       20000|\n",
      "|  Python|      100000|\n",
      "|  Python|      100000|\n",
      "|   Scala|        3000|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "multiple rename column with structure type of data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# columns = [\"language\",\"users_count\"]\n",
    "\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"),(\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "\n",
    "columns=[\n",
    "    StructField(\"language\",StringType(),True),\n",
    "    StructField(\"users_counts\",StringType(),True)\n",
    "]\n",
    "\n",
    "schema=StructType(columns)\n",
    "\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "783b242a-11b6-4870-bf2e-54a641165682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|users_counts|\n",
      "+------------+\n",
      "|       20000|\n",
      "|      100000|\n",
      "|      100000|\n",
      "|        3000|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "drop a column\n",
    "\"\"\"\n",
    "\n",
    "df.drop(\"language\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "307c17af-a848-4049-88e1-ffc380dcf4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|language|users_counts|\n",
      "+--------+------------+\n",
      "|  Python|      100000|\n",
      "|  Python|      100000|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "filter data\n",
    "\"\"\"\n",
    "df.filter(df.language=='Python').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e9d7e-a477-4657-b0f1-979fffc133d3",
   "metadata": {},
   "source": [
    "## distinct() and dropDuplicates() \n",
    "Duplicate rows could be remove or drop from Spark SQL DataFrame using distinct() and dropDuplicates() functions, distinct() can be used to remove rows that have the same values on all columns whereas dropDuplicates() can be used to remove rows that have the same values on multiple selected columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b3c19d7-1e0a-4355-b8f9-5c1a63b25aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before distinct= 4\n",
      "after distinct= 3\n",
      "+--------+------------+\n",
      "|language|users_counts|\n",
      "+--------+------------+\n",
      "|    Java|       20000|\n",
      "|  Python|      100000|\n",
      "|   Scala|        3000|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "distinct()\n",
    "\"\"\"\n",
    "\n",
    "print(\"before distinct=\",df.count())\n",
    "print(\"after distinct=\",df.distinct().count())\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a83c35da-59ae-4a92-88b4-e5ddf0b5c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|language|users_counts|\n",
      "+--------+------------+\n",
      "|    Java|       20000|\n",
      "|  Python|      100000|\n",
      "|   Scala|        3000|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "dropDuplicates()\n",
    "\"\"\"\n",
    "df.dropDuplicates([\"language\",\"users_counts\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e446b-ba61-4f36-8b68-edabac83d793",
   "metadata": {},
   "source": [
    "## How to Pivot and Unpivot a Spark DataFrame\n",
    "\n",
    "Spark pivot() function is used to pivot/rotate the data from one DataFrame/Dataset column into multiple columns (transform row to column) and unpivot is used to transform it back (transform columns to rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c3252e6-de2c-4a29-8b29-ce05fda6c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "| Banana|  1000|    USA|\n",
      "|Carrots|  1500|    USA|\n",
      "|  Beans|  1600|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Banana|   400|  China|\n",
      "|Carrots|  1200|  China|\n",
      "|  Beans|  1500|  China|\n",
      "| Orange|  4000|  China|\n",
      "| Banana|  2000| Canada|\n",
      "|Carrots|  2000| Canada|\n",
      "|  Beans|  2000| Mexico|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data =[(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"),\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"),\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"),\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")\n",
    "          ]\n",
    "\n",
    "sc=spark._sc\n",
    "rdd=sc.parallelize(data)\n",
    "\n",
    "df = rdd.toDF([\"Product\",\"Amount\",\"Country\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4f93996-5f50-4d1a-b235-bdfc8d6818c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico| USA|\n",
      "+-------+------+-----+------+----+\n",
      "| Orange|  null| 4000|  null|4000|\n",
      "|  Beans|  null| 1500|  2000|1600|\n",
      "| Banana|  2000|  400|  null|1000|\n",
      "|Carrots|  2000| 1200|  null|1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark SQL provides pivot() function to rotate the data from one column into multiple columns \n",
    "(transpose row to column). It is an aggregation where one of the grouping columns values transposed \n",
    "into individual columns with distinct data. From the above DataFrame, to get the total amount \n",
    "exported to each country of each product will do group by Product, pivot by Country, \n",
    "and the sum of Amount.\n",
    "\"\"\"\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a983878d-e37f-469d-85b1-5d7e434be2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "| Orange| China|   4000|\n",
      "| Orange|   USA|   4000|\n",
      "|  Beans| China|   1500|\n",
      "|  Beans|Mexico|   2000|\n",
      "|  Beans|   USA|   1600|\n",
      "| Banana|Canada|   2000|\n",
      "| Banana| China|    400|\n",
      "| Banana|   USA|   1000|\n",
      "|Carrots|Canada|   2000|\n",
      "|Carrots| China|   1200|\n",
      "|Carrots|   USA|   1500|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF.selectExpr(\"Product\", \"stack(4, 'Canada', Canada, 'China', China, 'Mexico', Mexico,'USA',USA) as (Amount, Country)\").where(\"Country is not null\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a734d06c-fe97-416a-a5c5-0ccece34a7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Product', 'string'),\n",
       " ('Canada', 'bigint'),\n",
       " ('China', 'bigint'),\n",
       " ('Mexico', 'bigint'),\n",
       " ('USA', 'bigint')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data types\n",
    "\"\"\"\n",
    "pivotDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed154d82-19d1-4692-9ffe-a5284442ba6c",
   "metadata": {},
   "source": [
    "## joins\n",
    "\n",
    "Spark DataFrame supports all basic SQL Join Types like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN. Spark SQL Joins are wider transformations that result in data shuffling over the network hence they have huge performance issues when not designed with care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ddfe851c-f4d7-4b96-adfd-bd43f888467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000),\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000),\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1),\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1)\n",
    "  ]\n",
    "  empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\",\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "  empDF = spark.createDataFrame(data=emp,schema=empColumns)\n",
    "  empDF.show()\n",
    "\n",
    "  dept = [(\"Finance\",10),\n",
    "    (\"Marketing\",20),\n",
    "    (\"Sales\",30),\n",
    "    (\"IT\",40)\n",
    "  ]\n",
    "\n",
    "  deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "  deptDF = spark.createDataFrame(data=dept,schema=deptColumns)\n",
    "  deptDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d7aa5c1-55df-47c7-a0b2-b6235e489e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "inner join\n",
    "\"\"\"\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aeba4f8a-3f68-409f-8ad4-9d0ccdbcf617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "outer join\n",
    "\"\"\"\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0935bd4a-e07a-4b6b-9db9-497749bdda1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show()\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\").show()\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f97c5b-32dd-47bc-97d1-9ca6d9b46a8f",
   "metadata": {},
   "source": [
    "## using sql expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4a8b8d6e-e1a0-40dc-a7a8-ad7f8b7af3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7ceca30c-e5c6-46fb-a25c-b0d2f5429ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+\n",
      "|emp_id|    name|year_joined|dept_name|\n",
      "+------+--------+-----------+---------+\n",
      "|     1|   Smith|       2018|  Finance|\n",
      "|     3|Williams|       2010|  Finance|\n",
      "+------+--------+-----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select e.emp_id,e.name,e.year_joined,d.dept_name from EMP e \\\n",
    "          inner join DEPT as d where e.emp_dept_id=d.dept_id\"\n",
    "         ).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "24b989d6-1e3f-4393-bd00-57d4cac6d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "+---------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from DEPT\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26153a-6f7e-40d8-88c5-eca84eac6009",
   "metadata": {},
   "source": [
    "## union() and unionAll()\n",
    "Dataframe union() – union() method of the DataFrame is used to combine two DataFrame’s of the same structure/schema. If schemas are not the same it returns an error.\n",
    "\n",
    "DataFrame unionAll() – unionAll() is deprecated since Spark “2.0.0” version and replaced with union().\n",
    "\n",
    "DataFrameunionAll() – unionAll()(same no difference) method combines two DataFrames and returns the new DataFrame with all rows from two Dataframes regardless of duplicate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b748f3c3-e563-4920-897d-e5d481b3bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000)\n",
    "               ]\n",
    "  df = spark.createDataFrame(simpleData,[\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"])\n",
    "  df.printSchema()\n",
    "  df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5e2b9be4-8238-431e-94c9-efebcf84c743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData2 = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "(\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "(\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "(\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "(\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "            ]\n",
    "df2 = spark.createDataFrame(simpleData2,(\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "21e70af2-79e9-4f0b-91ff-91aa2717824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "000808d4-60de-4458-af1c-564ea887c866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.unionAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4d5af166-ccff-4d28-bf77-e8bec12709d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2.createOrReplaceTempView(\"table2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7be2bf-006c-434d-8233-a1e434a01d40",
   "metadata": {},
   "source": [
    "## Dataframe union() – unionAll() sql \n",
    "union take only unique from both not duplicate\n",
    " but unionAll take all value with duplicate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a0065524-3f50-4524-960e-636a7c333829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|employee_name|\n",
      "+-------------+\n",
      "|        James|\n",
      "|      Michael|\n",
      "|       Robert|\n",
      "|        Maria|\n",
      "|          Jen|\n",
      "|         Jeff|\n",
      "|        Kumar|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|employee_name|\n",
      "+-------------+\n",
      "|        James|\n",
      "|      Michael|\n",
      "|       Robert|\n",
      "|        Maria|\n",
      "|        James|\n",
      "|        Maria|\n",
      "|          Jen|\n",
      "|         Jeff|\n",
      "|        Kumar|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"select t1.employee_name from table1 as t1 union select t2.employee_name from table2 as t2\").show()\n",
    "\n",
    "spark.sql(\"select t1.employee_name from table1 as t1 union all select t2.employee_name from table2 as t2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0f78f-40c4-443e-95d4-d2092db5bc65",
   "metadata": {},
   "source": [
    "## PySpark mapPartitions() Examples (can only work on rdd)\n",
    "https://sparkbyexamples.com/pyspark/pyspark-mappartitions/ <br>\n",
    "\n",
    "https://www.educba.com/pyspark-mappartitions/ <br>\n",
    "\n",
    "Key Points of PySpark MapPartitions(): <br>\n",
    "\n",
    "It is similar to map() operation where the output of mapPartitions() returns the same number of rows as in input RDD. <br>\n",
    "It is used to improve the performance of the map() when there is a need to do heavy initializations like Database connection. <br>\n",
    "mapPartitions() applies a heavy initialization to each partition of RDD instead of each element of RDD.\n",
    "It is a Narrow transformation operation <br>\n",
    "PySpark DataFrame doesn’t have this operation hence you need to convert DataFrame to RDD to use mapPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2b4aeab7-ef28-4b02-8c28-7eb1d79203ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|  3000|\n",
      "|     Anna|    Rose|     F|  4100|\n",
      "|   Robert|Williams|     M|  6200|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mapPartitions() is used to provide heavy initialization for each partition instead of applying \n",
    "to all elements this is the main difference between PySpark map() vs mapPartitions().\n",
    "similar to map(), this also returns the same number of elements but the number of \n",
    "columns could be different.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "03d66359-d262-42c3-9775-8f874f7c914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|bonus|\n",
      "+---------------+-----+\n",
      "|    James,Smith|300.0|\n",
      "|      Anna,Rose|410.0|\n",
      "|Robert,Williams|620.0|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This function calls for each partition\n",
    "def reformat(partitionData):\n",
    "    for row in partitionData:\n",
    "        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\n",
    "\n",
    "df2=df.rdd.mapPartitions(reformat).toDF([\"name\",\"bonus\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d3638-0fcd-4ecf-911a-cd23c4507d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c41183c-dbad-43f3-a948-8a7193afd29c",
   "metadata": {},
   "source": [
    "## PySpark UDF (User Defined Function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "da5c02b3-19dc-43d7-8915-c50071bf01bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "UDF’s are used to extend the functions of the framework and re-use these functions \n",
    "on multiple DataFrame’s. For example, you wanted to convert every first letter of\n",
    "a word in a name string to a capital case; PySpark build-in features don’t have this \n",
    "function hence you can create it a UDF and reuse this as needed on many Data Frames. \n",
    "UDF’s are once created they can be re-used on several DataFrame’s and SQL expressions.\n",
    "\"\"\"\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a0fad9a2-0eab-4809-b983-06ce50f5a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The first step in creating a UDF is creating a Python function. Below snippet creates a function \n",
    "convertCase() which takes a string parameter and converts the first letter of every word to capital letter.\n",
    "UDF’s take parameters of your choice and returns a value.\n",
    "\"\"\"\n",
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b017cca1-55b3-4e37-bb8f-48b822b3af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now convert this function convertCase() to UDF by passing the function to PySpark SQL udf(), \n",
    "this function is available at org.apache.spark.sql.functions.udf package. \n",
    "Make sure you import this package before using it.\n",
    "\n",
    "PySpark SQL udf() function returns org.apache.spark.sql.expressions.UserDefinedFunction class object.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Converting function to UDF \"\"\"\n",
    "\n",
    "convertUDF = udf(lambda z: convertCase(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7a308bed-9626-4279-a4da-336fa8a7eaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now you can use convertUDF() on a DataFrame column as a regular build-in function.\n",
    "\"\"\"\n",
    "\n",
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cdb10d38-c8e9-49b0-a69e-e213dd06edfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|Name        |capital Name|\n",
      "+-----+------------+------------+\n",
      "|1    |john jones  |JOHN JONES  |\n",
      "|2    |tracey smith|TRACEY SMITH|\n",
      "|3    |amy sanders |AMY SANDERS |\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## exm-2\n",
    "\"\"\"\n",
    "how to perform udf in one step using decorattor\n",
    "\n",
    "@udf(returnType=StringType()) \n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\"\"\"\n",
    "@udf(returnType=StringType()) \n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "# df.select(col(\"Seqno\"),upperCaseUDF(col(\"Name\"))).show()\n",
    "df.withColumn(\"capital Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fdcac409-dd26-4aa5-ad49-da7c942a25e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/09 11:53:27 WARN SimpleFunctionRegistry: The function convertudf replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"register udf in sql \"\"\"\n",
    "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
    "\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "601a990e-3bc9-499a-a09d-49ee89f9de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |null        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/09 11:57:27 WARN SimpleFunctionRegistry: The function _nullsafeudf replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|_nullsafeUDF(Name)|\n",
      "+------------------+\n",
      "|John Jones        |\n",
      "|Tracey Smith      |\n",
      "|Amy Sanders       |\n",
      "|None              |\n",
      "+------------------+\n",
      "\n",
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" null check \"\"\"\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "    \n",
    "\n",
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"None\" , StringType())\n",
    "\n",
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
    "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26110dc-7d0d-4f9c-bf0c-3e2fb390b16a",
   "metadata": {},
   "source": [
    "## foreach fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a2caebbb-0317-4cfb-a17a-b055c24a1540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 amy sanders\n",
      "1 john jones\n",
      "2 tracey smith\n"
     ]
    }
   ],
   "source": [
    "df.foreach(lambda x:print(x['Seqno'],x['Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a517de8-056d-4807-86a4-978d7fca5a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c5d13d-029d-43e3-b4cf-fe5222805008",
   "metadata": {},
   "source": [
    "## PySpark Groupby Explained with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ce8de80c-50c1-49e7-8996-74354394cb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "When we perform groupBy() on PySpark Dataframe, it returns GroupedData object which contains\n",
    "below aggregate functions.\n",
    "\n",
    "count() - Returns the count of rows for each group.\n",
    "\n",
    "mean() - Returns the mean of values for each group.\n",
    "\n",
    "max() - Returns the maximum of values for each group.\n",
    "\n",
    "min() - Returns the minimum of values for each group.\n",
    "\n",
    "sum() - Returns the total for values for each group.\n",
    "\n",
    "avg() - Returns the average for values for each group.\n",
    "\n",
    "agg() - Using agg() function, we can calculate more than one aggregate at a time.\n",
    "\n",
    "pivot() - This function is used to Pivot the DataFrame which I will not be covered in this article as I already have a dedicated article for Pivot &\n",
    "\"\"\"\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f85c1f81-ec09-4b2b-b793-68bfd1b4b724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|     257000|\n",
      "|   Finance|     351000|\n",
      "| Marketing|     171000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.\n",
    "Let’s do the groupBy() on department column of DataFrame and then find the sum of salary \n",
    "for each department using sum() aggregate function.\n",
    "\"\"\"\n",
    "df.groupBy(df.department).sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "870b1601-e669-4a9b-a6ca-cdc6330e0606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      81000|\n",
      "|   Finance|      79000|\n",
      "| Marketing|      80000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate the minimum salary of each department using min()\n",
    "\"\"\"\n",
    "\n",
    "df.groupBy(df.department).min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "02af93ac-eb0d-44ee-b440-937dbeb698d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      90000|\n",
      "|   Finance|      99000|\n",
      "| Marketing|      91000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "011ecc91-2f27-41b5-bc22-e43d93eadec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").avg(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "82f45d2a-eeb3-4bc7-8d60-5f08e75f31a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").mean( \"salary\") .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8052edf3-c12b-4577-b7b1-c2888bcb99c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|     Sales|   NY|     176000|     30000|\n",
      "|     Sales|   CA|      81000|     23000|\n",
      "|   Finance|   CA|     189000|     47000|\n",
      "|   Finance|   NY|     162000|     34000|\n",
      "| Marketing|   NY|      91000|     21000|\n",
      "| Marketing|   CA|      80000|     18000|\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PySpark groupBy and aggregate on multiple columns\n",
    "\"\"\"\n",
    "# //GroupBy on multiple columns\n",
    "df.groupBy(\"department\",\"state\") \\\n",
    "    .sum(\"salary\",\"bonus\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "60b4be85-73e1-42f7-98c2-e37ca51a96cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Running more aggregates at a time using agg\n",
    "\n",
    "Using agg() aggregate function we can calculate many aggregations at a time on a single statement\n",
    "using PySpark SQL aggregate functions sum(), avg(), min(), max() mean() e.t.c. In order to use \n",
    "these, we should import \"from pyspark.sql.functions import sum,avg,max,min,mean,count\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "002194de-3815-445b-8fca-f5a6193dfa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Using filter on aggregate data\"\"\"\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b875c4-2c27-4849-b5c0-5da00bdf0b5e",
   "metadata": {},
   "source": [
    "## concat_ws() methos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e6c652f1-40ea-4e90-a41a-27eddc84f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cdfcbd43-3bcb-4a20-b1ba-eab1b660449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+------------+\n",
      "|            name|languagesAtSchool|currentState|\n",
      "+----------------+-----------------+------------+\n",
      "|    James,,Smith|   Java,Scala,C++|          CA|\n",
      "|   Michael,Rose,|   Spark,Java,C++|          NJ|\n",
      "|Robert,,Williams|        CSharp,VB|          NV|\n",
      "+----------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"languagesAtSchool\",\n",
    "   concat_ws(\",\",col(\"languagesAtSchool\")))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e0130e84-859a-4898-b94f-338656e23d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|            name|languagesAtSchool|\n",
      "+----------------+-----------------+\n",
      "|    James,,Smith|     James,,Smith|\n",
      "|   Michael,Rose,|    Michael,Rose,|\n",
      "|Robert,,Williams| Robert,,Williams|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
    "\n",
    "spark.sql(\"select name,concat_ws(',',languagesAtSchool) as languagesAtSchool from ARRAY_STRING\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f557d75-be27-4d60-a4b9-e5d6f2df5074",
   "metadata": {},
   "source": [
    "## substring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9a9df1c0-fe87-4cd3-83ba-28596d451cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------+----------+\n",
      "|            name| languagesAtSchool|currentState|first_name|\n",
      "+----------------+------------------+------------+----------+\n",
      "|    James,,Smith|[Java, Scala, C++]|          CA|      Jame|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|          NJ|      Mich|\n",
      "|Robert,,Williams|      [CSharp, VB]|          NV|      Robe|\n",
      "+----------------+------------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"first_name\",substring(df.name,1,4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "64283e54-7c88-4db5-bae1-93442d247803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|            name|spilt_name|\n",
      "+----------------+----------+\n",
      "|    James,,Smith|      Jame|\n",
      "|   Michael,Rose,|      Mich|\n",
      "|Robert,,Williams|      Robe|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name,substring(name,1,4) as spilt_name from ARRAY_STRING\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6db8b-b4b0-4497-bc90-2ad8e1b88d7a",
   "metadata": {},
   "source": [
    "## split() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a0747349-8229-4f96-853f-113465140966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------+----------+\n",
      "|            name| languagesAtSchool|currentState|first_name|\n",
      "+----------------+------------------+------------+----------+\n",
      "|    James,,Smith|[Java, Scala, C++]|          CA|     James|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|          NJ|   Michael|\n",
      "|Robert,,Williams|      [CSharp, VB]|          NV|    Robert|\n",
      "+----------------+------------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#getting firstname\n",
    "df.withColumn(\"first_name\",split(df.name,\",\")[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5211c-ad86-4c23-a5b6-4b0d88e72e8e",
   "metadata": {},
   "source": [
    "## regexp_replace(),overlay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b586de42-bccb-4eef-be4b-5f9449dcd80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|  14851 Jeffrey Rd|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "df =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "50560867-0409-49a9-aa97-cbe65fe480c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "|id |address           |state|\n",
      "+---+------------------+-----+\n",
      "|1  |14851 Jeffrey Road|DE   |\n",
      "|2  |43421 Margarita St|NY   |\n",
      "|3  |13111 Siemon Ave  |CA   |\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "By using PySpark SQL function regexp_replace() you can replace a column value with a string \n",
    "for another string/substring. regexp_replace() uses Java regex for matching, if the regex does\n",
    "not match it returns an empty string, the below example replace the street name Rd value \n",
    "with Road string on address column.\n",
    "\"\"\"\n",
    "df.withColumn('address', regexp_replace('address', 'Rd', 'Road')) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "5e2cc059-3082-44bb-8f56-6d639483f11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-----+\n",
      "|id |address              |state|\n",
      "+---+---------------------+-----+\n",
      "|1  |14851 Jeffrey Road   |DE   |\n",
      "|2  |43421 Margarita State|NY   |\n",
      "|3  |13111 Siemon Avenue  |CA   |\n",
      "+---+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" replace with when condition \"\"\"\n",
    "df.withColumn('address',\n",
    "              when(df.address.endswith('Rd'),regexp_replace(df.address, 'Rd', 'Road')). \\\n",
    "              when(df.address.endswith('St'),regexp_replace(df.address, 'St', 'State')). \\\n",
    "              when(df.address.endswith('Ave'),regexp_replace(df.address, 'Ave', 'Avenue')). \\\n",
    "              otherwise(df.address)\n",
    "                 ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e646ece-5917-44a2-a9d0-bb814e7d063d",
   "metadata": {},
   "source": [
    "## PySpark to_timestamp() – Convert String to Timestamp type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f9e13f15-94dc-4ea2-99d6-eae26ab02a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n",
      "+---+--------------------+\n",
      "| id|     input_timestamp|\n",
      "+---+--------------------+\n",
      "|  1|2019-06-24 12:01:...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6e94e021-5f74-43bb-9e96-f79ef9b230e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.withColumn(\"datetime\",to_timestamp(df.input_timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "a84aba72-7ee6-42d9-92cb-5f099e35c660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edfae8c-7d20-42ea-b263-9c12aa2a01a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf95eab-6d32-4fb2-9eb8-76fbfc970460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6db538-65a8-4000-b3dd-70b19fd94044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
