{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e8362b-741a-45eb-b6cd-c68537bc2484",
   "metadata": {},
   "source": [
    "## PySpark partitionBy() – Write to Disk Example\n",
    "\n",
    "PySpark partitionBy() is a function of pyspark.sql.DataFrameWriter class which is used to partition the large dataset (DataFrame) into smaller files based on one or multiple columns while writing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a006d322-8614-4112-985e-6fbf356a15b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/09 13:39:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/09 13:39:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/09 13:39:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"spark sql\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae0de52-a60e-4b76-8181-01360bc51d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Partitioning the data on the file system is a way to improve the performance of the query when\n",
    "dealing with a large dataset in the Data lake. A Data Lake is a centralized repository of structured,\n",
    "semi-structured, unstructured, and binary data that allows you to store a large amount of data as-is\n",
    "in its original raw format.\n",
    "\"\"\"\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da405c0b-ddc7-4cea-95e3-1a458cc849f2",
   "metadata": {},
   "source": [
    "## 2. Partition Advantages\n",
    "As you are aware PySpark is designed to process large datasets with 100x faster than the tradition processing, this wouldn’t have been possible with out partition. Below are some of the advantages using PySpark partitions on memory or on disk.\n",
    "\n",
    "Fast accessed to the data\n",
    "Provides the ability to perform an operation on a smaller dataset\n",
    "Partition at rest (disk) is a feature of many databases and data processing frameworks and it is key to make jobs work at scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b11fbad7-db25-4c4d-87cd-417d03f58110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.option(\"header\",True) \\\n",
    "        .csv(\"../data/simple-zipcodes.csv\")\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9feb12b4-21e3-4d00-901d-78738790b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitionBy()\n",
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"state\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"../data/zipcodes-state\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b17042-e699-4451-8871-22fe366e6fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3603a6-a82d-4bba-a291-9b2d1f099367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt creates a folder hierarchy for each partition; we have mentioned the first partition as state\\nfollowed by city hence, it creates a city folder inside the state folder (one folder for each city in\\na state).\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "On each directory, you may see one or more part files (since our dataset is small, \n",
    "all records for each state are kept in a single part file). You can change this behavior by repartition()\n",
    "the data in memory first. Specify the number of partitions (part files) you would want for each \n",
    "state as an argument to the repartition() method.\n",
    "\"\"\"\n",
    "#partitionBy()\n",
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"state\",\"city\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"../data/zipcodes-state\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "It creates a folder hierarchy for each partition; we have mentioned the first partition as state\n",
    "followed by city hence, it creates a city folder inside the state folder (one folder for each city in\n",
    "a state).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b24284-a926-499a-990e-7303aa326618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Using repartition() and partitionBy() together\n",
    "\n",
    "For each partition column, if you wanted to further divide into several partitions, \n",
    "use repartition() and partitionBy() together as explained in the below example.\n",
    "\n",
    "repartition() creates specified number of partitions in memory. The partitionBy() \n",
    "will write files to disk for each memory partition and partition column. \n",
    "\"\"\"\n",
    "\n",
    "#Use repartition() and partitionBy() together\n",
    "df.repartition(2) \\\n",
    "        .write.option(\"header\",True) \\\n",
    "        .partitionBy(\"state\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"../data/zipcodes-state-more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6eb2f7e-bc52-4d16-ac5e-2f326b1effbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7. Data Skew – Control Number of Records per Partition File\n",
    "\"\"\"\n",
    "\n",
    "#partitionBy() control number of partitions\n",
    "df.write.option(\"header\",True) \\\n",
    "        .option(\"maxRecordsPerFile\", 2) \\\n",
    "        .partitionBy(\"state\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"../data/zipcodes-state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95df7ecf-e4d7-4765-8972-5d378d43a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      "\n",
      "+------------+-------+-------------+-------+\n",
      "|RecordNumber|Country|         City|Zipcode|\n",
      "+------------+-------+-------------+-------+\n",
      "|       54356|     US|  SPRUCE PINE|  35585|\n",
      "|       54354|     US|SPRING GARDEN|  36275|\n",
      "|       54355|     US|  SPRINGVILLE|  35146|\n",
      "+------------+-------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Read a Specific Partition\n",
    "dfSinglePart=spark.read.option(\"header\",True) \\\n",
    "            .csv(\"../data/zipcodes-state/state=AL\")\n",
    "dfSinglePart.printSchema()\n",
    "dfSinglePart.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd304f8b-18a6-4fee-9ce0-d1a73b8f24bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
