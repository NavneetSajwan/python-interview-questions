{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Difficulty of programming in Mapreduce and Hadoop\n",
    "    - simple word count programming takes 60-70 lines of code\n",
    "    - performance bottlenecks: multiple disk writes\n",
    "\n",
    "2. Support for iterative jobs\n",
    "3. Support of streaming jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in-memory computation\n",
    "- distribitedd processing with parallelization\n",
    "- support for multiple cluster manmagers\n",
    "- fault-tolerant\n",
    "- lazy evaluation\n",
    "- cache and persistence\n",
    "- inbuild optimization and dataframes\n",
    "- supports ansi sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Ecosystem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark compute engine + spark core api(scala, python, java, r)\n",
    "- high level api on top of core api:\n",
    "    - spark sql, dataframes, datasets\n",
    "    - streaming\n",
    "    - MLlib\n",
    "    - GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark installation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparkcontext:\n",
    "\n",
    "- entry point for spark functionality\n",
    "- represents connection to a cluster\n",
    "- used to create RDD and boradcast variables on the cluster\n",
    "- master:  local[*] means all available threads\n",
    "- app_id: name of the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparksession:\n",
    "\n",
    "- create dataframe\n",
    "- register dfs as tables\n",
    "- execute sql over tables\n",
    "- cache tables\n",
    "- read parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"word count\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['city', 'users']\n",
    "data = [\n",
    "    ('Banglore', '7654'), \n",
    "    ('Delhi','4234'),\n",
    "    ('Mumbai', '234')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = rdd.toDF()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RDD(resilient distributed dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fundamental bukilding block of spark\n",
    "- fault-tolerant\n",
    "- immutable distributed collection of objects\n",
    "- just like a list in python, but data is distributed over nodes in a cluster\n",
    "- rdd creates logical partitions of data\n",
    "- it abstracts the parallelization part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Context webui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jobs\n",
    "- Stages\n",
    "- tasks\n",
    "- storage\n",
    "- environemnt\n",
    "- executors\n",
    "- sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc1 = SparkContext('local','test')\n",
    "# sc2 = SparkContext('local','test2')\n",
    "\n",
    "#### Note:  Only one sparkcontext at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in spark 2.0 sparksession was used, which is kind of an alternative to sparkcontext. And we can create multiple sparksession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How fast is spark comapred to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 18:51:09 WARN Utils: Your hostname, Navneets-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.229.113 instead (on interface en0)\n",
      "24/02/06 18:51:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/06 18:51:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4])\n",
    "rdd1_first = rdd1.filter(lambda x: x<3)\n",
    "rdd1_first.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcount with spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 18:51:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonWordCountAnalysis\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.191:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PythonWordCountAnalysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x103efbc40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the input file in spark\n",
    "fname = \"/Users/navneet/Documents/python-interview-questions/docker_commands.txt\"\n",
    "df = spark.read.text(fname)\n",
    "# loads all the lines into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|   docker stop mysql|\n",
      "|     docker rm mysql|\n",
      "|                    |\n",
      "|docker run --name...|\n",
      "|      -p 3308:3306 \\|\n",
      "|    -e MYSQL_ROOT...|\n",
      "|    -v data:/var/...|\n",
      "|             mysql:8|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=df.rdd.map(lambda r: r[0])\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: 3\n",
      "stop: 1\n",
      "mysql: 3\n",
      "rm: 1\n",
      ": 17\n",
      "run: 1\n",
      "--name: 1\n",
      "-d: 1\n",
      "\\: 4\n",
      "-p: 1\n",
      "3308:3306: 1\n",
      "-e: 1\n",
      "MYSQL_ROOT_PASSWORD=change-me: 1\n",
      "-v: 1\n",
      "data:/var/lib/mysql: 1\n",
      "mysql:8: 1\n"
     ]
    }
   ],
   "source": [
    "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "                .map(lambda x: (x, 1)) \\\n",
    "                .reduceByKey(add)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions:\n",
    "\n",
    "- return values ot the driver prpgram\n",
    "- return anything other than rdd\n",
    "- trigger computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/07 11:46:49 WARN Utils: Your hostname, Navneets-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.20.10.7 instead (on interface en0)\n",
      "24/01/07 11:46:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/07 11:46:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Actions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10b0fed90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Actions\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('a',3),('b',5),('c',12),('b','13'),('b',19)]\n",
    "inputrdd = spark.sparkContext.parallelize(data)\n",
    "inputrdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations:\n",
    "\n",
    "- spark operations that allow changing one df to another\n",
    "- lazily evaulated\n",
    "- executed only when actions are triggered\n",
    "- https://sparkbyexamples.com/spark/spark-rdd-transformations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Narrow Transformations:\n",
    "\n",
    "-  one to one mapping\n",
    "- shuffling bw partitions is not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdd = spark.sparkContext.textFile(\"/Users/navneet/Documents/python-interview-questions/subject material/spark/[English (auto-generated)] Scaling Privacy in a Spark Ecosystem [DownSub.com].txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdd = rdd1.map(lambda x :x +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wide transformations:\n",
    "\n",
    "- shuffling is required\n",
    "- groupbykey, reduceby key\n",
    "- data needs to be exchangeqd bw partitions in order to complete the transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization in spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Serialization:\n",
    "\n",
    "- java: default\n",
    "- kyro: 10x faster than java\n",
    "- tradeoff bw performance and versatility # TBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Memory Tuning:\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parallelism: cluster utilization\n",
    "- memory use of reduce tasks\n",
    "- boradcasting for lasrge variables\n",
    "- data \n",
    "- take() over collect()\n",
    "- persistence in spark\n",
    "- avoid groupbykey, use groupbykey\n",
    "- aggregate with accumulators\n",
    "- braodcast variables\n",
    "- partitioning:\n",
    "    - depends on the no. of cores\n",
    "    - less partition means resource underutilized\n",
    "    - more partitions means heavy shuffling\n",
    "    - generally: 128 mb is max no. of bytes in a single partition\n",
    "- repartition:\n",
    "    - full data shuffle\n",
    "- coalesce:\n",
    "    - works when decreasing the  partitions.\n",
    "    - minimizes the data movement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how are stages and jobs created in spark\n",
    "lambda limitations\n",
    "what happens when you submit a spark job to spark\n",
    "python threadding and multiprocessing\n",
    "async operation in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
